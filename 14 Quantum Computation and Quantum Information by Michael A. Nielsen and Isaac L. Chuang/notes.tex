\documentclass{article}
\usepackage{amsfonts} % For mathbb
\usepackage{amsmath} % For align*
\usepackage{bookmark} % For links
\usepackage{braket} % For bra-ket notation
\usepackage{qcircuit} % For quantum circuit diagrams

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue
}

\newcommand{\tr}{\operatorname{tr}}

\title{Quantum Computation and Quantum Information by Michael A. Nielsen and Isaac L. Chuang}
\author{Chris Doble}
\date{May 2024}

\begin{document}

\maketitle

\tableofcontents

\part{Fundamental concepts}

\section{Introduction and overview}

\setcounter{subsection}{1}
\subsection{Quantum bits}

\begin{itemize}
  \item The special states $\ket{0}$ and $\ket{1}$ form an orthonormal basis and are known as \textbf{computational basis states}.

  \item A quantum bit (\textbf{qubit}) is a linear combination of the computational basis states \[\ket{\psi} = \alpha \ket{0} + \beta \ket{1}\] where $\alpha$ and $\beta$ are complex numbers.

  \item When we measure a qubit we either get $\ket{0}$ with probability $|\alpha|^2$ or $\ket{1}$ with probability $|\beta|^2$. Thus, $|\alpha|^2 + |\beta|^2 = 1$ and a qubit can be thought of as a unit vector in a two-dimensional complex vector space.

  \item If a qubit is in the state \[\ket{+} = \frac{1}{\sqrt{2}} (\ket{0} + \ket{1})\] there's a 50/50 chance of measuring $\ket{0}$ or $\ket{1}$.

  \item If we let \[\alpha = e^{i \gamma} \cos \frac{\theta}{2}\] and \[\beta = e^{i \gamma} e^{i \varphi} \sin \frac{\theta}{2}\] then \begin{align*}
          |\alpha|^2 + |\beta|^2 & = \alpha^* \alpha + \beta^* \beta                   \\
                                 & = \cos^2 \frac{\theta}{2} + \sin^2 \frac{\theta}{2} \\
                                 & = 1
        \end{align*} so the qubit is still normalised and it can be written \[\ket{\psi} = e^{i \gamma} \left( \cos \frac{\theta}{2} \ket{0} + e^{i \varphi} \sin \frac{\theta}{2} \ket{1} \right).\] It turns out that $e^{i \gamma}$ has no observable effects and we can effectively write \[\ket{\psi} = \cos \frac{\theta}{2} \ket{0} + e^{i \varphi} \sin \frac{\theta}{2} \ket{1}.\] This defines a point on a three-dimensional sphere known as the \textbf{Bloch sphere} where $\theta$ and $\varphi$ take on their usual roles in a spherical coordinate system.

  \item Before measurement a qubit is in a linear combination of $\ket{0}$ and $\ket{1}$ but when measured you get one or the other and the state of the system changes to match the measured result.
\end{itemize}

\subsubsection{Multiple Bits}

\begin{itemize}
  \item A two qubit system has four computational basis state $\ket{00}$, $\ket{01}$, $\ket{10}$, and $\ket{11}$ so the general expression for the state of such a system is \[\ket{\psi} = \alpha_{00} \ket{00} + \alpha_{01} \ket{01} + \alpha_{10} \ket{10} + \alpha_{11} \ket{11}.\]

  \item If you were to measure the first qubit, you would get $\ket{0}$ with probability $|\alpha_{00}|^2 + |\alpha_{01}|^2$ and the system would be left in the state \[\ket{\psi} = \frac{\alpha_{00} \ket{00} + \alpha_{01} \ket{01}}{\sqrt{|\alpha_{00}|^2 + |\alpha_{01}|^2}},\] i.e. it is renormalised such that the normalisation condition still holds.
\end{itemize}

\subsection{Quantum Computation}

\subsubsection{Single Qubit Gates}

\begin{itemize}
  \item The quantum \textsc{not} changes $\ket{0}$ to $\ket{1}$ and $\ket{1}$ to $\ket{0}$. It acts linearly on superpositions of those states, i.e. it turns $\alpha \ket{0} + \beta \ket{1}$ into $\beta \ket{0} + \alpha \ket{1}$. If a quantum state $\ket{\psi} = \alpha \ket{0} + \beta \ket{1}$ is written in vector notation as \[\ket{\psi} = \begin{bmatrix}
            \alpha \\
            \beta
          \end{bmatrix}\] then the quantum \textsc{not} gate can be expressed in matrix form as \[X = \begin{bmatrix}
            0 & 1 \\
            1 & 0
          \end{bmatrix}.\]

  \item In order to preserve the normalisation condition, matrix representations of quantum gates must be unitary, i.e. $M^\dagger M = I$ where $I$ is the identity matrix.

  \item An arbitrary unitary 2x2 matrix can be decomposed into a finite set of other 2x2 matrices. This means an arbitrary single qubit gate can be generated by a finite set of other gates.
\end{itemize}

\subsubsection{Multiple Qubit Gates}

\begin{itemize}
  \item The controlled-\textsc{not} or \textsc{cnot} gate is a multi-qubit gate that has two input qubits known as the control qubit and the target qubit. If the control qubit is set to $\ket{0}$ the target qubit is left alone, but if it's set to $\ket{1}$ the target qubit is flipped. Another way of writing this is $\ket{A, B} \rightarrow \ket{A, A \oplus B}$ where $\oplus$ is modulo-two addition. Yet another way of writing this is in matrix form \[U_{CN} = \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 1 & 0
          \end{bmatrix}\] where the first column describes what happens to the $\ket{00}$ basis state, etc.

  \item Other classical gates like \textsc{nand} or \textsc{xor} can't be represented as quantum gates as they're irreversibile. For example, given the output $A \oplus B$ from an \textsc{xor} gate it's not possible to determine what the inputs $A$ and $B$ were.

  \item Any multiple qubit logic gate may be composed from \textsc{cnot} and single qubit gates.
\end{itemize}

\subsubsection{Measurements in bases other than the computational basis}

\begin{itemize}
  \item Given any basis states $\ket{a}$ and $\ket{b}$ it is possible to express an arbitrary state as a linear combination $\alpha \ket{a} + \beta \ket{b}$ of those states. For example, if $\ket{a} = \ket{+} = (\ket{0} + \ket{1}) / \sqrt{2}$ and $\ket{b} = \ket{-} = (\ket{0} - \ket{1}) / \sqrt{2}$ then \[\ket{\phi} = \alpha \ket{0} + \beta \ket{1} = \alpha \frac{\ket{+} + \ket{-}}{\sqrt{2}} + \beta \frac{\ket{+} - \ket{-}}{\sqrt{2}} = \frac{\alpha + \beta}{\sqrt{2}} \ket{+} + \frac{\alpha - \beta}{\sqrt{2}} \ket{-}.\]

  \item If $\ket{a}$ and $\ket{b}$ are orthonormal, it's also possible to perform a measurement with respect to that basis. In the example above you would measure $\ket{+}$ with probability $|\alpha + \beta|^2 / 2$ and $\ket{-}$ with probability $|\alpha - \beta|^2 / 2$.
\end{itemize}

\subsubsection{Quantum circuits}

\begin{itemize}
  \item Applying a \textsc{cnot} gate three times swaps the state of two qubits: \begin{align*}
          \ket{a, b} & \rightarrow \ket{a, a \oplus b}                                           \\
                     & \rightarrow \ket{a \oplus (a \oplus b), a \oplus b} = \ket{b, a \oplus b} \\
                     & \rightarrow \ket{b, (a \oplus b) \oplus b} = \ket{b, a}.
        \end{align*} This can be represented in a quantum circuit diagram \[\Qcircuit @C=1em @R=.7em {
          & \ctrl{1} & \targ & \ctrl{1} & \qw \\
          & \targ & \ctrl{-1} & \targ & \qw
          }\] which is equivalent to \[\Qcircuit @C=1em @R=1.5em {
          & \qswap & \qw \\
          & \qswap \qwx & \qw
          }\]

  \item Quantum circuits don't allow loops (they are acyclic), they don't allow multiple wires to be joined into one, and they don't allow one wire to be split into multiple wires.

  \item If $U$ is a unitary matrix operating on $n$ qubits then $U$ can be regarded as a quantum gate. Then we can define a controlled-$U$ gate which takes a single control qubit and $n$ target qubits. If the control qubit is set to $0$ then nothing happens to the target qubits. If it's set to 1 then the $U$ gate is applied to the target qubits. \[\Qcircuit @C=1em @R=.7em {
          & \ctrl{1} & \qw \\
          & \multigate{2}{U} & \qw \\
          & \ghost{U} & \qw \\
          & \ghost{U} & \qw
          }\]

  \item Measurement is represented as \[\Qcircuit @C=1em @R=.7em {
          \lstick{\ket{\psi}} & \meter & \cw
          }\] where the double lines represent a classical bit.
\end{itemize}

\subsubsection{Qubit copying circuit?}

\begin{itemize}
  \item A \textsc{cnot} gate can be used to copy a classical bit. As input it takes a bit in state $x$ and a ``scratchpad'' bit set to zero. The output is a bit in state $x$ and a bit in state $0$ if $x = 0$ and $1$ if $x = 1$ â€” both bits are in state $x$.

  \item However, a \textsc{cnot} gate cannot be used to copy quantum information. As input it takes a qubit in the state $\ket{\psi} = a \ket{0} + b \ket{1}$ and a ``scratchpad'' qubit set to $\ket{0}$. The input state may be written as \[[a \ket{0} + b \ket{1}] \ket{0} = a \ket{0 0} + b \ket{1 0}.\] The gate negates the second bit if the first bit is $1$ so the output is $a \ket{0 0} + b \ket{1 1}$. The desired output is \[\ket{\psi} \ket{\psi} = a^2 \ket{0 0} + a b \ket{0 1} + a b \ket{1 0} + b^2 \ket{1 1}.\] Unless $a = 0$ or $b = 0$, i.e. it's a classical bit, the qubit hasn't been copied. This is known as the \textbf{no-cloning theorem}.
\end{itemize}

\subsubsection{Example: Bell states}

\begin{itemize}
  \item The Hadamard gate acts on a single qubit, mapping $\ket{0}$ to $(\ket{0} + \ket{1}) / \sqrt{2}$ and $\ket{1}$ to $(\ket{0} - \ket{1}) / \sqrt{2}$.

  \item This circuit \[\Qcircuit @C=1em @R=.7em {
          & \gate{H} & \ctrl{1} & \qw \\
          & \qw & \targ & \qw
          }\] applies a Hadamard gate to the first qubit which then acts as the control input to the \textsc{cnot}. The output states \begin{align*}
          \ket{\beta_{00}} & = \frac{\ket{00} + \ket{11}}{\sqrt{2}},             \\
          \ket{\beta_{01}} & = \frac{\ket{01} + \ket{10}}{\sqrt{2}},             \\
          \ket{\beta_{10}} & = \frac{\ket{00} - \ket{11}}{\sqrt{2}}, \text{ and} \\
          \ket{\beta_{11}} & = \frac{\ket{01} - \ket{10}}{\sqrt{2}}              \\
        \end{align*} are known as the \textbf{Bell states}, \textbf{EPR states}, or \textbf{EPR pairs}.
\end{itemize}

\subsubsection{Example: quantum teleportation}

\begin{itemize}
  \item If Alice and Bob prepare a Bell pair, Alice takes the first qubit, and Bob takes the second qubit, it's possible to teleport an arbitrary qubit from Alice to Bob at a later time by communicating only classical information.

  \item This doesn't enable faster-than-light communication because Alice still needs to communicate classical information to Bob which is limited by the speed of light. Bob then uses this information to conditionally apply further gates to his qubit to get it into the teleported state.
\end{itemize}

\subsection{Quantum algorithms}

\subsubsection{Classical computations on a quantum computer}

\begin{itemize}
  \item Classical circuits may contain irreversible operations, e.g. \textsc{nand} gates, but quantum circuits must be reversible. In order to simulate classical circuits on a quantum computer we must find a way to make these irreversible operations reversible.

  \item Any classical circuit can be replaced by an equivalent classical circuit containing only reversible elements by making use of the \textbf{Toffoli gate}. This gate has three input bits and three output bits. Two bits are control bits and are unaffected by the gate. The third bit is a target bit that is flipped if both of the control bits are $1$ and is otherwise left alone. Applying the Toffoli gate twice has the effect $(a, b, c) \rightarrow (a, b, c \oplus a b) \rightarrow (a, b, c)$ and thus the gate is reversible.

  \item The Toffoli gate can also be used to simulate \textsc{nand} gates and to do \textsc{fanout}. With this it is possible to make any classical circuit reversible.

  \item The equivalent quantum Toffoli gate works as expected, e.g. it changes $\ket{1 1 0}$ to $\ket{1 1 1}$. This ensures quantum computers are capable of performing any classical computation.

  \item If the classical computer is non-deterministic, i.e. it can generate random numbers, this can be simulated on a quantum computer by preparing a qubit in the state $\ket{0}$, sending it through a Hadamard gate to produce $(\ket{0} + \ket{1}) / \sqrt{2}$, and then measuring the state. The result will be $\ket{0}$ or $\ket{1}$ with 50/50 probability.
\end{itemize}

\subsubsection{Quantum parallelism}

\begin{itemize}
  \item Quantum parallelism allows quantum computers to evaluate a function $f(x)$ over many different values of $x$ simultaneously. For example, if you have a gate that accepts two input qubits $\ket{x, y}$ and produces two output qubits $\ket{x, y \oplus f(x)}$ you can start with $\ket{0, 0}$, send the first bit through a Hadamard gate to produce $(\ket{0} + \ket{1}) \ket{0} / \sqrt{2}$, then send both bits through the $f(x)$ gate to produce $(\ket{0, f(0)} + \ket{1, f(1)}) / \sqrt{2}$.

  \item The above process can be generalised to functions on an arbitrary number of bits by using a general operation known as the \textbf{Hadamard transform} which is $n$ Hadamard gates operating in parallel on $n$ qubits. This is denoted $H^{\otimes n}$, e.g. \[H^{\otimes 2} = \left( \frac{\ket{0} + \ket{1}}{\sqrt{2}} \right) \left( \frac{\ket{0} + \ket{1}}{\sqrt{2}} \right) = \frac{1}{2} (\ket{0 0} + \ket{0 1} + \ket{1 0} + \ket{1 1}).\]

  \item However, quantum parallelism isn't particularly useful in itself because upon measurement you only get a single value $\ket{x, f(x)}$.
\end{itemize}

\subsubsection{Deutsch's algorithm}

\begin{itemize}
  \item Deutsch's algorithm can determine $f(0) \oplus f(1)$ â€” a global property of $f(x)$ â€”Â using only one evaluation of $f$. A classical computer would require at least two.

  \item This demonstrates a property of quantum computers where in the state $\ket{0} \ket{f(0)} + \ket{1} \ket{f(1)}$ it's possible to make the two alternatives inferere with each other resulting in some global property of the function $f$.
\end{itemize}

\subsubsection{The Deutsch-Jozsa algorithm}

\begin{itemize}
  \item The Deutsch-Jozsa algorithm can be used to determine if a function $f$ is constant (always returns the same value) or balanced (returns $0$ for half of its domain and $1$ for the other half) with a single evaluation of $f$. The same problem on a classical computer requires $2^n / 2 + 1$ evaluations of $f$.
\end{itemize}

\subsubsection{Quantum algorithms summarized}

\begin{itemize}
  \item There are three classes of quantum algorithms which provide an advantage over known classical algorithms:

        \begin{itemize}
          \item algorithms based on quantum versions of the Fourier transform,

          \item quantum search algorithms, and

          \item quantum simulation where a quantum computer is used to simulate a quantum system.
        \end{itemize}

  \item Classically the fast Fourier transform takes roughly $N \log (N) = n 2^n$ steps to transform $N = 2^n$ numbers. On a quantum computer this can be accomplished in about $\log^2 (N) = n^2$ steps which is an exponential saving.

  \item However, the Fourier transform is being performed on the amplitudes of the quantum state, not on the input vector itself. This means we need clever ways to extract this information.

  \item Classical search algorithms are typically $\mathcal{O}(n)$ whereas the quantum equivalents are $\mathcal{O}(\sqrt{n})$.

  \item \textbf{Computational complexity theory} is the subject of classifying the difficuly of various computational problems. The most basic idea is a \textbf{complexity class} which can be though of as a collection of computational problems that all share some common feature with respect to the computational resources needed to solve those problems.

  \item One complexity class is $\mathbf{P}$ â€”Â the class of problems that can be solved quickly on a classical computer.

  \item Another is $\mathbf{NP}$ â€”Â the class of problems whose solutions can be verified quickly on a classical computer.

  \item $\mathbf{P}$ is a subset of $\mathbf{NP}$, but it's not known if the're equal.

  \item $\mathbf{NP}$-complete problems are an important subclass of $\mathbf{NP}$ problems. They're important because: there are many problems that are known to be $\mathbf{NP}$-complete, and any given $\mathbf{NP}$-complete problem is known to be ``at least as hard'' as all other $\mathbf{NP}$ problems, i.e. an algorithm to solve a specific $\mathbf{NP}$-complete problem can be modified to solve any other $\mathbf{NP}$ problem. If $\mathbf{P} \ne \mathbf{NP}$ then it will follow that no $\mathbf{NP}$-complete problem can be efficiently solved on a classical computer.
\end{itemize}

\subsection{Experimental quantum information processing}

\subsubsection{The Stern-Gerlach experiment}

\begin{itemize}
  \item The Stern-Gerlach device sends an atom through a magnetic field and measures its deflection. The atom is electrically neutral so it is not deflected by the Lorentz force. The electron is in the $n = 1$ level so there one possible angular momentum value. You would expect there to be a single measured value but there are two. This is due to the spin of the electron.

  \item If you chain Stern-Gerlach devices, the first measuring in the $\hat{z}$ direction and the $\ket{+z}$ output passing into another device measuring in the $\hat{x}$ direction you see a 50/50 split in the output. If you pass the $\ket{+x}$ output into another device measuring in the $\hat{z}$ direction you again see a 50/50 split.
\end{itemize}

\subsection{Quantum information}

\subsubsection{Quantum information theory: example problems}

\begin{itemize}
  \item Shannon's noiseless channel coding theorem quantifies how many bits are required to store information being emitted by a source of information. A classical information source is described by a set of probabilities $p_j, \,j = 1, 2, \ldots, d$. Each use of the source results in the ``letter'' $j$ being emitted, chosen at random with probability $p_j$, independently for each use of the source. If letters that occur more frequently can be represented using fewer bits, the information can be compressed. Shannon's noiseless channel coding theorem quantifies exactly how well such a compression scheme can work.

  \item Shannon's noisy channel coding theorem quantifies the amount of information that can be transmitted through a noisy channel. The idea is to encode the information using error-correcting codes so that any noise introduced by the channel can be corrected at the other end. For example, a single bit of information may be encoded across two bits, in which case the channel has a capacity of half a bit.
\end{itemize}

\section{Introduction to quantum mechanics}

\subsection{Linear algebra}

\subsubsection{Bases and linear independence}

\begin{itemize}
  \item A \textbf{spanning set} for a vector space is a set of vectors $\ket{v_1}, \ldots, \ket{v_n}$ such that any vector $\ket{v}$ in the vector space can be written as a linear combination $\ket{v} = \sum_i a_i \ket{v}_i$ of vectors in that set. We say that the vectors $\ket{v_1}, \ldots, \ket{v_n}$ \textbf{span} the vector space.

  \item A set of non-zero vectors $\ket{v_1}, \ldots, \ket{v_n}$ are \textbf{linearly dependent} if there exists a set of complex numbers $a_1, \ldots, a_n$ with $a_i \ne 0$ for at least one value of $i$ such that \[a_1 \ket{v_1} + \ldots + a_n \ket{v_n} = 0.\] A set of vectors is \textbf{linearly independent} if it's not linearly dependent.

  \item Any two sets of linearly independent vectors which span a vector space $V$ contain the same number of elements. We call such a set a \textbf{basis} for $V$.
\end{itemize}

\subsubsection{Linear operators and matrices}

\begin{itemize}
  \item A \textbf{linear operator} between vector spaces $V$ and $W$ is defined to be any function $A : V \rightarrow W$ which is linear in its inputs \[A \left( \sum_i a_i \ket{v_i} \right) = \sum_i a_i A(\ket{v_i}).\]

  \item We usually write $A \ket{v}$ to denote $A(\ket{v})$.

  \item Linear operators can be expressed as matrices. Suppose $A : V \rightarrow W$ is a linear operator between the vector spaces $V$ and $W$, $\ket{v_1}, \ldots, \ket{v_m}$ is a basis for $V$, and $\ket{w_1}, \ldots, \ket{w_n}$ is a basis for $W$. Then for each $j$ in the range $1, \ldots, m$ there exist complex numbers $A_{1 j}$ throught $A_{n j}$ such that \[A \ket{v_j} = \sum_i A_{i j} \ket{w_i},\] i.e. the result of applying $A$ to each basis vector $\ket{v_j}$ can be expressed as a linear combination of the basis vectors $\ket{w_i}$ and the coefficients $A_{i j}$ form the matrix representation of $A$.
\end{itemize}

\subsubsection{The Pauli matrices}

\begin{itemize}
  \item The Pauli matrices are defined as \begin{align*}
          \sigma_0 & = I              \\
                   & = \begin{bmatrix}
                         1 & 0 \\
                         0 & 1
                       \end{bmatrix} \\
          \sigma_1 & = \sigma_x       \\
                   & = X              \\
                   & = \begin{bmatrix}
                         0 & 1 \\
                         1 & 0
                       \end{bmatrix} \\
          \sigma_2 & = \sigma_y       \\
                   & = Y              \\
                   & = \begin{bmatrix}
                         0 & -i \\
                         i & 0
                       \end{bmatrix} \\
          \sigma_3 & = \sigma_z       \\
                   & = Z              \\
                   & = \begin{bmatrix}
                         1 & 0  \\
                         0 & -1
                       \end{bmatrix}
        \end{align*}
\end{itemize}

\subsubsection{Inner products}

\begin{itemize}
  \item An \textbf{inner product} is a function which takes as input two vectors $\ket{v}$ and $\ket{w}$ from a vector space and produces a complex number as output. It can be denoted $(\ket{v}, \ket{w})$, but this isn't the standard quantum mechanical notation. The standard notation is $\braket{v | w}$ where $\bra{v}$ is the dual vector to the vector $\ket{v}$. The dual vector is a linear operator from the inner product space $V$ to the complex numbers $\mathbb{C}$, defined by \[\bra{v} (\ket{w}) = \braket{v | w} = (\ket{v}, \ket{w}).\]

  \item A function $(\cdot, \cdot)$ from $V \times V$ to $\mathbb{C}$ is an inner product if

        \begin{itemize}
          \item it is linear in the second argument \[\left( \ket{v}, \sum_i \lambda_i \ket{w_i} \right) = \sum_i \lambda_i (\ket{v}, \ket{w_i}),\]

          \item $(\ket{v}, \ket{w}) = (\ket{w}, \ket{v})^*$, and

          \item $(\ket{v}, \ket{v}) \ge 0$ with equality if and only if $\ket{v} = 0$.
        \end{itemize}

  \item A vector space equipped with an inner product is called an \textbf{inner product space}.

  \item In finite dimensional complex vector spaces, a Hilbert space is the same as an inner product space.

  \item Vectors $\ket{w}$ and $\ket{v}$ are \textbf{orthogonal} if their inner product is zero.

  \item The \textbf{norm} of a vector is defined as \[||\ket{v}|| = \sqrt{\braket{v, v}}.\]

  \item A \textbf{unit vector} is a vector $\ket{v}$ such that $||\ket{v}|| = 1$. We also say such a vector is \textbf{normalised}.

  \item A vector can be normalised by dividing by its norm.

  \item A set $\ket{i}$ of vectors with index $i$ is \textbf{orthonormal} if each vector is a unit vector and distinct vectors in the set are orthogonal, i.e. $\braket{i | j} = \delta_{i j}$.

  \item If $\ket{v}$ is a vector in an inner product space $V$ and $\ket{w}$ is a vector in an inner product space $W$, then $\ket{w} \bra{v}$ is the linear operator from $V$ to $W$ whose action is defined by \[(\ket{w} \bra{v}) (\ket{v'}) = \ket{w} \braket{v | v'} = \braket{v | v'} \ket{w}.\]

  \item Let $\ket{i}$ be any orthonormal basis for the vector space $V$, so an arbitrary vector $\ket{v}$ can be written $\ket{v} = \sum_i v_i \ket{i}$ for some set of complex numbers $v_i$. Therefore \[\left( \sum_i \ket{i} \bra{i} \right) \ket{v} = \sum_i \ket{i} \braket{i | v} = \sum_i v_i \ket{i} = \ket{v}\] and thus \[\sum_i \ket{i} \bra{i} = I.\] This is known as the \textbf{completeness relation}.

  \item Suppose $A : V \rightarrow W$ is a linear operator, $\ket{v_i}$ is an orthonormal basis for $V$, and $\ket{w_j}$ is an orthonormal basis for $W$. We can use the completeness relation twice to obtain \begin{align*}
          A & = I_W A I_V                                              \\
            & = \sum_{i j} \ket{w_j} \braket{w_j | A | v_i} \bra{v_i}  \\
            & = \sum_{i j} \braket{w_k | A | v_i} \ket{w_j} \bra{v_i}.
        \end{align*} This is known as the \textbf{outer product representation} of $A$. From this we can see the matrix representation of $A$ has $\braket{w_k | A | v_i}$ in the $i$th column and $j$th row.
\end{itemize}

\subsubsection{Eigenvector and eigenvalues}

\begin{itemize}
  \item An \textbf{eigenvector} of a linear operator $A$ on a vector space is a non-zero vector $\ket{v}$ such that $A \ket{v} = v \ket{v}$ where $v$ is a complex number known as the \textbf{eigenvalue} of $A$ corresponding to $\ket{v}$.

  \item The \textbf{characteristic function} of an operator is defined to be $c(\lambda) = \det |A - \lambda I|$ and the solutions to $c(\lambda) = 0$ are the eigenvalues of the operator $A$.

  \item The \textbf{eigenspace} corresponding to en eigenvalue $v$ is the set of vectors which have eigenvalue $v$.

  \item A \textbf{diagonal representation} of an operator $A$ on a vector space $V$ is a representation $A = \sum_i \lambda_i \ket{i} \bra{i}$ where the vectors $\ket{i}$ form an orthonormal set of eigenvectors for $A$ with corresponding eigenvalues $\lambda_i$. In other words, the basis vectors are chosen to be orthonormal eigenvectors. The matrix representation in this basis has the eigenvalues along the diagonal and zeroes everywhere else.

  \item When an eigenspace is more than one-dimensional (i.e. there are multiple eigenvectors associated with a particular eigenvalue) we say that it is \textbf{degenerate}.
\end{itemize}

\subsubsection{Adjoints and Hermitian operators}

\begin{itemize}
  \item If $A$ is any linear operator on a Hilbert space $V$, then there exists a unique linear operator $A^\dag$ on $V$ such that for all vectors $\ket{v}, \ket{w} \in V$, \[(\ket{v}, A \ket{w}) = (A^\dag \ket{v}, \ket{w}).\] This operator is known as the \textbf{adjoint} or \textbf{Hermitian conjugate} of the operator $A$. From definition it can be seen that $(A B)^\dag = B^\dag A^\dag$, $\ket{v}^\dag = \bra{v}$, and $(A \ket{v})^\dag = \bra{v} A^\dag$.

  \item In a matrix representation of an operator $A$, the action of the Hermitian conjugation operation is to take the conjugate transpose of its matrix $A^\dag = (A^*)^T$.

  \item An operator that is its own adjoint is known as \textbf{Hermitian} or \textbf{self-adjoint}.

  \item Suppose $W$ is a $k$-dimensional vector subspace of the $d$-dimensional vector space $V$. Using the Gram-Schmidt procedure it's possible to construct an orthonormal basis $\ket{1}, \ldots, \ket{d}$ for $V$ such that $\ket{1}, \ldots, \ket{k}$ is an orthonormal basis for $W$. For example, $\hat{x}, \hat{y}, \hat{z}$ is an orthonormal basis for $\mathbb{R}^3$ where $\hat{x}, \hat{y}$ is an orthonormal basis for the subspace $\mathbb{R}^2$.

        By definition, \[P = \sum_{i = 1}^k \ket{i} \bra{i}\] is the \textbf{projector} onto the subspace $W$ â€”Â it ``picks out'' only those components of the vector present in $W$. $\ket{v} \bra{v}$ is Hermitian for any vector $\ket{v}$ \[(\ket{v} \bra{v})^\dag = \bra{v}^\dag \ket{v}^\dag = \ket{v} \bra{v}\] and thus $P$ is also Hermitian $P^\dag = P$.

        The \textbf{orthogonal complement} of $P$ is the operator $Q = I - P$. This is a projector onto the vector space spanned by $\ket{k + 1}, \ldots, \ket{d}$. This can be seen as \[Q \ket{v} = (I - P) \ket{v} = I \ket{v} - P \ket{v},\] i.e. subtract from $\ket{v}$ the components present in $W$ leaving only those outside of $W$.

  \item An operator $A$ is said to be \textbf{normal} if $A A^\dag = A^\dag A$. An operator that is Hermitian is guaranteed to be normal.

  \item The \textbf{spectral decomposition theorem} states that any normal operator $M$ on a vector space $V$ is diagonal with respect to some orthonormal basis for $V$ and, conversely, any diagonalisable operator is normal.

  \item A matrix $U$ is said to be unitary if $U^\dag U = I$. Similarly an operator $U$ is unitary if $U^\dag U = I$. A unitary operator also satisfies $U U^\dag = I$ and therefore $U$ is normal and has a spectral decomposition.

  \item Unitary operators preserve inner products between vectors.

  \item A \textbf{positive operator} $A$ is defined to be an operator such that for any vector $\ket{v}$, $(\ket{v}, A \ket{v})$ is a real, non-negative number. If $(\ket{v}, A \ket{v})$ is strictly greater than zero for all $\ket{v} \ne 0$ then we say $A$ is \textbf{positive definite}.
\end{itemize}

\subsubsection{Tensor Products}

\begin{itemize}
  \item Suppose $V$ and $W$ are vector spaces of dimension $m$ and $n$ respectively. Then $V \otimes W$ (read ``$V$ tensor $W$'') is an $m n$ dimensional vector space. The elements of $V \otimes W$ are linear combinations of ``tensor products'' $\ket{v} \otimes \ket{w}$ of elements $\ket{v}$ of $V$ and $\ket{w}$ of $W$. In particular, if $\ket{i}$ and $\ket{j}$ are orthonormal bases for the spaces $V$ and $W$ then $\ket{i} \otimes \ket{j}$ is a basis for $V \otimes W$. We often use the abbreviated notations $\ket{v} \ket{w}$, $\ket{v, w}$, and $\ket{v w}$ for $\ket{v} \otimes \ket{w}$.

  \item The tensor product satisfies the following properties:

        \begin{enumerate}
          \item For an arbitrary scalar $z$ and elements $\ket{v}$ of $V$ and $\ket{w}$ of $W$, \[z (\ket{v} \otimes \ket{w}) = (z \ket{v}) \otimes \ket{w} = \ket{v} \otimes (z \ket{w}).\]

          \item For arbitrary $\ket{v_1}$ and $\ket{v_2}$ in $V$ and $\ket{w}$ in $W$, \[(\ket{v_1} + \ket{v_2}) \otimes \ket{w} = \ket{v_1} \otimes \ket{w} + \ket{v_2} \otimes \ket{w}.\]

          \item For arbitrary $\ket{v}$ in $V$ and $\ket{w_1}$ and $\ket{w_2}$ in $W$, \[\ket{v} \otimes (\ket{w_1} + \ket{w_2}) = \ket{v} \otimes \ket{w_1} + \ket{v} \otimes \ket{w_2}.\]
        \end{enumerate}

  \item Suppose $\ket{v}$ and $\ket{w}$ are vectors in $V$ and $W$, and $A$ and $B$ are linear operators on $V$ and $W$, respectively. Then we can define a linear operator $A \otimes B$ on $V \otimes W$ by the equation \[(A \otimes B) (\ket{v} \otimes \ket{w}) = A \ket{v} \otimes B \ket{w}.\]

  \item An arbitrary linear operator $C$ mapping $V \otimes W$ to $V' \otimes W'$ can be represented as a linear combination of tensor products of operators mapping $V$ to $V'$ and $W$ to $W'$, \[C = \sum_i c_i A_i \otimes B_i.\]

  \item The inner product on $V \otimes W$ is defined as \[\left( \sum_i a_i \ket{v_i} \otimes \ket{w_i}, \sum_j b_j \ket{v_j'} \otimes \ket{w_j'} \right) = \sum_{i j} a_i^* b_j \braket{v_i | v_j'} \braket{w_i | w_j'}.\]

  \item Suppose $A$ is an $m$ by $n$ matrix and $B$ is a $p$ by $q$ matrix. \textbf{Kronecker product} representation of the tensor product is defined as \[A \otimes B = \begin{bmatrix}
            A_{1 1} B & A_{1 2} B & \cdots & A_{1 n} B \\
            A_{2 1} B & A_{2 2} B & \cdots & A_{b n} B \\
            \vdots    & \vdots    & \vdots & \vdots    \\
            A_{m 1} B & A_{m 2} B & \cdots & A_{m n} B
          \end{bmatrix}\] where the terms $A_{i j} B$ represent a $p$ by $q$ submatrices.

        For example, the tensor product of the Pauli matrices $X$ and $Y$ is \[X \otimes Y = \begin{bmatrix}
            0 Y & 1 Y \\
            1 Y & 0 Y
          \end{bmatrix} = \begin{bmatrix}
            0 & 0  & 0 & -i \\
            0 & 0  & i & 0  \\
            0 & -i & 0 & 0  \\
            i & 0  & 0 & 0
          \end{bmatrix}.\]

  \item The notation $\ket{\psi}^{\otimes k}$ means $\ket{\psi}$ tensored with itself $k$ times.
\end{itemize}

\subsubsection{Operator functions}

\begin{itemize}
  \item Generally speaking, given a function $f : \mathbb{C} \rightarrow \mathbb{C}$, it is possible to define a corresponding matrix function on normal matrices (or some subclass such as the Hermitian matrices). Let \[A = \sum_i \lambda_i \ket{i} \bra{i}\] be the spectral decomposition for a normal operator $A$. Then, keeping in mind that $\ket{i} \bra{i}$ is a matrix then \[f(A) = \sum_i f(i) \ket{i} \bra{i}.\] For example, \[e^{\theta Z} = \begin{bmatrix}
            e^\theta & 0           \\
            0        & e^{-\theta}
          \end{bmatrix}.\]

  \item The \textbf{trace} of a matrix $A$ is the sum of its diagonal elements \[\tr A = \sum_i A_{i i}.\]

  \item The trace is cyclic $\tr (A B) = \tr (B A)$ and linear $\tr (A + B) = \tr A + \tr B$, $\tr (z A) = z \tr A$. From the cyclic property it can be seen that the trace is invariant under the unitary similarity transformation $A \rightarrow U A U^\dag$ as $\tr (U A U^\dag) = \tr (U^\dag U A) = \tr A$.

  \item The trace of an operator $A$ is defined as the trace of any matrix representation of $A$ under any orthonormal basis \[\tr A = \sum_i \braket{i | A | i}.\]

  \item To evaluate $\tr (A \ket{\psi} \bra{\psi})$ we can use the Gram-Schmidt procedure to express $\ket{\psi}$ in an orthonormal basis $\ket{i}$ that includes the normalised $\ket{\psi}$ as an element \begin{align*}
          \tr (A \ket{\psi} \bra{\psi}) & = \sum_i \braket{i | (A \ket{\psi} \bra{\psi}) | i} \\
                                        & = \sum_i \braket{i | A | \psi} \braket{\psi | i}    \\
                                        & = \braket{\psi | A | \psi}.
        \end{align*}
\end{itemize}

\subsubsection{The commutator and anti-commutator}

\begin{itemize}
  \item The \textbf{commutator} of two operators $A$ and $B$ is defined to be \[[A, B] = A B - B A.\] If $[A, B] = 0$ then we say $A$ commutes with $B$.

  \item The \textbf{anti-commutator} of two operators $A$ and $B$ is defined to be \[\{A, B\} = A B + B A.\] If $\{A, B\} = 0$ we say $A$ anti-commutes with $B$.

  \item The \textbf{simultaneous diagonalisation theorem} states that, if $A$ and $B$ are Hermitian operators, then $[A, B] = 0$ if and only if there exists an orthonormal basis such that both $A$ and $B$ are diagonal with respect to that basis. That is, they share a common set of eigenvectors in that basis.
\end{itemize}

\subsubsection{The polar and singular value decompositions}

\begin{itemize}
  \item If $A$ is a linear operator on a vector space $V$ then there exists unitary $U$ and positive operators $J$ and $K$ such that \[A = U J = K U\] where the unique positive operators $J$ and $K$ satisfying these equations are defined by $J = \sqrt{A^\dag A}$ and $K = \sqrt{A A^\dag}$. Moveover, if $A$ is invertible then $U$ is unique.

  \item If $A$ is a square matrix then there exist unitary matrices $U$ and $V$, and a diagonal matrix $D$ with non-negative entries such that \[A = U D V.\] The diagonal elements of $D$ are called the singular values of $A$.
\end{itemize}

\subsection{The postulates of quantum mechanics}

\subsubsection{State space}

\begin{itemize}
  \item Associated to any isolated physical system is a complex vector space with inner product (i.e. a Hilbert space) known as the \textbf{state space} of the system. The system is completely described by its \textbf{state vector}, which is a unit vector in the system's state space.

  \item The simplest quantum mechanical system is the qubit. It has a two-dimensional state space where $\ket{0}$ and $\ket{1}$ forn an orthonormal basis for the state space.

  \item An arbitrary state vector in this state space can be written \[\ket{\psi} = a \ket{0} + b \ket{1}.\] The requirement that $\ket{\psi}$ be a unit vector means that \[|a|^2 + |b|^2 = 1.\]
\end{itemize}

\subsubsection{Evolution}

\begin{itemize}
  \item The evolution of a closed quantum system is described by a \textbf{unitary transformation}. That is, the state $\ket{\psi}$ of the system at time $t_1$ is related to the state $\ket{\psi'}$ of the system at time $t_2$ by a unitary operator $U$ which depends only on the times $t_1$ and $t_2$, \[\ket{\psi'} = U \ket{\psi}.\] As a reminder, a unitary operator is one where \[U^\dagger U = U U^\dagger = I.\]

  \item The time evolution of the state of a closed quantum system is described by the \textbf{SchrÃ¶dinger equation}, \[i \hbar \frac{d \ket{\psi}}{d t} = H \ket{\psi}\] where $\hbar$ is \textbf{Planck's constant}, $H$ is a Hermitian operator known as the \textbf{Hamiltonian} of the system.

  \item Because the Hamiltonian is a Hermitian operator it has a spectral decomposition \[H = \sum_E E \ket{E} \bra{E}\] with eigenvalues $E$ and normalised eigenvectors $\ket{E}$. The states $\ket{E}$ are known as the \textbf{energy eigenstates} and $E$ is the energy of the state $\ket{E}$.
\end{itemize}

\end{document}