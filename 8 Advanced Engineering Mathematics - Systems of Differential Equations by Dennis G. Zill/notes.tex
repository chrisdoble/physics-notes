\documentclass{article}
\usepackage{amsfonts} % For \mathbb
\usepackage{amsmath} % For align*
\usepackage{enumitem} % For customisable list labels
\usepackage{graphicx} % For images
\usepackage{siunitx} % For units
\graphicspath{{./images/}}

\renewcommand{\Im}{\operatorname{Im}}
\renewcommand{\Re}{\operatorname{Re}}

\title{Advanced Engineering Mathematics Systems of Differential Equations by Dennis G. Zill Notes}
\author{Chris Doble}
\date{August 2023}

\begin{document}

\maketitle

\tableofcontents

\setcounter{section}{9}
\section{Systems of Linear Differential Equations}

\subsection{Theory of Linear Systems}

\begin{itemize}
  \item A system of the form \begin{align*}
          \frac{d x_1}{d t} & = g_1(t, x_1, x_2, \ldots, x_n) \\
          \frac{d x_2}{d t} & = g_2(t, x_1, x_2, \ldots, x_n) \\
          \vdots                                              \\
          \frac{d x_n}{d t} & = g_n(t, x_1, x_2, \ldots, x_n)
        \end{align*} is called a \textbf{first-order system}.

  \item When each of the functions $g_n(t, x_1, x_2, \ldots, x_n)$ is linear in the dependent variables $x_1, x_2, \ldots, x_n$, we get the \textbf{normal form} of a first-order system of linear equations \begin{align*}
          \frac{d x_1}{d t} & = a_{11}(t) x_1 + a_{12}(t) x_2 + \ldots + a_{1n}(t) x_n + f_1(t)  \\
          \frac{d x_2}{d t} & = a_{21}(t) x_1 + a_{22}(t) x_2 + \ldots + a_{2n}(t) x_n + f_2(t)  \\
          \vdots                                                                                 \\
          \frac{d x_n}{d t} & = a_{n1}(t) x_1 + a_{n2}(t) x_2 + \ldots + a_{nn}(t) x_n + f_n(t). \\
        \end{align*} Such a system is called a \textbf{linear system}.

  \item When $f_i(t) = 0$ for $i = 1, 2, \ldots, n$ the linear system is said to be \textbf{homogeneous}, otherwise it's \textbf{nonhomogenous}.

  \item If $\mathbf{X}$, $\mathbf{A}(t)$, and $\mathbf{F}(t)$ denote the matrices \begin{align*}
          \mathbf{X}    & = \begin{pmatrix}
                              x_1(t) \\
                              x_2(t) \\
                              \vdots \\
                              x_n(t)
                            \end{pmatrix}                             \\
          \mathbf{A}(t) & = \begin{pmatrix}
                              a_{11}(t) & a_{12}(t) & \ldots & a_{1n}(t) \\
                              a_{21}(t) & a_{22}(t) & \ldots & a_{2n}(t) \\
                              \vdots    &           &        & \vdots    \\
                              a_{n1}(t) & a_{n2}(t) & \ldots & a_{nn}(t)
                            \end{pmatrix} \\
          \mathbf{F}(t) & = \begin{pmatrix}
                              f_1(t) \\
                              f_2(t) \\
                              \vdots \\
                              f_n(t)
                            \end{pmatrix}
        \end{align*} then homogeneous linear systems can be written \[\mathbf{X}' = \mathbf{A} \mathbf{X}\] and nonhomogeneous linear systems can be written \[\mathbf{X}' = \mathbf{A} \mathbf{X} + \mathbf{F}.\]

  \item A \textbf{solution vector} on an interval $I$ is any column matrix \[\mathbf{X} = \begin{pmatrix}
            x_1(t) \\
            x_2(t) \\
            \vdots \\
            x_n(t)
          \end{pmatrix}\] whose entries are differentiable functions satisfying the linear system on the interval.

  \item The entries of a solution vector can be considered a set of parametric equations that define a curve in $n$-space. Such a curve is called a \textbf{trajectory}.

  \item The problem of solving \[\mathbf{X}' = \mathbf{A}(t) \mathbf{X} + \mathbf{F}(t)\] subject to \[\mathbf{X}(t_0) = \mathbf{X}_0\] is an \textbf{initial value problem} in matrix form.

  \item The \textbf{superposition principle} states that if $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ are solution vectors of a homogeneous linear system on an interval $I$, then \[\mathbf{X} = c_1 \mathbf{X}_1 + c_2 \mathbf{X}_2 + \ldots + c_n \mathbf{X}_n\] where $c_n$ are arbitrary constants is also a solution.

  \item If $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ are a set of solution vectors of a homogeneous linear system on an interval $I$, the set is said to be \textbf{linearly dependent} if there exist constants $c_1, c_2, \ldots, c_n$ not all zero such that \[c_1 \mathbf{X}_1 + c_2 \mathbf{X}_2 + \ldots + x_n \mathbf{X}_n = \mathbf{0}\] for every $t$ in the interval. Otherwise the set is said to be \textbf{linearly independent}.

  \item A set of solution vectors \[\mathbf{X}_1 = \begin{pmatrix}
            x_{11} \\
            x_{21} \\
            \vdots \\
            x_{n1}
          \end{pmatrix}, \quad \mathbf{X}_2 = \begin{pmatrix}
            x_{12} \\
            x_{22} \\
            \vdots \\
            x_{n2}
          \end{pmatrix}, \quad \ldots, \quad \mathbf{X}_n = \begin{pmatrix}
            x_{1n} \\
            x_{2n} \\
            \vdots \\
            x_{nn}
          \end{pmatrix}\] is linearly independent on an interval $I$ if the \textbf{Wronskian} \[W(\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n) = \begin{vmatrix}
            x_{11} & x_{12} & \ldots & x_{1n} \\
            x_{21} & x_{22} & \ldots & x_{2n} \\
            \vdots &        &        & \vdots \\
            x_{n1} & x_{n2} & \ldots & x_{nn}
          \end{vmatrix} \ne 0\] for every $t$ in the interval.

  \item Any set of $n$ linearly independent solution vectors of a homogeneous linear system on an interval $I$ is said to be a \textbf{fundamental set of solutions} on that interval.

  \item If $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ are a fundamental set of solutions of a homogeneous linear system on an interval $I$, then the \textbf{general solution} of the system on that interval is \[\mathbf{X} = c_1 \mathbf{X}_1 + c_2 \mathbf{X}_2 + \ldots + c_n \mathbf{X}_n\] where $c_i$ are arbitrary constants.

  \item For nonhomogenous systems, a \textbf{particular solution} $\mathbf{X}_p$ on an interval $I$ is any vector, free from arbitrary parameters, whose entries are functions that satify the system.

  \item For nonhomogeneous systems, the \textbf{general solution} of the system on the interval is \[\mathbf{X} = \mathbf{X}_c + \mathbf{X}_p\] where $\mathbf{X}_c$ is the general solution of the associated homogeneous system (the \textbf{complementary function}) and $\mathbf{X}_p$ is a particular solution of the nonhomogeneous system.
\end{itemize}

\subsection{Homogeneous Linear Systems}

\subsubsection{Distinct Real Eigenvalues}

\begin{itemize}
  \item If $\mathbf{X}' = \mathbf{A} \mathbf{X}$ is a homogeneous linear system, $\lambda_1, \lambda_2, \ldots, \lambda_n$ are $n$ real, distinct eigenvalues of $\mathbf{A}$, and $\mathbf{K}_1, \mathbf{K}_2, \ldots, \mathbf{K}_n$ are the corresponding eigenvectors of $\mathbf{A}$, then \[\mathbf{X} = c_1 \mathbf{K}_1 e^{\lambda_1 t} + c_2 \mathbf{K}_2 e^{\lambda_2 t} + \ldots + c_n \mathbf{K}_n e^{\lambda_n t}\] is the general solution of the system.

  \item If a system of linear equations consists of variables $x$ and $y$, then the $x-y$ plane is called the \textbf{phase plane}.

  \item Solution vectors of a linear system can be considered parametric equations and plotted on the phase plane. These are called trajectories.

  \item When multiple trajectories are plotted in the phase plane, it's called a \textbf{phase portrait}.
\end{itemize}

\subsubsection{Repeated Eigenvalues}

\begin{itemize}
  \item If the coefficient matrix $\mathbf{A}$ of a linear system has an eigenvalue $\lambda$ of multiplicity $m$, it may be possible to find $m$ linearly independent eigenvectors $\mathbf{K}_1, \mathbf{K}_2, \ldots, \mathbf{K}_m$ associated with the eigenvalue in which case the $m$ solution vectors associated with the eigenvalue are \begin{align*}
          \mathbf{X}_1 & = \mathbf{K}_1 e^{\lambda t}  \\
          \mathbf{X}_2 & = \mathbf{K}_2 e^{\lambda t}  \\
          \vdots                                       \\
          \mathbf{X}_m & = \mathbf{K}_m e^{\lambda t}.
        \end{align*}

  \item If the coefficient matrix $\mathbf{A}$ of a linear system has an eigenvalue $\lambda$ of multiplicity $m$ and it's not possible to find $m$ linearly independent eigenvectors associated with the eigenvalue, then the $m$ solution vectors associated with the eigenvalue are \begin{align*}
          \mathbf{X}_1 & = \mathbf{K}_1 e^{\lambda t}                                                                                                                          \\
          \mathbf{X}_2 & = \mathbf{K}_1 t e^{\lambda t} + \mathbf{K_2} e^{\lambda t}                                                                                           \\
          \vdots                                                                                                                                                               \\
          \mathbf{X}_m & = \mathbf{K}_1 \frac{t^{m - 1}}{(m - 1)!} e^{\lambda t} + \mathbf{K}_2 \frac{t^{m - 2}}{(m - 2)!} e^{\lambda t} + \ldots + \mathbf{K}_m e^{\lambda t}
        \end{align*} where $\mathbf{K}_i$ are the solutions to the equations \begin{align*}
          (\mathbf{A} - \lambda \mathbf{I}) \mathbf{K}_1 & = \mathbf{0}          \\
          (\mathbf{A} - \lambda \mathbf{I}) \mathbf{K}_2 & = \mathbf{K}_1        \\
          \vdots                                                                 \\
          (\mathbf{A} - \lambda \mathbf{I}) \mathbf{K}_m & = \mathbf{K}_{m - 1}. \\
        \end{align*}
\end{itemize}

\subsubsection{Complex Eigenvalues}

\begin{itemize}
  \item If $\mathbf{A}$ is the coefficient matrix of a homogeneous linear system and it has a complex eigenvalue $\lambda = \alpha + i \beta$ and associated eigenvector $\mathbf{K}_1$, then \[\mathbf{X}_1 = \mathbf{K}_1 e^{\lambda t} \text{ and } \mathbf{X}_2 = \overline{\mathbf{K}}_1 e^{\overline{\lambda} t}\] are solutions of the system.

  \item The solutions above can be made real by writing them as \begin{align*}
          \mathbf{X}_1 & = [\mathbf{B}_1 \cos \beta t - \mathbf{B}_2 \sin \beta t] e^{\alpha t} \\
          \mathbf{X}_2 & = [\mathbf{B}_2 \cos \beta t + \mathbf{B}_1 \sin \beta t] e^{\alpha t}
        \end{align*} where $\mathbf{B}_1 = \Re (\mathbf{K}_1)$ and $\mathbf{B}_2 = \Im(\mathbf{K}_1)$.
\end{itemize}

\subsection{Solution by Diagonalization}

\begin{itemize}
  \item A homogeneous linear system $\mathbf{X}' = \mathbf{A} \mathbf{X}$ in which each $x_i'$ is expressed as a linear combination of $x_1, x_2, \ldots, x_n$ is said to be \textbf{coupled}. If each $x_i'$ is expressed solely in terms of $x_i$ the system is said to be \textbf{uncoupled}.

  \item Given a linear system $\mathbf{X' = A X}$, if the coefficient matrix $\mathbf{A}$ is diagonalisable such that $\mathbf{P^{-1} A P = D}$ then the system can be solved by:

        \begin{enumerate}
          \item Substituting $\mathbf{X = P Y}$ which gives $\mathbf{P Y' = A P Y}$ or \\ $\mathbf{Y' = P^{-1} A P Y = D Y}$

          \item Because $\mathbf{D}$ is a diagonal matrix with $\mathbf{A}$'s eigenvalues along the diagonal, this means the solutions to $\mathbf{Y' = D Y}$ are \[\mathbf{Y} = \begin{pmatrix}
                    c_1 e^{\lambda_1 t} \\
                    c_2 e^{\lambda_2 t} \\
                    \vdots              \\
                    c_n e^{\lambda_n t}
                  \end{pmatrix}\]

          \item These solutions can then be substituted into $\mathbf{X = P Y}$ to solve for $\mathbf{X}$
        \end{enumerate}
\end{itemize}

\subsection{Nonhomogeneous Linear Systems}

\subsubsection{Undetermined Coefficients}

\begin{itemize}
  \item The \textbf{method of undetermined coefficients} can be applied to a linear system $\mathbf{X' = A X + F(t)}$ when the entries of $\mathbf{A}$ are constants and the entries of $\mathbf{F}(t)$ are constants, polynomials, exponential functions, sines and cosines, or finite sums and products of these functions.

  \item To apply the method of undetermined coefficients:

        \begin{enumerate}
          \item Solve the associated homogeneous linear system to find the complementary function $\mathbf{X}_c$.

          \item Assume the particular solution $\mathbf{X}_p$ has the same form as $\mathbf{F}(t)$.

          \item Substitute the trial solution into the system and solve for the unknowns.

          \item The general solution is $\mathbf{X} = \mathbf{X_c + X_p}$.
        \end{enumerate}

  \item If $\mathbf{F}(t)$ contains a term that's present in the complementary function, that term needs to be adjusted (similar to how you multiply by $x^n$ in the method of undetermined coefficients for ODEs). The textbook doesn't cover the rules for this.
\end{itemize}

\subsubsection{Variation of Parameters}

\begin{itemize}
  \item If $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ is a fundamental set of solutions of the homogeneous linear system $\mathbf{X' = A X}$ on an interval $I$, then the general solution is \[\mathbf{X} = c_1 \mathbf{X}_1 + c_2 \mathbf{X}_2 + \ldots + c_n \mathbf{X}_n\] which can also be written \[\mathbf{X} = \mathbf{\Phi}(t) \mathbf{C} = \begin{pmatrix}
            \mathbf{X}_1 & \mathbf{X}_2 & \ldots & \mathbf{X}_n
          \end{pmatrix} \mathbf{C}\] where $\mathbf{\Phi}(t)$ is called a \textbf{fundamental matrix} and $\mathbf{C}$ is a column vector containing the arbitrary constants $c_1, c_2, \ldots, c_n$.

  \item A fundamental matrix:

        \begin{itemize}
          \item always has an inverse, and

          \item has the property that $\mathbf{\Phi}'(t) = \mathbf{A} \mathbf{\Phi}(t).$
        \end{itemize}

  \item The \textbf{method of variation of parameters} finds a particular solution to a nonhomogeneous linear system by replacing column vector of unknown constants $\mathbf{C}$ with a column vector of functions \[\mathbf{U}(t) = \begin{pmatrix}
            u_1(t) \\
            u_2(t) \\
            \vdots \\
            u_n(t)
          \end{pmatrix}\] such that $\mathbf{X}_p = \mathbf{\Phi}(t) \mathbf{U}(t)$ is a particular solution to the system.

  \item $\mathbf{U}(t)$ can be calculated as \[\mathbf{U}(t) = \int \mathbf{\Phi}^{-1}(t) \mathbf{F}(t) \,dt\] so \[\mathbf{X}_p = \mathbf{\Phi}(t) \int \mathbf{\Phi}^{-1}(t) \mathbf{F}(t) \,dt\] and \[\mathbf{X} = \mathbf{X}_c + \mathbf{X}_p = \mathbf{\Phi}(t) \mathbf{C} + \mathbf{\Phi}(t) \int \mathbf{\Phi}^{-1}(t) \mathbf{F}(t) \,dt.\]

  \item When solving initial value problems via the method of variation of parameters where you're given $\mathbf{X}(t_0) = \mathbf{X}_0$, the column vector of arbitrary constants $\mathbf{C}$ can be calculated as \[\mathbf{C} = \mathbf{\Phi}^{-1}(t_0) \mathbf{X}_0.\]
\end{itemize}

\subsubsection{Diagonalization}

\begin{itemize}
  \item If the coefficient matrix $\mathbf{A}$ in a nonhomogeneous linear system \\ $\mathbf{X' = A X + F}(t)$ is diagonalizable, the system can be solved by:

        \begin{enumerate}
          \item Substituting $\mathbf{X = P Y}$ which gives $\mathbf{P Y' = A P Y + F}(t)$ or \\ $\mathbf{Y' = P^{-1} A P Y + P^{-1} F}(t)$ or $\mathbf{Y' = D Y + G}$

          \item Because $\mathbf{D}$ is a diagonal matrix with $\mathbf{A}$'s eigenvalues along the diagonal and $\mathbf{G = P^{-1} F}(t)$ this means $\mathbf{Y' = D Y + G(t)}$ is a set of $n$ uncoupled equations of the form \[\begin{pmatrix}
                    y_1'   \\
                    y_2'   \\
                    \vdots \\
                    v_n'
                  \end{pmatrix} = \begin{pmatrix}
                    \lambda_1 y_1 + g_1(t) \\
                    \lambda_2 y_2 + g_2(t) \\
                    \vdots                 \\
                    \lambda_n y_n + g_n(t)
                  \end{pmatrix}\]

          \item These equations can be solved and substituted into $\mathbf{X = P Y}$ to solve for $\mathbf{X}$.
        \end{enumerate}
\end{itemize}

\end{document}