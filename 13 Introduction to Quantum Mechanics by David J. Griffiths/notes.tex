\documentclass{article}
\usepackage{amsfonts} % For mathbb
\usepackage{amsmath} % For align*
\usepackage{bookmark} % For links
\usepackage{braket} % For bra-ket notation
\usepackage{float} % For the [H] option on figures
\usepackage{graphicx} % For images
\usepackage{siunitx} % For units

\graphicspath{{./images/}}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue
}

\newcommand{\ev}[1]{\langle #1 \rangle}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}

\title{Introduction to Quantum Mechanics by David J. Griffiths Notes}
\author{Chris Doble}
\date{March 2024}

\begin{document}

\maketitle

\tableofcontents

\part{Theory}

\section{The Wave Function}

\subsection{The Schrödinger Equation}

\begin{itemize}
  \item The \textbf{Schrödinger equation} \[i \hbar \frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2 m} \frac{\partial^2 \Psi}{\partial x^2} + V \Psi\] is to quantum mechanics what Newton's second law is to classical mechanics. Given suitable initial conditions — typically $\Psi(x, 0)$ — the Schrödinger equation determines $\Psi(x, t)$ for all future time.
\end{itemize}

\subsection{The Statistical Interpretation}

\begin{itemize}
  \item The \textbf{Born rule} states that $|\Psi(x, t)|^2$ gives the probability of finding the particle at point $x$ at time $t$ or \[\int_a^b |\Psi(x, t)|^2 \,d x\] gives the probability of finding the particle between $a$ and $b$ at time $t$.

  \item This statistical interpretation introduces indeterminacy to quantum mechanics — we can't predict with certainty the particle's position.

  \item Suppose we measure a particle's position to be $C$. Where was it before we took the measurement? In the past there were three main schools of thought:

        \begin{enumerate}
          \item The \textbf{realist} position believes that the particle was at $C$ but $\Psi$ doesn't give us enough information to determine that — there's another \textbf{hidden variable} that would allow us to.

          \item The \textbf{orthodox} position (also known as the \textbf{Copenhagen interpretation}) believes that the particle didn't have a definite position but the act of measuring it forced it to do so.

          \item The \textbf{agnostic} position believes that it doesn't matter and is potentially unknowable.
        \end{enumerate}

  \item \textbf{Bell's theorem} confirms the orthodox interpretation.

  \item If we take two consecutive measurements of a particle, they will both yield the same result. The first measurement causes the wavefunction to \textbf{collapse} such that it is peaked only at the particle's measured location. If the system is allowed to evolve between the measurements the wavefunction will ``spread out'' but if done in quick succession the result won't change.
\end{itemize}

\subsection{Probability}

\subsubsection{Discrete Variables}

\begin{itemize}
  \item The average value of a discrete variable $j$ is \[\ev{j} = \frac{\sum j N(j)}{N} = \sum_{j = 0}^\infty j P(j)\] where $N$ is the size of the population, $N(j)$ is the number of $j$s in the population, and $P(j)$ is the probability of randomly selecting a $j$ from the population.

  \item In quantum mechanics the average is usually the quantity of interest and it is called the \textbf{expectation value} (even though it may not be the most probable value).

  \item The average value of a function $f$ of a discrete variable $j$ is \[\ev{f(j)} = \sum_{j = 0}^\infty f(j) P(j).\]

  \item Two distributions could have the same median, mean, mode, and size but be spread out differently. One way to quantify this could be to calculate how far each element is from the average \[\Delta j = j - \ev{j}\] and calculate the average of $\Delta j$. However, it ends up being zero \begin{align*}
          \langle \Delta j \rangle & = \sum (j - \ev{j}) P(j)         \\
                                   & = \sum j P(j) - \ev{j} \sum P(j) \\
                                   & = \ev{j} - \ev{j}                \\
                                   & = 0.
        \end{align*} To avoid this we calculate the average of the square of $\Delta j$ \[\sigma^2 = \langle (\Delta j)^2 \rangle\] which is known as the \textbf{variance} of the distribution.

  \item The square root of the variance is called the \textbf{standard deviation}.

  \item A useful theorem on variances is \begin{align*}
          \sigma^2 & = \langle (\Delta j)^2 \rangle                              \\
                   & = \sum (\Delta j)^2 P(j)                                    \\
                   & = \sum (j - \ev{j})^2 P(j)                                  \\
                   & = \sum (j^2 - 2 j \ev{j} + \ev{j}^2) P(j)                   \\
                   & = \sum j^2 P(j) - 2 \ev{j} \sum j P(j) + \ev{j}^2 \sum P(j) \\
                   & = \ev{j^2} - 2 \ev{j}^2 + \ev{j}^2                          \\
                   & = \ev{j^2} - \ev{j}^2
        \end{align*} and thus the standard deviation can be calculated as \[\sigma = \sqrt{\ev{j^2} - \ev{j}^2}\] which is usually easier than the full formula.
\end{itemize}

\subsubsection{Continuous Variables}

\begin{itemize}
  \item The equations above translate as expected to continuous variables: \begin{align*}
          P_{a b}   & = \int_a^b \rho(x) \,d x                     \\
          1         & = \int_{-\infty}^{+\infty} \rho(x) \,d x     \\
          \ev{x}    & = \int_{-\infty}^{+\infty} x \rho(x) \,d x   \\
          \ev{f(x)} & = \int{-\infty}^{+\infty} f(x) \rho(x) \,d x \\
          \sigma^2  & = \ev{x^2} - \ev{x}^2
        \end{align*}
\end{itemize}

\subsection{Normalization}

\begin{itemize}
  \item In order for the statistical interpretation of the wavefunction to make sense, it must be the case that \[\int_{-\infty}^\infty |\Psi(x, t)|^2 \,d x = 1,\] i.e. the particle must be somewhere.

  \item The process of multiplying a candidate wavefunction by a complex constant $A$ to make this hold is called \textbf{normalization}.

  \item For some solutions to the Schrödinger equation, the integral is infinite in which case there is no $A$ that normalizes the wavefunction. The same goes for $\Psi = 0$. Such \textbf{non-normalizable} solutions can't represent particles so they must be rejected.

  \item But if we normalize the wavefunction at $t = 0$, how do we know it stays normalized? It turns out that the integral \[\int_{-\infty}^\infty |\Psi(x, t)|^2 \,d x\] is constant (independent of time) so if it's normalized at $t = 0$ it stays normalized.
\end{itemize}

\subsection{Momentum}

\begin{itemize}
  \item What does \[\ev{x} = \int_{-\infty}^\infty x |\Psi(x, t)|^2 \,d x\] mean? It doesn't mean that if you measure the position of a particle over and over again and take the average of the results you'll get $\int x |\Psi|^2 \,d x$. The first measurement causes the wavefunction to collapse so you'll get the same measurement each time. Instead, it means if you have a large number of particles all in the same state $\Psi$, measure the position of each of them, and take the average of the results you'll get $\int x |\Psi|^2 \,d x$.

  \item The expectation value of the velocity is equal to the time derivative of the expectation value of the position \[\ev{v} = \frac{d \ev{x}}{d t} = -\frac{i \hbar}{m} \int \Psi^* \frac{\partial \Psi}{\partial x} \,d x\] however it is customary to work with momentum $p = m v$ rather than velocity \[\ev{p} = m \frac{d \ev{x}}{d t} = -i \hbar \int \Psi^* \frac{\partial \Psi}{\partial x} \,d x.\]

  \item The above can be rewritten in the form \begin{align*}
          \ev{x} & = \int \Psi^* [x] \Psi \,d x                                                  \\
          \ev{p} & = \int \Psi^* \left[ -i \hbar \frac{\partial}{\partial x} \right] \Psi \,d x.
        \end{align*} The values in the square brackets are called \textbf{operators} and we say that the operator $x$ ``represents'' position and the operator $-i \hbar (\partial / \partial x)$ ``represents'' momentum. To calculate expectation values we place the appropriate operator between $\Psi^*$ and $\Psi$ and integrate.

  \item All other values of interest can be expressed in terms of position and momentum, e.g. kinetic energy is \[T = \frac{1}{2} m v^2 = \frac{p^2}{2 m}.\] Their operators can be determined by substituting $p = -i \hbar (\partial / \partial x)$, e.g. kinetic energy is \[-\frac{\hbar^2}{2 m} \frac{\partial^2}{\partial x^2}\] such that \[\ev{T} = -\frac{\hbar^2}{2 m} \int \Psi^* \frac{\partial^2 \Psi}{\partial x^2} \,d x.\] In general \[\ev{Q(x, p)} = \int \Psi^* \left[ Q(x, -i \hbar \partial / \partial x) \right] \Psi \,d x.\]
\end{itemize}

\subsection{The Uncertainty Principle}

\begin{itemize}
  \item If a wave is spread out (e.g. if you shake a rope up and down repeatedly), its position isn't well defined but its wavelength is. On the other hand, if a wave is localised (e.g. if you shake a rope up and down once to make a spike), its position is well defined but its wavelength isn't.

  \item The momentum of a particle is related to its wavelength by the \textbf{de Broglie formula} \[p = \frac{h}{\lambda} = \frac{2 \pi \hbar}{\lambda}.\]

  \item If a wavefunction is periodic its position isn't well defined but its wavelength is and thus so is its momentum. If a wavefunction is localised its position is well defined but its wavelength isn't and thus neither is its momentum. This is formalised by the \textbf{Hiesenberg uncertainty principle} \[\sigma_x \sigma_p \ge \frac{\hbar}{2}\] where $\sigma_x$ and $\sigma_p$ are the standard deviations in $x$ and $p$, respectively.
\end{itemize}

\section{Time-Independent Schrödinger Equation}

\subsection{Stationary States}

\begin{itemize}
  \item The Schrödinger equation \[i \hbar \frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2 m} \frac{\partial^2 \Psi}{\partial x^2} + V \Psi\] can be solved via separation of variables if $V$ is independent of $t$. In that case we assume $\Psi(x, t) = \psi(x) \varphi(t)$ and separation gives the two ODEs \[i \hbar \frac{1}{\varphi} \frac{d \varphi}{d t} = E\] which has no name and \[-\frac{\hbar^2}{2 m} \frac{\partial^2 \psi}{d x^2} + V \psi = E \psi\] which is called the \textbf{time-independent Schrödinger equation}. The solution to the former is \[\varphi(t) = C e^{-i E t / \hbar}\] but the constant $C$ can be absorbed into $\psi$. The latter can't be solved until $V$ is specified.

  \item While there are many solutions to the Schrödinger equation that don't have the form $\psi(x) \varphi(t)$, those that do have three interesting properties:

        \begin{enumerate}
          \item They are stationary states. The wave function has the form \[\Psi(x, t) = \psi(x) e^{-i E t / \hbar}\] so the probability density is \[|\Psi|^2 = \Psi^* \Psi = \psi^* e^{i E t / \hbar} \psi e^{-i E t / \hbar} = |\psi|^2,\] i.e. it's constant in time. Similarly, time dependence drops out of the expectation value formula so they're also constant in time.

          \item They are states of definite total energy. In classical mechanics the total energy is called the Hamiltonian \[H(x, p) = \frac{p^2}{2 m} + V(x).\] The equivalent quantum operator is \[\hat{H} = -\frac{\hbar^2}{2 m} \frac{\partial^2}{\partial x^2} + V(x)\] thus the time-independent Schrödinger equation can be written \[\hat{H} \psi = E \psi\] and the expectation value of the total energy can be written \[\ev{H} = \int \psi^* \hat{H} \psi \,d x = E \int |\psi|^2 \,d x = E \int |\Psi|^2 \,d x = E.\] Moreover, \[\hat{H}^2 \psi = \hat{H} (\hat{H} \psi) = \hat{H} (E \psi) = E (\hat{H} \psi) = E^2 \psi\] so \[\ev{H^2} = \int \psi^* \hat{H}^2 \psi \,d x = E^2 \int |\psi|^2 \,d x = E^2 \int |\Psi|^2 \,dx = E^2.\] Thus the variance of $H$ is \[\sigma_H^2 = \ev{H^2} - \ev{H}^2 = 0\] meaning every measurement of the total energy of the system will return the same value $E$.

          \item The general solution to the time-dependent Schrödinger equation is a linear combination of separable solutions. The time-independent Schrödinger equation yields an infinite number of solutions $\psi_1(x)$, $\psi_2(x)$, $\ldots$, $\psi_n(x)$ each with its associated separation constant $E_1$, $E_2$, $\ldots$, $E_n$ — one for each \textbf{allowed energy}. The general solution is then given by \[\Psi(x, t) = \sum_{n = 1}^\infty c_n \psi_n(x) e^{-i E_n t / \hbar}\] with appropriate choice of $c_n$ to fit the initial conditions.
        \end{enumerate}

  \item The general approach to solving the time-dependent Schrödinger equation with given initial conditions and potential energy function is:

        \begin{enumerate}
          \item Solve the time-independent Schrödinger equation to get an infinite set of solutions $\psi_n(x)$.

          \item Fit the linear combination of those solutions \[\Psi(x, 0) = \sum_{n = 1}^\infty c_n \psi_n(x)\] to the initial conditions.

          \item Multiply each term in the sum by its associated time dependence \[\Psi(x, t) = \sum_{n = 1}^\infty c_n \psi_n(x) e^{-i E_n t / \hbar} = \sum_{n = 1}^\infty c_n \Psi_n(x, t).\]
        \end{enumerate}

  \item The physical meaning of the constants $c_n$ is thus: $|c_n|^2$ is the probability that a measurement of the system's energy would return the value $E_n$. This means that \[\sum_{n = 1}^\infty |c_n|^2 = 1\] and \[\ev{H} = \sum_{n = 1}^\infty |c_n|^2 E_n.\] The probability of measuring any particular energy is constant in time and thus the expectation value of $H$ — this is part of energy conservation.

  \item For a solution to the Schrödinger equation to be normalisable, $E$ must be real and must exceed the minimum value of $V$.
\end{itemize}

\subsection{The Infinite Square Well}

\begin{itemize}
  \item The potential of an infinite square well is \[V(x) = \begin{cases}
            0      & 0 \le x \le a    \\
            \infty & \text{otherwise}
          \end{cases}\] and thus $\Psi(x, t) = 0$ for $x$ outside $[0, a]$.

  \item Inside such a well the time-independent Schrödinger equation reads \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} = E \Psi\] where $E$ must be real and greater than $0$. This can be rearranged to \[\frac{d^2 \psi}{d x^2} = -k^2 \psi\] where \[k = \frac{\sqrt{2 m E}}{\hbar}.\] This has the form of a simple harmonic oscillator and has the solution \[\psi = A \sin (k x) + B \cos (k x).\] Applying the boundary condition $\psi(0) = 0$ gives $B = 0$ and thus \[\psi = A \sin (k x).\] Applying the boundary condition $\psi(a) = 0$ gives $A \sin (k a) = 0$. We must reject $A = 0$ as that would result in the non-normalisable solution $\psi = 0$, so $\sin (k a) = 0$ and thus $k a = 0, \,\pm \pi, \,\pm 2 \pi, \,\ldots$ Again, $k = 0$ results in $\psi = 0$ so it must be rejected and we're left with \[\psi = A \sin (k_n x)\] where \[k_n = \frac{n \pi}{a}, \,n \in \mathbb{Z}^+.\]

  \item From the above, the possible values of $E$ are \[E_n = \frac{\hbar^2 k_n^2}{2 m} = \frac{n^2 \pi^2 \hbar^2}{2 m a^2}, \,n \in \mathbb{Z}^+.\] In other words, unlike in classical mechanics, a quantum particle in an infinite square well can't have any energy — it must be one of these values.

  \item To determine the value of the constant $A$ above we normalise $\psi$ \[\int_0^a |A|^2 \sin^2 (k x) \,d x = |A|^2 \frac{a}{2} = 1.\] This only determines the magnitude of $A$ but the phase doesn't affect anything so we might as well pick \[A = \sqrt{\frac{2}{a}}\] and the solutions to the time-independent Schrödinger equation inside the infinite square well are \[\psi_n(x) = \sqrt{\frac{2}{a}} \sin \left( \frac{n \pi}{a} x \right).\]

  \item The solutions $\psi_n$ have some interesting properties:

        \begin{enumerate}
          \item They alternate even and odd with respect to the centre of the well.

          \item Each subsequent solution has one more node (crossing of the axis).

          \item They are mutually orthogonal, i.e. \[\int \psi_m^* \psi_n \,d x = \begin{cases}
                    0 & m \ne n \\
                    1 & m = n
                  \end{cases}.\]

          \item The are complete, in that any other function $f(x)$ can be expressed as a linear combination of them \[f(x) = \sum_{n = 1}^\infty c_n \psi_n(x) = \sqrt{\frac{2}{a}} \sum_{n = 1}^\infty c_n \sin \left( \frac{n \pi}{a} x \right)\] where \[c_n = \int \psi_n(x)^* f(x) \,d x.\]
        \end{enumerate}

  \item The stationary states of the infinite square well are thus \[\Psi_n(x, t) = \sqrt{\frac{2}{a}} \sin \left( \frac{n \pi}{a} x \right) e^{-i (n^2 \pi^2 \hbar / 2 m a^2) t}\] and the general solution to the time-dependent Schrödinger equation is \[\Psi(x, t) = \sum_{n = 1}^\infty c_n \Psi_n(x, t)\] where $c_n$ are given by \[c_n = \sqrt{\frac{2}{a}} \int_0^a \sin \left( \frac{n \pi}{a} x \right) \Psi(x, 0) \,d x.\]
\end{itemize}

\subsection{The Harmonic Oscillator}

\begin{itemize}
  \item The equation of motion for a harmonic oscillator is \[\frac{d^2 x}{d t^2} = -k x\] where $k$ is the force constant and $x$ is the displacement from equilibrium. The solution to this differential equation is \[x = A \sin (\omega t) + B \cos (\omega t)\] where $\omega = \sqrt{k / m}$ is the angular frequency of oscillation.

  \item The potential energy of a harmonic oscillator is \[V(x) = \frac{1}{2} k x^2\] which is a parabola.

  \item Any potential function can be approximated by a parabola in the neighborhood of a local minimum $x_0$ providing the amplitude is small by expanding it as a Taylor series, dropping the first term as it's constant and doesn't affect the force, dropping the second term because $V'(x_0) = 0$, keeping the third term, and dropping higher order terms.

  \item The quantum problem is to solve the Schrödinger equation for potential \[V(x) = \frac{1}{2} m \omega^2 x^2\] where $k$ has been replaced with $m \omega^2$. The first step is to solve the time-independent Schrödinger equation: \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} + \frac{1}{2} m \omega^2 x^2 \psi = E \psi.\] This is typically done in one of two ways: a clever algebraic technique using ladder-operators, and a more straightforward ``brute-force'' technique using power series.
\end{itemize}

\subsubsection{Algebraic Method}

\begin{itemize}
  \item We can factor the time-independent Schrödinger equation above as \[\frac{1}{2 m} \left[ -\hbar^2 \frac{d^2}{d x^2} + (m \omega x)^2 \right] \psi = \frac{1}{2 m} [\hat{p}^2 + (m \omega x)^2] = E \psi.\] As the Hamiltonian is equal to the total energy of the system, this means \[\hat{H} = \frac{1}{2 m} [\hat{p}^2 + (m \omega x)^2].\]

  \item This form invites factorisation of the form $u^2 + v^2 = (i u + v) (-i u + v)$ however $\hat{p}$ and $x$ are operators and they do not, in general, commute (i.e. $\hat{p} x \ne x \hat{p}$). We can try anyway by introducing the quantities \[\hat{a}_\pm = \frac{1}{\sqrt{2 \hbar m \omega}} (\mp i \hat{p} + m \omega x).\] Their product is \begin{align*}
          \hat{a}_- \hat{a}+ & = \frac{1}{2 \hbar m \omega} (i \hat{p} + m \omega x) (-i \hat{p} + m \omega x)                 \\
                             & = \frac{1}{2 \hbar m \omega} [\hat{p}^2 + (m \omega x)^2 - i m \omega (x \hat{p} - \hat{p} x)].
        \end{align*} The extra term involving $x \hat{p} - \hat{p} x$ is called the \textbf{commutator} of $x$ and $\hat{p}$ — it's a measure of how badly they fail to commute.

  \item In general, the commutator of two operators $\hat{A}$ and $\hat{B}$ is \[[\hat{A}, \hat{B}] = \hat{A} \hat{B} - \hat{B} \hat{A}.\] It's often easier to calculate the commutator of two operators if you introduce a test function $f$, expand, and remove $f$ at the end.

  \item Using this notation, the product above can be rewritten \[\hat{a}_- \hat{a}_+ = \frac{1}{2 \hbar m \omega} [\hat{p}^2 + (m \omega x)^2] - \frac{i}{2 \hbar} [x, \hat{p}].\]

  \item It turns out that \[[x, \hat{p}] = i \hbar.\] This equation is called the \textbf{canonical commutation relation} and using it we find \[\hat{a}_- \hat{a}_+ = \frac{1}{\hbar \omega} \hat{H} + \frac{1}{2}\] or \[\hat{H} = \hbar \omega \left( \hat{a}_- \hat{a}_+ - \frac{1}{2} \right).\]

  \item Note that the order of the operators is important. Going through the same process for $\hat{a}_+ \hat{a}_-$ gives \[\hat{a}_+ \hat{a}_- = \frac{1}{\hbar \omega} \hat{H} - \frac{1}{2}\] and \[\hat{H} = \hbar \omega \left( \hat{a}_+ \hat{a}_- + \frac{1}{2} \right).\]

  \item If $\psi$ satisfies the Schrödinger equation with energy $E$, i.e. $\hat{H} \psi = E \psi$, then $\hat{a}_+ \psi$ satisfies it with energy $E + h \omega$ \[\hat{H} (\hat{a}_+ \psi) = (E + h \omega) \psi\] and $\hat{a}_- \psi$ satisfies it with energy $E - h \omega$ \[\hat{H} (\hat{a}_- \psi) = (E - h \omega) \psi.\]

  \item $\hat{a}_\pm$ are called \textbf{ladder operators} because, if we know one solution to the Schrödinger equation, they can be used to climb up and down the energy ladder.

  \item However, we can't apply the lowering operator $\hat{a}_-$ an infinite number of times — eventually we'll reach a state of zero energy. The ``lowest rung'' $\psi_0$ occurs when $\hat{a}_- \psi_0 = 0$. Solving this equation we find \[\psi_0(x) = \left( \frac{m \omega}{\pi \hbar} \right)^{1 / 4} e^{-\frac{m \omega}{2 \hbar} x^2}\] and \[E_0 = \frac{1}{2} \hbar \omega.\]

  \item From here we can apply the raising operator $\hat{a}_+$ to find higher energy states \[\psi_n(x) = \frac{1}{\sqrt{n!}} (\hat{a}_+)^n \psi_0 \text{ with } E_n = \left( n + \frac{1}{2} \right) \hbar \omega.\]

  \item In the case of the infinite square well, the stationary states of the harmonic oscillator are orthogonal \[\int_{-\infty}^\infty \psi_m^* \psi_n \,d x = \delta_{m n}.\]
\end{itemize}

\subsubsection{Analytic Method}

\begin{itemize}
  \item By introducing the term \[\xi = \sqrt{\frac{m \omega}{\hbar}} x\] the Schrödinger equation for the harmonic oscillator \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} + \frac{1}{2} m \omega^2 x^2 \psi = E \psi\] can be rewritten \[\frac{d^2 \psi}{d \xi^2} = (\xi^2 - K) \psi\] where \[K = \frac{2 E}{\hbar \omega}\] is the energy of the system in units of $\hbar \omega / 2$.

  \item For large $\xi$ this has the approximate solution \[\psi(\xi) = h(\xi) e^{-\xi^2 / 2}.\] Substituting this into the Schrödinger equation gives \[\frac{d^2 h}{d \xi^2} - 2 \xi \frac{d h}{d \xi} + (K - 1) h = 0.\] If we assume the solution $h(\xi)$ is a power series in $\xi$ we get \[\sum_{j = 0}^\infty [(j + 1) (j + 2) a_{j + 2} - 2 j a_j + (K - 1) a_j] \xi^j = 0.\] By the uniqueness of power series expansions, the coefficient of each power of $\xi$ must be $0$ in order for the series overall to equal $0$ \[(j + 1) (j + 2) a_{j + 2} - 2 j a_j + (K - 1) a_j = 0.\] This gives a recurrence relation \[a_{j + 1} = \frac{2 j + 1 - K}{(j + 1) (j + 2)} a_j\] that can be used to generate the even- and odd-numbered coefficients by starting with $a_0$ and $a_1$, respectively.

  \item However, not all choices of $a_j$ result in normalisable wavefunctions. In order for them to be valid, the power series must terminate — there must be some ``highest'' $j$ (call it $n$) such that $a_{n + 2} = 0$. This will truncate \textbf{either} the even series or the odd series. The other one must be zero from the start — $a_1 = 0$ if $n$ is even, $a_0 = 0$ if $n$ is odd.

  \item In general $h_n(\xi)$ will be a polynomial of degree $n$ in $\xi$, involving even powers only if $n$ is even, and odd powers only if it's odd. Apart from the overall factor $a_0$ or $a_1$ these are the \textbf{Hermite polynomials, $\boldsymbol{H_n(\xi)}$}.

  \item By convention the arbitrary multiplicative factor is chosen such that the coefficient of the highest power of $\xi$ is $2^n$. With this convention, the normalised stationary states for the harmonic oscillator are \[\psi_n = \left( \frac{m \omega}{\pi \hbar} \right)^{1 / 4} \frac{1}{\sqrt{2^n n!}} H_n(\xi) e^{-\xi^2 / 2}.\]
\end{itemize}

\subsection{The Free Particle}

\begin{itemize}
  \item For a free particle, $V = 0$ everywhere. This gives a time-independent Schrödinger equation of \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} = E \psi\] or \[\frac{d^2 \psi}{d x^2} = -k^2 \psi \text{ where } k = \frac{\sqrt{2 m E}}{\hbar}\] the solution to which is \[\psi(x) = A e^{i k x} + B e^{-i k x}.\]

  \item Introducing the standard time dependence gives \[\Psi(x, t) = A e^{i k \left( x - \frac{\hbar k}{2 m} t \right)} + B e^{-i k \left( x + \frac{\hbar k}{2 m} t \right)}.\] The first term represents a wave travelling to the left and the second term represents a wave travellign to the right. We can combine them \[\Psi_k(x, t) = A e^{i \left( k x - \frac{\hbar k^2}{2 m} t \right)}\] if we let $k$ go negative \[k = \pm \frac{\sqrt{2 m E}}{\hbar} \text{ with } \begin{cases}
            k > 0 & \text{travelling to the right,} \\
            k < 0 & \text{travelling to the left.}
          \end{cases}\]

  \item Unfortunately this wavefunction can't be normalised \[\int_{-\infty}^\infty \Psi_k^* \Psi_k = |A|^2 \int_{-\infty}^\infty \,d x = |A|^2 (\infty).\] This means that it doesn't represent a physically realisable state. In other words, there's no such thing as a free particle with a definite energy.

  \item The solution is still a linear combination of separable solutions, but it's an integral over the continuous variable $k$ instead of a sum over the discrete variable $n$ \[\Psi(x, t) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \phi(k) e^{i \left( k x - \frac{\hbar k^2}{2 m} t \right)} \,d k\] where $(1 / \sqrt{2 \pi}) \phi(k) \,d k$ plays the role of the coefficient $c_n$ in previous wavefunctions. This wavefunction can be normalised but it carries a range of $k$s and thus a range of energies and speeds. This is called a \textbf{wave packet}.

  \item In a typicaly quantum problem you'll be given $\Psi(x, 0)$ and be asked to find $\Psi(x, t)$. How do you determine $\phi(k)$ to match the initial wave function \[\Psi(x, 0) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \phi(k) e^{i k x} \,d k.\] The answer is given by \textbf{Plancherel's theorem} \[f(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty F(k) e^{i k x} \,d k \Longleftrightarrow F(k) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty f(x) e^{-i k x} \,d x\] where $F(k)$ is the \textbf{Fourier transform} of $f(x)$. Thus \[\phi(k) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \Psi(x, 0) e^{-i k x} \,d x.\]

  \item A wave packet is comprised of multiple constituent waves that have different velocities and wavelengths. The velocity of a constituent wave is called the \textbf{phase velocity} while the velocity of the packet as a whole is called the \textbf{group velocity}.

  \item The relationship between $\omega$ and $k$ (in the equation above it's $\omega = \hbar k^2 / 2 m$) is called the \textbf{dispersion relation} and it determines the group and phase velocities \begin{align*}
          v_\text{group} & = \frac{d \omega}{d k} \\
          v_\text{phase} & = \frac{\omega}{k}.
        \end{align*}
\end{itemize}

\subsection{The Delta-Function Potential}

\subsubsection{Bound States and Scattering States}

\begin{itemize}
  \item In classical mechanics, if the potential energy function $V(x)$ rises higher than the system's total energy $E$ then the particle is ``stuck'' in a potential well. It moves back and forth between the \textbf{turning points} but it can't escape. This is called a \textbf{bound state}.

  \item On the other hand, if $E$ exceeds $V(x)$ on one or both sides then the particle comes in from ``infinity'', slows down or speeds up under the influence of the potential, and returns to infinity. This is called a \textbf{scattering state}.

  \item Some potentials admit only bound states (e.g. the harmonic oscillator), some admit only scattering states, and some admit both kinds depending on the energy of the particle.
\end{itemize}

\subsubsection{The Delta-Function Well}

\begin{itemize}
  \item The \textbf{Dirac-Delta function} is defined as \[\delta(x) = \begin{cases}
            0      & x \ne 0 \\
            \infty & x = 0
          \end{cases}\] and has the property \[\int_{-\infty}^\infty \delta(x) \,d x = 1.\] Note that $\delta(x - a)$ is a spike at the point $x = a$ and thus \[f(x) \delta(x - a) = f(a) \delta(x - a).\] This behaviour can be used to ``pick out'' values from under integrals \[\int_{-\infty}^\infty f(x) \delta(x - a) \,d x = f(a) \int_{-\infty}^\infty \delta(x - a) \,d x = f(a).\]

  \item The boundary conditions for $\psi$ are:

        \begin{enumerate}
          \item $\psi$ is always continuous, and

          \item $d \psi / d x$ is continuous except at points where the potential is infinite.
        \end{enumerate}

  \item The potential function $V(x) = -\alpha \delta(x)$ exhibits:

        \begin{itemize}
          \item a bound state with \[\psi(x) = \frac{\sqrt{m \alpha}}{\hbar} e^{-m \alpha x / \hbar^2}\] for \[E = -\frac{m \alpha^2}{2 \hbar^2}\]

          \item a scattering state with \[\psi(x) = \begin{cases}
                    A e^{i k x} + B e^{-i k x} & x < 0 \\
                    F e^{i k x} + G e^{-i k x} & x > 0
                  \end{cases}\] for $E > 0$
        \end{itemize}

  \item In the scattering states \[R = \frac{\beta^2}{1 + \beta^2} \text{ where } \beta = \frac{m \alpha}{\hbar^2 k}\] is called the \textbf{reflection coefficient}. This is the relative probability that a particle coming in from the left will be reflected — if you have a beam of particles it is the fraction of particles that will be reflected.

  \item In the scattering states \[T = \frac{1}{1 + \beta^2}\] is called the \textbf{transmission coefficient}. This is the relative probability taht a particle coming in from the left will be transmitted over the potential well and out to the right — if you have beam of particles it is the fraction of particles that will be transmitted.

  \item Because $k = \sqrt{2 m E} / \hbar$ this means that $R$ and $T$ are functions of $E$ \[R = \frac{1}{1 + (2 \hbar^2 E / m \alpha^2)} \text{ and } T = \frac{1}{1 + (m \alpha^2 / 2 \hbar^2 E)}.\] As $E$ increases more particles are transmitted over the potential well.
\end{itemize}

\subsection{The Finite Square Well}

\begin{itemize}
  \item The potential function for a finite square well is \[V(x) = \begin{cases}
            -V_0 & -a \le x \le a \\
            0    & |x| > a
          \end{cases}\] where $V_0$ is a positive constant.

  \item The bound states can be found by:

        \begin{enumerate}
          \item setting $E < 0$,

          \item solving the Schrödinger equation for the regions $x < -a$, $-a < x < a$, and $x > a$, and

          \item applying the boundary conditions that $\psi$ and $d \psi / d x$ be continuous at $x = \pm a$.
        \end{enumerate}

        This finds that the allowed energies are described by \[\tan z = \sqrt{(z_0 / z)^2 - 1}\] where \begin{align*}
          z   & = l a                                         \\
          z_0 & = \frac{a}{\hbar} \sqrt{2 m V_0} \text{, and} \\
          l   & = \frac{\sqrt{2 m (E + V_0)}}{\hbar}.
        \end{align*}

  \item If $z_0$ is very large (the well is wide and deep), the intersections of $\tan z$ and $\sqrt{(z_0 / z)^2 - 1}$ occur just below $z_n = n \pi / 2$ with $n$ odd so \[E_n + V_0 \approx \frac{n^2 \pi^2 \hbar^2}{2 m (2 a)^2}.\] The left hand side is the ``distance'' between the energy level and the bottom of the well, while the right hand side is the allowed energies for an infinite square well of width $2 a$. This shows that the allowed energies of a finite square well approach those of an infinite square well as $V_0 \rightarrow \infty$ however for any finite $V_0$ there is a finite number of bound states.

  \item As $z_0$ decreases (the well is narrow and shallow) there are fewer intersections of $\tan z$ and $\sqrt{(z_0 / z)^2 - 1}$, i.e. there are fewer bound states, until $z_0 < \pi / 2$ results in a single bound state. There is always one bound state.

  \item The scattering states can be found by:

        \begin{enumerate}
          \item setting $E > 0$,

          \item solving the Schrödinger equation for the regions $x < -a$, $-a < x < a$, and $x > a$, and

          \item applying the boundary conditions that $\psi$ and $d \psi / d x$ be continuous at $x = \pm a$.
        \end{enumerate}

        Assuming that the incident wave comes in from the left this finds \[\psi(x) = \begin{cases}
            A e^{i k x} + B e^{-i k x}  & x < - a    \\
            C \sin (l x) + D \cos (l x) & -a < x < a \\
            F e^{i k x}                 & x > a
          \end{cases}\] where $A$ is the incident amplitude, $B$ is the reflected amplitude, and $F$ is the transmitted amplitude. The boundary conditions can be used to eliminate $C$ and $D$, resulting in \begin{align*}
          B & = i \frac{\sin (2 l a)}{2 k l} (l^2 - k^2) F \text{, and}                         \\
          F & = \frac{e^{-2 i k a} A}{\cos (2 l a) - i \frac{(k^2 + l^2)}{2 k l} \sin (2 l a)}.
        \end{align*} This gives a transmission coefficient of \[T^{-1} = 1 + \frac{V_0^2}{4 E (E + V_0)} \sin^2 \left( \frac{2 a}{\hbar} \sqrt{2 m (E + V_0)} \right).\]

  \item Notice that $T = 1$ whenever the sine is zero. This happens when \[E_n + V_0 = \frac{n^2 \pi^2 \hbar^2}{2 m (2 a)^2},\] i.e. the energy is equal to one of the allowed energies of an infinite square well.
\end{itemize}

\section{Formalism}

\subsection{Hilbert Space}

\begin{itemize}
  \item Quantum theory is based on two constructs: \textbf{wave functions} and \textbf{operators}.

  \item The \textbf{state} of a system is represented by its wave function, \textbf{observables} are represented by operators.

  \item Mathematically, wave functions satisfy the conditions to be called \textbf{vectors} and operators act on them as \textbf{linear transformations}.

  \item The collection of all functions of $x$ is a vector space but it's too large for use in quantum mechanics. For a wave function to represent a physically realisable state it must be normalised \[\int |\Psi|^2 \,d x = 1.\] The set of all \textbf{square-integrable} functions on a specified interval \[f(x) \text{ such that } \int_a^b |f(x)|^2 \,d x < \infty,\] is called \textbf{Hilbert space}. Thus, wave functions live in Hilbert space.

  \item The inner product of two functions is defined as \[\braket{f | g} = \int_a^b f(x)^* g(x) \,d x.\] If $f$ and $g$ are both square-integrable (i.e. they are both in Hilbert space) their inner product is guaranteed to exist.

  \item Note that \[\braket{g | f} = \braket{f | g}^*.\]

  \item The inner product of a function with itself \[\braket{f | f} = \int_a^b |f(x)|^2 \,d x\] is real and non-negative. It is only zero when $f(x) = 0$.

  \item A function is said to be \textbf{normalised} if its inner product with itself is $1$.

  \item Two functions are said to be \textbf{orthogonal} if their inner product is $0$.

  \item A set of functions is said to be \textbf{orthonormal} if they're normalised and mutually orthogonal.

  \item A set of function is said to be \textbf{complete} if any other function in Hilbert space can be expressed as a linear combination of them \[f(x) = \sum_{n = 1}^\infty c_n f_n(x)\] where \[c_n = \braket{f_n | f}.\]
\end{itemize}

\subsection{Observables}

\subsubsection{Hermitian Operators}

\begin{itemize}
  \item The \textbf{Hermitian conjugate} (or \textbf{adjoint}) of a matrix, indicated by a dagger $\vec{T}^\dagger$, is its transpose conjugate. A square matrix is \textbf{hermitian} (or \textbf{self-adjoint}) if it is equal to its hermitian conjugate. Using this notation, the inner product of two vectors can be written as \[\braket{\alpha | \beta} = \vec{a}^\dagger \vec{b}.\]

  \item The expectation value of an observable $Q(x, p)$ can be expressed with inner-product notation \[\ev{Q} = \int \Psi^* \hat{Q} \Psi \,d x = \braket{\Psi | \hat{Q} \Psi}.\]

  \item The outcome of a measurement has to be real, so \[\ev{Q} = \ev{Q}^*\] but the complex conjugate of an inner product reverses the order so \[\ev{\Psi | \hat{Q} \Psi} = \ev{\hat{Q} \Psi | \Psi}.\] Operators that have this property (that they may be applied to either the first or second member of an inner product with the same result) are called \textbf{hermitian}. They're called this because \begin{align*}
          \braket{\Psi | \hat{Q} \Psi} & = \braket{\hat{Q} \Psi | \Psi}      \\
          \Psi^\dagger \hat{Q} \Psi    & = (\hat{Q} \Psi)^\dagger \Psi       \\
                                       & = \Psi^\dagger \hat{Q}^\dagger \Psi
        \end{align*} i.e. $\hat{Q} = \hat{Q}^\dagger$ or $\hat{Q}$ is a hermitian matrix.

  \item Observables are represented by hermitian operators.

  \item In general, matrix multiplication is not commutative, i.e. $\vec{A} \vec{B} \ne \vec{B} \vec{A}$. The diference between the two orderings is called the \textbf{commutator} \[[\vec{A}, \vec{B}] = \vec{A} \vec{B} - \vec{B} \vec{A}\] and is a measure of ``how much'' they fail to commute.
\end{itemize}

\subsubsection{Determinate States}

\begin{itemize}
  \item Normally when you measure an observable $Q$ on an ensemble of identically prepared systems you don't get the same result each time. A state in which every measurement of $Q$ is guaranteed to return the same value $q$ is called a \textbf{determinate state} for $Q$.

  \item In such states, the standard deviation of $Q$ is $0$, i.e. \[\sigma^2 = \Braket{(Q - \braket{Q})^2} = \Braket{\Psi | (\hat{Q} - q)^2 \Psi} = \Braket{(\hat{Q} - q) \Psi | (\hat{Q} - q) \Psi} = 0.\] The only vector whose inner product with itself vanishes is $0$, so \[\hat{Q} \Psi = q \Psi.\] This is the \textbf{eigenvalue equation} for the operator $\hat{Q}$; $\Psi$ is an eigenfunction of $\hat{Q}$ and $q$ is the corresponding \textbf{eigenvalue}. Thus, \textbf{determinate states of $\boldsymbol{Q}$ are eigenfunctions of $\boldsymbol{\hat{Q}}$}.

  \item The collection of all the eigenvalues of an operator is called its \textbf{spectrum}.

  \item If two or more linearly independent eigenfunctions share the same eigenvalue the spectrum is said to be degenerate.
\end{itemize}

\subsection{Eigenfunctions of a Hermitian Operator}

\begin{itemize}
  \item If the spectrum of eigenvalues of a hermitian operator is \textbf{discrete}, i.e. the eigenvalues are separated from one another, then the eigenfunctions lie in Hilbert space and are physically realisable states.

  \item If the spectrum is \textbf{continuous}, i.e. the eigenvalues fill a range, then the eigenfunctions are not normalisable and don't represent possible wave functions (although linear combinations of them may be normalisable).

  \item Some operators have a discrete spectrum only (e.g. the Hamiltonian for the harmonic oscillator), some have a continuous spectrum only (e.g. the Hamiltonian for a free particle), and some have both (e.g. the Hamiltonian for a finite square well).
\end{itemize}

\subsubsection{Discrete Spectra}

\begin{itemize}
  \item The normalisable eigenfunctions of hermitian operators have two important properties:

        \begin{itemize}
          \item their eigenvalues are real, and

          \item eigenfunctions belonging to distinct eigenvalues are orthogonal.
        \end{itemize}

  \item In degenerate states where two or more eigenfunctions share a single eigenvalue, any linear combination of them is also an eigenfunction. The Gram-Schmidt orthogonalisation procedure can be used to construct orthogonal eigenfunctions in each degenerate subspace, so even in the presence of degeneracy the eigenfunctions can be chosen to be orthonormal.

  \item In a finite-dimensional vector space the eigenvectors of a hermitian matrix span the space, but the proof of this doesn't translate to infinite-dimensional spaces. However this property is essential to quantum mechanics so we take it as an axiom:

        \begin{itemize}
          \item The eigenfunctions of an observable operators are complete: Any function (in Hilbert space) can be expressed as a linear combination of them.
        \end{itemize}
\end{itemize}

\subsubsection{Continuous Spectra}

\begin{itemize}
  \item If the eigenvalues of a hermitian operator are continuous its eigenfunctions aren't normalisable, however they may be \textbf{Dirac orthonormalisable}. For example the general solution to the eigenvalue equation for the momentum operator is \[f_p(x) = A e^{i p x / \hbar}\] which isn't normalisable, but if we restrict ourselves to real eigenvalues then \[\int_{-\infty}^\infty f_{p'}^*(x) f_p(x) \,d x = |A|^2 \int_{-\infty}^\infty e^{i (p - p') x / \hbar} \,d x = |A|^2 2 \pi \hbar \delta(p - p').\] Choosing $A = 1 / \sqrt{2 \pi \hbar}$ then \[\braket{f_{p'} | f_p} = \delta(p - p')\] which is reminiscent of true orthonormality.

  \item The eigenfunctions associated with real eigenvalues are also complete, so any square integrable function $f(x)$ can be written in the form \[f(x) = \int_{-\infty}^\infty c(p) f_p(x) \,d p = \frac{1}{\sqrt{2 \pi \hbar}} \int_{-\infty}^\infty c(p) e^{i p x / \hbar} \,d p\] where the coefficients $c(p)$ can be determined by Fourier's trick: \[\braket{f_{p'} | f_p} = \int_{-\infty}^\infty c(p) \braket{f_{p'} | f_p} \,d p = \int_{-\infty}^\infty c(p) \delta(p - p') \,d p' = c(p').\]

  \item In summary: if an the spectrum of a hermitian operator is continuous, the eigenfunctions are not normalisable, they are not in Hilbert space, and they do not represent physically possibly states. However they are Dirac orthonormalisable and complete.
\end{itemize}

\subsection{Generalised Statistical Interpretation}

\begin{itemize}
  \item The \textbf{generalised statistical interpretation} states: If you measure an observable $Q(x, p)$ on a particle in the state $\Psi(x, t)$, you are certain to get one of the eigenvalues of the hermitian operator $\hat{Q}(x, -i \hbar d / d x)$. If the spectrum of $\hat{Q}$ is discrete, the probability of getting the particular eigenvalue $q_n$ associated with the (orthonormalised) eigenfunction $f_n(x)$ is \[|c_n|^2 \text{ where } c_n = \braket{f_n | \Psi}.\] If the spectrum is continuous, with real eigenvalues $q(z)$ and associated (Dirac-orthonormalised) eigenfunctions $f_z(x)$, the probability of getting a result in the range $d z$ is \[|c(z)|^2 \,d z \text{ where } c(z) = \braket{f_z | \Psi}.\] Upon measurement the wave function ``collapses'' to the corresponding eigenstate.

  \item Because the eigenfunctions of an observable operator are complete the wavefunction can be written as a linear combination of them \[\Psi(x, t) = \sum_n c_n(t) f_n(x)\] and because they're orthonormal the coefficients are given by Fourier's trick \[c_n(t) = \braket{f_n | \Psi} = \int f_n(x)^* \Psi(x, t) \,d x.\]

  \item The Dirac-orthonormalised eigenfunctions of the momentum operator are \[f_p(x) = \frac{1}{\sqrt{2 \pi \hbar}} e^{i p x / \hbar}\] so \[c(p) = \braket{f_p | \Psi} = \frac{1}{\sqrt{2 \pi \hbar}} \int_{-\infty}^\infty e^{i p x / \hbar} \Psi(x, t) \,d x.\] This is such an important quantity that it is called the \textbf{momentum space wave function $\boldsymbol{\Phi(x, t)}$}. It is essentially the Fourier transform of the position space wave function and, according to the generalised statistical interpretation, the probability that a measurement of momentum would yield a result in the range $d p$ is given by \[|\Phi(p, t)|^2 \,d p.\]
\end{itemize}

\subsection{The Uncertainty Principle}

\subsubsection{Proof of the Generalised Uncertainty Principle}

\begin{itemize}
  \item For two observables $A$ and $B$ and associated operators $\hat{A}$ and $\hat{B}$, the \textbf{generalised uncertainty principle} states \[\sigma_A^2 \sigma_B^2 \ge \left( \frac{1}{2 i} \braket{[\hat{A}, \hat{B}]} \right)^2\] where $[\hat{A}, \hat{B}]$ is the commutator of the two operators.

  \item The implication of this is that there's an ``uncertainty principle'' for each pair of observables whose operators don't commute. These are called \textbf{incompatible observables} and they don't have a complete set of common eigenfunctions. On the other hand, \textbf{compatible observables} do have a complete set of common eigenfunctions and taking one measurement gives you a single eigenfunction from which you can determine all the compatible observables without needing to take another measurement.
\end{itemize}

\subsubsection{The Minimum-Uncertainty Wave Packet}

\begin{itemize}
  \item The minimum uncertainty wave packet where $\sigma_x \sigma_p = \hbar / 2$ has the form \[\Psi(x, t) = A e^{-a (x - \braket{x})^2 / 2 \hbar} e^{i \braket{p} x / \hbar}\] i.e. it is Gaussian.
\end{itemize}

\subsubsection{The Energy-Time Uncertainty Principle}

\begin{itemize}
  \item The \textbf{energy-time uncertainty principle} is \[\Delta t \Delta E \ge \frac{\hbar}{2}\] where $\Delta t$ is ``the time it takes the system to change substantially'' and $\Delta E$ is ``the uncertainty in $E$.''

  \item The \textbf{Generalised Ehrenfest theorem} is \[\frac{d}{d t} \braket{Q} = \frac{i}{\hbar} \Braket{\left[ \hat{H}, \hat{Q} \right]} + \Braket{\frac{\partial \hat{Q}}{\partial t}},\] i.e. the time rate change in the expected value of an observable is determined by the commutator of its operator with the Hamiltonian and the derivative of its operator with respect to time. In the typical case where its the operator doesn't explicitly depdend on time, if $\hat{Q}$ commutes with $\hat{H}$ then $\braket{Q}$ is constant and $Q$ is a conserved quantity.

  \item Substituting the above into the generalised undertainty principle we get \[\sigma_H \sigma_Q \ge \frac{\hbar}{2} \left| \frac{d \braket{Q}}{d t} \right|.\] If we define $\Delta E = \sigma_H$ and \[\Delta t = \frac{\sigma_Q}{|d \braket{Q} / d t|}\] then \[\Delta E \Delta t \ge \frac{\hbar}{2}.\] Note that $\Delta t$ is the amount time it takes for the expectation value of $Q$ to change by one standard deviation.

  \item If $\Delta E$ is small then $\Delta t$ must be large — if we know the energy the rate of change of all observables must be gradual. On the other hand, if $\Delta t$ is small we don't know as much about the energy.
\end{itemize}

\subsection{Vectors and Operations}

\subsubsection{Bases in Hilbert Space}

\begin{itemize}
  \item In order to express a Euclidean vector $\vec{v}$ as a tuple of components we first need to choose a basis, e.g. $\hat{x}$, $\hat{y}$, and $\hat{z}$ in $\mathbb{R}^3$. The vector can then be expressed as a linear combination of these basis vectors \[\vec{v} = v_x \hat{x} + v_y \hat{y} + v_z \hat{z}.\] The components $v_x$, $v_y$, and $v_z$ can be determined by taking the inner product between $\vec{v}$ and the associated basis vector \[v_x = \vec{v} \cdot \hat{x}, \ v_y = \vec{v} \cdot \hat{y}, \ v_z = \vec{v} \cdot \hat{z}.\] This projects $\vec{v}$ onto the basis vectors and tells us ``how much'' of each to include. Different basis vectors would result in different components.

  \item A quantum state can be represented by a vector $\ket{S(t)}$ in Hilbert space.

  \item As with Euclidean vectors, $\ket{S(t)}$ can be expressed in different ways by choosing different bases.

  \item Because the eigenfunctions of Hermitian operators are complete and orthonormal, they can be used as a basis.

  \item Some examples of eigenfunctions of the position operator $\hat{x}$ are $\ket{1}$, $\ket{4.2}$, and $\ket{-1337}$ corresponding to the positions $x = 1$, $x = 4.2$, and $x = -1337$, respectively.

  \item A quantum state can be expressed in the position basis as a linear combination of the eigenfunctions of the position operator $\hat{x}$ \[\ket{S(t)} = c_1(t) \ket{1} + c_2(t) \ket{2} + \cdots\] however because there is an eigenfunction for every real value $x$ this must be expressed as an integral \[\ket{S(t)} = \int c(x, t) \ket{x} \,d x\] where the coefficient function is none other than the wave function in position space, i.e. \[\ket{S(t)} = \int \Psi(x, t) \ket{x} \,d x.\]

  \item As with Euclidean vectors we can determine ``how much'' of a particular basis vector, e.g. $\ket{1}$ in the position basis, makes up a quantum state $\ket{S(t)}$ by taking the inner product \[\braket{1 | S(t)} = \int_{-\infty}^\infty \delta(x - 1) \Psi(x, t) \,d x = \Psi(1, t).\] If we take the inner product between $\ket{x}$ (a basis vector in position space but with $x$ left as a variable) and a quantum state $\ket{S(t)}$ we leave $x$ ``unset'' and extract the wave function in position space \[\braket{x | S(t)} = \int_{-\infty}^\infty \delta(x - y) \Psi(y, t) \,d y = \Psi(x, t).\]

  \item The eigenfunctions of other Hermitian operators can also be used as a basis, e.g. $\hat{p}$, $\hat{H}$.

  \item Operators are linear transformations on Hilbert space — they ``transform'' one vector into another \[\ket{\beta} = \hat{Q} \ket{\alpha}.\]

  \item Just as vectors are represented by their components with respect to an orthonormal basis \[\ket{\alpha} = \sum_n a_n \ket{e_n}\] operators are represented with respect to a particular basis by their \textbf{matrix elements} \[\braket{e_m | \hat{Q} | e_n} = Q_{m n}.\] One way of reading this is:

        \begin{enumerate}
          \item Apply the transformation $\hat{Q}$ to the basis vector $\ket{e_n}$.

          \item Take the inner product between the basis vector $\ket{e_m}$ and the transformed basis vector $\hat{Q} \ket{e_n}$.

          \item Construct a matrix using these elements \[\begin{pmatrix}
                    Q_{1 1} & Q_{1 2} & \ldots \\
                    Q_{2 1} & Q_{2 2} & \ldots \\
                    \vdots  & \ddots
                  \end{pmatrix}.\]
        \end{enumerate}

        When the matrix is applied to another vector $\vec{b} = \vec{Q} \vec{a}$, each element $Q_{m n}$ represents ``how much should the $n$th component of $\vec{a}$ contribute to the $m$th component of $\vec{b}$.''

  \item Just as vectors and quantum states are expressed differently under different bases, so too are operators. For example, the position operator $\hat{x}$ is $x$ in position space but $i \hbar \partial / \partial p$ in momentum space.
\end{itemize}

\subsubsection{Dirac Notation}

\begin{itemize}
  \item Dirac proposed to split the notation for the inner product $\braket{\alpha | \beta}$ in two pieces: the \textbf{bra} $\bra{\alpha}$, and the \textbf{ket} $\ket{\beta}$.

  \item A bra is a vector. In a finite-dimensional vector space it can be expressed as a column vector.

  \item A ket is a linear function of vectors. When it is applied to a vector on the right it yields a complex number — the inner product. In a finite-dimensional vector space it can be expressed as a row vector.

  \item The collection of all bras constitutes another vector space called \textbf{the dual space}.

  \item If $\ket{\alpha}$ is a normalised vector, the operator \[\hat{P} = \ket{\alpha} \bra{\alpha}\] picks out the portion of any other vector that ``lies along'' $\ket{\alpha}$. It is called the \textbf{projection operator}.

  \item If $\{\ket{e_n}\}$ is a discrete orthonormal basis then \[\sum_n \ket{e_n} \bra{e_n} = 1\] is the identity operator. This is because if we let it act on any vector $\ket{\alpha}$ we recover the expansion of $\ket{\alpha}$ in the $\{\ket{e_n}\}$ basis \[\sum_n (\braket{e_n | \alpha}) \ket{e_n} = \ket{\alpha}.\]

  \item Similarly, if $\{\ket{e_z}\}$ is a Dirac orthonormalised basis then \[\int \ket{e_n} \bra{e_z} \,d z = 1.\]

  \item The sum of two operators is \[(\hat{Q} + \hat{R}) \ket{\alpha} = \hat{Q} \ket{\alpha} + \hat{R} \ket{\alpha}\] and the product of two operators is \[\hat{Q} \hat{R} \ket{\alpha} = \hat{Q} (\hat{R} \ket{\alpha}).\]

  \item Functions of operators are defined by the power series \begin{align*}
          e^{\hat{Q}}           & = 1 + \hat{Q} + \frac{1}{2} \hat{Q}^2 + \frac{1}{3!} \hat{Q}^3 + \cdots \\
          \frac{1}{1 - \hat{Q}} & = 1 + \hat{Q} + \hat{Q}^2 + \hat{Q}^3 + \cdots                          \\
          \ln (1 + \hat{Q})     & = \hat{Q} - \frac{1}{2} \hat{Q}^2 + \frac{1}{3} \hat{Q}^3 + \cdots
        \end{align*}
\end{itemize}

\subsubsection{Changing Bases in Dirac Notation}

\begin{itemize}
  \item The identity operator can be written in terms of any complete set of eigenstates, for example the position eigenstates $\ket{x}$, the momentum eigenstates $\ket{p}$, and the energy eigenstates $\ket{n}$ \begin{align*}
          1 & = \int d x \, \ket{x} \bra{x} \\
          1 & = \int d p \, \ket{p} \bra{p} \\
          1 & = \sum \ket{n} \bra{n}.
        \end{align*} Applying each of these identity operators to a quantum state $\ket{S(t)}$ gives \begin{align*}
          \ket{S(t)} & = \int d x \ket{x} \braket{x | S(t)}   \\
                     & = \int \Psi(x, t) \ket{x} \,d x        \\
          \ket{S(t)} & = \int \,d p \ket{p} \braket{p | S(t)} \\
                     & = \int \Phi(p, t) \ket{p} \,d p        \\
          \ket{S(t)} & = \sum_n \ket{n} \braket{n | S(t)}     \\
                     & = \sum_n c_n(t) \ket{n}.
        \end{align*}

  \item Just as wave functions take different forms in different bases, so do operators. For example, in the position basis the position operator is given by $\hat{x} = x$ but in the momentum basis it is given by $\hat{x} = i \hbar \frac{\partial}{\partial p}$. The result of an operator can be expressed in any basis by taking the inner product with an appropriate basis vector. For example, the result of the position operator in the position basis is \[\braket{x | \hat{x} | S(t)} = x \Psi(x, t)\] while in the momentum basis it is \[\braket{p | \hat{x} | S(t)} = i \hbar \frac{\partial \Phi}{\partial p}.\]

  \item The latter result can be obtained by inserting the identity operator \begin{align*}
          \braket{p | \hat{x} | S(t)} & = \Braket{p | \hat{x} \int d x \Ket{x} \Bra{x} | S(t)}                                                    \\
                                      & = \int \braket{p | \hat{x} | x} \braket{x | S(t)} \,d x                                                   \\
                                      & = \int \braket{p | x | x} \Psi(x, t) \,d x                                                                \\
                                      & = \int x \braket{p | x} \Psi(x, t) \,d x                                                                  \\
                                      & = \int x \braket{x | p}^* \Psi(x, t) \,d x                                                                \\
                                      & = \int x \frac{e^{-i p x / \hbar}}{\sqrt{2 \pi \hbar}} \Psi(x, t) \,d x                                   \\
                                      & = i \hbar \frac{\partial}{\partial p} \int \frac{e^{-i p x / \hbar}}{\sqrt{2 \pi \hbar}} \Psi(x, t) \,d x \\
                                      & = i \hbar \frac{\partial \Phi}{\partial p}.
        \end{align*}
\end{itemize}

\section{Quantum Mechanics in Three Dimensions}

\subsection{The Schrödinger Equation}

\begin{itemize}
  \item The Schrödinger equation in three-dimensions is \[i \hbar \frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2 m} \nabla^2 \Psi + V \Psi\] where the potential energy function $V$ and the wave function $\Psi$ are now functions of $\vec{r} = (x, y, z)$ and $t$. The probability of finding the particle in the volume $d^3 \vec{r} = d x \,d y \,d z$ is $|\Psi(\vec{r}, t)|^2 \,d^3 \vec{r}$ and the normalisation condition reads \[\int |\Psi|^2 \,d^3 \vec{r} = 1.\]

  \item If the potential energy function $V$ is independent of time there will be a complete set of stationary states \[\Psi_n(\vec{r}, t) = \psi_n(\vec{r}) e^{-i E_n t / \hbar}\] where $\psi_n$ satisfies the time-independent Schrödinger equation \[-\frac{\hbar^2}{2 m} \nabla^2 \psi + V \psi = E \psi.\] The general solution to the time-depdendent Schrödinger equation is \[\Psi(\vec{r}, t) = \sum c_n \psi_n(\vec{r}) e^{-i E_n t / \hbar}\] where the constants $c_n$ are determined by the initial state of the wave function $\Psi(\vec{r}, 0)$.
\end{itemize}

\subsubsection{Spherical Coordinates}

\begin{itemize}
  \item In spherical coordinates the Laplacian takes the form \[\nabla^2 = \frac{1}{r^2} \frac{\partial}{\partial r} \left( r^2 \frac{\partial}{\partial r} \right) + \frac{1}{r^2 \sin \theta} \frac{\partial}{\partial \theta} \left( \sin \theta \frac{\partial}{\partial \theta} \right) + \frac{1}{r^2 \sin^2 \theta} \left( \frac{\partial^2}{\partial \phi^2} \right)\] so the Schrödinger equation takes the form \begin{align*}
    -\frac{\hbar^2}{2 m} \left[ \frac{1}{r^2} \frac{\partial}{\partial r} \left( r^2 \frac{\partial \psi}{\partial r} \right) + \frac{1}{r^2 \sin \theta} \frac{\partial}{\partial \theta} \left( \sin \theta \frac{\partial \psi}{\partial \theta} \right) + \frac{1}{r^2 \sin^2 \theta} \left( \frac{\partial^2 \psi}{\partial \phi^2} \right) \right] + V \psi = E \psi.
  \end{align*}

  \item If we apply separation of variables and look for solutions of the form \[\psi(r, \theta, \phi) = R(r) Y(\theta, \phi)\] substituting this and rearranging we find \begin{align*}
    \left\{ \frac{1}{R} \frac{d}{d r} \left( r^2 \frac{d R}{d r} \right) - \frac{2 m r^2}{\hbar^2} [V(r) - E] \right\} + \frac{1}{Y} \left\{ \frac{1}{\sin \theta} \frac{\partial}{\partial \theta} \left( \sin \theta \frac{\partial Y}{\partial \theta} \right) + \frac{1}{\sin^2 \theta} \frac{\partial^2 Y}{\partial \phi^2} \right\} = 0.
  \end{align*}

  \item The first term above only depends on $r$ and the second term only depends on $\theta$ and $\phi$ so they both must be constant \begin{align*}
    \frac{1}{R} \frac{d}{d r} \left( r^2 \frac{d R}{d r} \right) - \frac{2 m r^2}{\hbar^2} [V(r) - E] &= \ell (\ell + 1) \\
    \frac{1}{Y} \left\{ \frac{1}{\sin \theta} \frac{\partial}{\partial \theta} \left( \sin \theta \frac{\partial Y}{\partial \theta} \right) + \frac{1}{\sin^2 \theta} \frac{\partial^2 Y}{\partial \phi^2} \right\} &= -\ell (\ell + 1).
  \end{align*}
\end{itemize}

\subsubsection{The Angular Equation}

\begin{itemize}
  \item The \textbf{Legendre differential equation} is defined as \[(1 - x^2) y'' - 2 x y' + n (n + 1) y = 0.\]

  \item The \textbf{Legendre polynomials} are solutions to the Legendre differential equation for non-negative integer $n$. They are given by \textbf{Rodrigues' formula} \[P_n(x) = \frac{1}{2^n n!} \frac{d^n}{d x^n} (x^2 - 1)^n.\]

  \item The \textbf{Legendre functions} are solutions to the Legendre differential equation for non-integer (possibly complex) $n$.

  \item The \textbf{associated Legendre differential equation} is defined as \[(1 - x^2) y'' - 2 x y' + \left[ n (n + 1) - \frac{m^2}{1 - x^2} \right] y = 0.\]

  \item The \textbf{associated Legendre polynomials} are solutions to the associated Legendre differential equation for non-negative integer $n$ and $|m| \le n$. For non-negative $m$ they can be defined as \[P_n^m = (-1)^m (1 - x^2)^{m / 2} \frac{d^m}{d x^m} (P_n(x))\] and for negative $m$ they can be defined as \[P_n^{-m}(x) = (-1)^m \frac{(n - m)!}{(n + m)!} P_n^m(x).\]

  \item The \textbf{associated Legendre functions} are solutions to the associated Legendre differential equation for non-integer (possible complex) $n$ and $m$.

  \item The following summarises the family of Legendre polynomials/functions:

  \begin{center}
    \begin{tabular}{ |c||c|c| }
      \hline
      & Legendre DE & Associated Legendre DE \\
      \hline \hline
      Integer parameters & $P_n(x)$ & $P_n^m(x)$ \\
      Non-integer parameters & $P_\lambda(x)$, $Q_\lambda(x)$ & $P_\lambda^\mu(x)$, $Q_\lambda^\mu(x)$ \\
      \hline
    \end{tabular}
  \end{center}

  \item Multiplying the $\theta$, $\phi$ term of the three-dimensional Schrödinger equation in spherical coordinates by $Y \sin^2 \theta$ we get \[\sin \theta \frac{\partial}{\partial \theta} \left( \sin \theta \frac{\partial Y}{\partial \theta} \right) + \frac{\partial^2 Y}{\partial \phi^2} = -\ell (\ell + 1) \sin^2 \theta Y.\] Assuming a solution of the form \[Y(\theta, \phi) = \Theta(\theta) \Phi(\phi)\] and applying separation of variables we find \[\Phi(\phi) = e^{i m \phi}\] for integer $m$. The $\theta$ equation is \[\sin \theta \frac{d}{d \theta} \left( \sin \theta \frac{d \Theta}{d \theta} \right) + [\ell (\ell + 1) \sin^2 \theta - m^2] \Theta = 0\] the solution to which is \[\Theta(\theta) = A P_\ell^m (\cos \theta).\]

  \item The normalised angular wave functions are called \textbf{spherical harmonics} \[Y_\ell^m(\theta, \phi) = \sqrt{\frac{2 \ell + 1}{4 \pi} \frac{(\ell - m)!}{(\ell + m)!}} e^{i m \phi} P_\ell^m (\cos \theta).\]
\end{itemize}

\subsubsection{The Radial Equation}

\begin{itemize}
  \item The radial part of the wave equation is \[\frac{d}{d r} \left( r^2 \frac{d R}{d r} \right) - \frac{2 m r^2}{\hbar^2} [V(r) - E] R = \ell (\ell + 1) R.\] If we substitute $u(r) = r R(r)$ then this becomes \[-\frac{\hbar^2}{2 m} \frac{d^2 u}{d r^2} + \left[ V + \frac{\hbar^2}{2 m} \frac{\ell (\ell + 1)}{r^2} \right] u = E u.\] This is identical in form to the one-dimensional Schrödinger equation but with an \textbf{effective potential} of \[V_\text{eff} = V + \frac{\hbar^2}{2 m} \frac{\ell (\ell + 1)}{r^2}.\] The extra term is known as the \textbf{centrifugal term} and it tends to force the particle outwards from the origin.

  \item Differential equations of the form \[x^2 \frac{d^2 y}{d x^2} + 2 x \frac{d y}{d x} + (x^2 - n (n + 1)) y = 0\] have two linearly independent solutions: the \textbf{spherical Bessel function} of order $n$ \[j_n(x) = (-x)^n \left( \frac{1}{x} \frac{d}{d x} \right)^n \frac{\sin x}{x}\] and the \textbf{spherical Neumann function} of order $n$ \[n_n(x) = -(-x)^n \left( \frac{1}{x} \frac{d}{d x} \right)^n \frac{\cos x}{x}.\]
  
  \item The Bessel functions are oscillatory and have an infinite number of zeroes, but they are not located at ``nice'' points and must be computed numerically.

  \item In an \textbf{infinite spherical well} with potential \[V(r) = \begin{cases}
    0 & r \le a \\
    \infty & r > a
  \end{cases}\] the radial equation takes the form \[\frac{d^2 u}{d r^2} = \left[ \frac{\ell (\ell + 1)}{r^2} - k^2 \right] u\] where $k = \sqrt{2 m E} / \hbar$. This has the general solution \[u(r) = A r j_\ell(k r) + B r n_\ell(k r).\] The allowed energies are given by \[E_{N \ell} = \frac{\hbar^2}{2 m a^2} \beta_{N \ell}^2\] where $N$ is a positive integer and $\beta_{N \ell}$ is the $N$th zero of the $\ell$th spherical Bessel function.

  \item The \textbf{principal quantum number} $n$ is used to order the energies of a system — $n = 1$ is the ground state, $n = 2$ is the next highest energy, any so on.
\end{itemize}

\subsection{The Hydrogen Atom}

\begin{itemize}
  \item The potential energy of the electron in a hydrogen atom is \[V(r) = -\frac{e^2}{4 \pi \epsilon_0} \frac{1}{r}\] and thus its radial equation is \[-\frac{\hbar^2}{2 m_e} \frac{d^2 u}{d r^2} + \left[ -\frac{e^2}{4 \pi \epsilon_0} \frac{1}{r} + \frac{\hbar^2}{2 m_e} \frac{\ell (\ell + 1)}{r^2} \right] u = E u.\]
\end{itemize}

\subsubsection{The Radial Wave Function}

\begin{itemize}
  \item For bound states $E < 0$ and we can introduce the constant \[\kappa = \frac{\sqrt{-2 m_e E}}{\hbar}.\] Dividing the radial equation above by $E$ and substituting $\kappa$ gives \[\frac{1}{\kappa^2} \frac{d^2 u}{d r^2} = \left[ 1 - \frac{m_e e^2}{2 \pi \epsilon_0 \hbar^2 \kappa} \frac{1}{(\kappa r)} + \frac{\ell (\ell + 1)}{(\kappa r)^2} \right] u.\] This suggests we should introduce \[\rho = \kappa r \quad \text{and} \quad \rho_0 = \frac{m_e e^2}{2 \pi \epsilon_0 \hbar^2 \kappa}\] which gives \[\frac{d^2 u}{d \rho^2} = \left[ 1 - \frac{\rho_0}{\rho} + \frac{\ell (\ell + 1)}{\rho^2} \right] u.\]

  \item As $\rho \rightarrow \infty$ the constant term dominates and $u(\rho) \approx A e^{-\rho}$. As $\rho \rightarrow 0$ the centrifugal term dominates and $u(\rho) \approx C \rho^{\ell + 1}$. Using this behaviour we can propose a solution of the form \[u(\rho) = \rho^{\ell + 1} e^{-\rho} v(\rho)\] where $v(\rho)$ can be expressed in a power series in $\rho$ \[v(\rho) = \sum_{j = 0}^\infty c_j \rho^j.\] Differentiating $v(\rho)$ and substituting it into the radial equation we can find a recurrence relation for the coefficients $c_j$.

  \item The coefficients $c_j$ for large $j$ determine the behaviour of $v(\rho)$ for large $\rho$ because they are associated with greater powers of $\rho$. If we assume their behaviour holds for all coefficients we find that $v(\rho)$ blows up for large $\rho$. We conclude that the series must terminate at some $j = N > 0$, i.e. $c_N = 0$.
  
  \item The spatial wave functions are defined as \[\psi_{n \ell m}(r, \theta, \phi) = R_{n \ell}(r) Y_\ell^m(\theta, \phi)\] where $n > 0$ is the \textbf{principal quantum number}, $\ell < n$ is the \textbf{azimuthal quantum number}, $|m| \le \ell$ is the \textbf{magnetic quantum number}, \[R_{n \ell}(r) = \frac{1}{r} \rho^{\ell + 1} e^{-\rho} v(\rho),\] and $v(\rho)$ is a polynomial of degree $n - \ell - 1$ (this is why $\ell$ must be less than $n$) whose coefficients are determined by the recurrence relation.
  
  \item Apart from normalisation, $v(\rho)$ can be written as \[v(\rho) = L_{n - \ell - 1}^{2 \ell + 1}(2 \rho)\] where \[L_q^p(x) = (-1)^p \left( \frac{d}{d x} \right)^p L_{p + q}(x)\] is an \textbf{associated Laguerre polynomial} and \[L_q(x) = \frac{e^x}{q!} \left( \frac{d}{d x} \right)^q (e^{-x} x^q)\] is the $q$th \textbf{Laguerre polynomial}.
  
  \item The allowed energies of the electron in a hydrogen atom are \[E_n = -\left[ \frac{m_e}{2 \hbar^2} \left( \frac{e^2}{4 \pi \epsilon_0} \right)^2 \right] \frac{1}{n^2} = \frac{E_1}{n^2}, \,n = 1, 2, 3, \ldots\] This is called the \textbf{Bohr formula}.

  \item The \textbf{ground state} is the state of lowest energy \[E_1 = \qty{-13.6}{eV}.\]

  \item For each value of $n$ there are $n - 1$ values of $\ell$ and for each value of $\ell$ there are $2 \ell + 1$ values of $m$. As the allowed energies are determined entirely by $n$ this means that the degeneracy of energy $E_n$ is \[d(n) = \sum_{\ell = 0}^{n - 1} (2 \ell + 1) = n^2.\]
  
  \item The normalised hydrogen wave functions are \[\psi_{n \ell m} = \sqrt{\left( \frac{2}{n a} \right)^3 \frac{(n - \ell - 1)!}{2 n (n + \ell)!}} e^{-r / n a} \left( \frac{2 r}{n a} \right)^\ell \left[ L_{n - \ell - 1}^{2 \ell + 1} (2 r / n a) \right] Y_\ell^m(\theta, \phi).\]
\end{itemize}

\subsubsection{The Spectrum of Hydrogen}

\begin{itemize}
  \item In principle, if you put a hydrogen atom into some stationary state $\Psi_{n \ell m}$ it should stay there forever. However, if it is pertubed (e.g. by collision with another atom, or by shining light on it), it may undergo a \textbf{transition} to another stationary state — either by absorbing energy and moving to a higher energy state or by giving off energy in the form of electromagnetic radition and moving to a lower energy state. In practice these perturbations are always present.

  \item The energy of the emitted light is determined by the energy difference between the initial and final states \[E_\gamma = E_i - E_f = E_1 \left( \frac{1}{n_i^2} - \frac{1}{n_f^2} \right).\]

  \item The energy of a photon is determined by its frequency as given by the Planck formula \[E_\gamma = h f\] and its wavelength is given by \[\lambda = \frac{c}{f}\] so \begin{align*}
          \frac{1}{\lambda} & = \frac{f}{c}                                                                                                                \\
                            & = \frac{E_\gamma}{c h}                                                                                                       \\
                            & = \frac{E_1}{c h} \left( \frac{1}{n_i^2} - \frac{1}{n_f^2} \right)                                                           \\
                            & = \frac{m_e}{4 \pi c \hbar^3} \left( \frac{e^2}{4 \pi \epsilon_0} \right)^2 \left( \frac{1}{n_f^2} - \frac{1}{n_i^2} \right) \\
                            & = \mathcal{R} \left( \frac{1}{n_f^2} - \frac{1}{n_i^2} \right)
        \end{align*} where \[\mathcal{R} = \frac{m_e}{4 \pi c \hbar^3} \left( \frac{e^2}{4 \pi \epsilon_0} \right)^2 = \qty{1.097e7}{m^{-1}}\] is known as the \textbf{Rydberg constant} and the equation above is known as the \textbf{Rydberg formula} for the spectrum of hydrogen.

  \item Transitions to the ground state ($n_f = 1$) are known as the \textbf{Lyman series}, transitions to the first excited state ($n_f = 2$) are known as the \textbf{Balmer series}, and transitions to the second excited state ($n_f = 3$) are known as the \textbf{Paschen series}.
\end{itemize}

\subsection{Angular Momentum}

\begin{itemize}
  \item The angular momentum operators are defined as \begin{align*}
          L_x & = y p_z - z p_y              \\
          L_y & = z p_x - x p_z \text{, and} \\
          L_z & = x p_y - y p_x
        \end{align*} where $p_x = -i \hbar \partial / \partial x$, etc.
\end{itemize}

\subsubsection{Eigenvalues}

\begin{itemize}
  \item The angular momentum operators don't commute \begin{align*}
          [L_x, L_y] & = i \hbar L_z              \\
          [L_y, L_z] & = i \hbar L_x \text{, and} \\
          [L_z, L_x] & = i \hbar L_y.
        \end{align*}

  \item According to the generalised uncertainty principle \begin{align*}
          \sigma_{L_x}^2 \sigma_{L_y}^2 & \ge \left( \frac{1}{2 i} \braket{i \hbar L_z} \right)^2 \\
                                        & = \frac{\hbar^2}{4} \braket{L_z}^2                      \\
          \sigma_{L_x} \sigma_{L_y}     & \ge \frac{\hbar}{2} |\braket{L_z}|
        \end{align*} this means there are no states that are simultaneously eigenfunctions of $L_x$ and $L_y$ so you can't measure them at the same time.

  \item On the other hand, the square of the total angular momentum \[L^2 = L_x^2 + L_y^2 + L_z^2\] does commute with each component of the angular momentum \begin{align*}
          [L^2, L_x] & = 0              \\
          [L^2, L_y] & = 0 \text{, and} \\
          [L^2, L_z] & = 0
        \end{align*} so they can be measured at the same time.

  \item The eigenvalues of $L^2$ are $\hbar^2 \ell (\ell + 1)$ where $\ell$ may be an integer or half integer. The eigenvalues of $L_z$ are $m \hbar$ where $m$ goes from $-\ell$ to $\ell$ in integer steps. \begin{align*}
          \ell & = 0, \,1 / 2, \,2, \,3 / 2, \ldots                 \\
          m    & = -\ell, \,-\ell + 1, \,\ldots, \,\ell - 1, \,\ell
        \end{align*}

  \item This means the magnitude of the angular momentum is $\hbar \sqrt{\ell (\ell + 1)}$ and the maximum $z$ component of the angular momentum is $\hbar \ell$. Because $\sqrt{\ell (\ell + 1)} > \ell$ (except when $\ell = 0$) this means the angular momentum can never point in the $z$ direction. Another way of phrasing this is: if the angular momentum pointed in the $z$ direction we would know all three components (the other two components are $0$) and we would be in violation of the uncertainty principle because the components don't commute.
\end{itemize}

\subsubsection{Eigenfunctions}

\begin{itemize}
  \item The cartesian components of the angular momentum operator in spherical coordinates are \begin{align*}
          L_x & = -i \hbar \left( -\sin \phi \frac{\partial}{\partial \theta} - \cos \phi \cot \theta \frac{\partial}{\partial \phi} \right),            \\
          L_y & = -i \hbar \left( \cos \phi \frac{\partial}{\partial \theta} - \sin \phi \cot \theta \frac{\partial}{\partial \phi} \right), \text{ and} \\
          L_z & = -i \hbar \frac{\partial}{\partial \phi}.
        \end{align*}

  \item The squared magnitude of the angular momentum operator in spherical coordinates is \[L^2 = -\hbar^2 \left[ \frac{1}{\sin \theta} \frac{\partial}{\partial \theta} \left( \sin \theta \frac{\partial}{\partial \theta} \right) + \frac{1}{\sin^2 \theta} \frac{\partial^2}{\partial \phi^2} \right].\]

  \item The eigenfunctions of the operators $H$, $L^2$, and $L_z$ are the spherical harmonics $Y_\ell^m$.
\end{itemize}

\subsection{Spin}

\begin{itemize}
  \item \textbf{Spin} is the intrinsic angular momentum of a particle and is denoted $\vec{S}$.

  \item The commutation relations between the components of $\vec{S}$ are \begin{align*}
          [S_x, S_y] & = i \hbar S_z,             \\
          [S_y, S_z] & = i \hbar S_x, \text{ and} \\
          [S_z, S_x] & = i \hbar S_y.
        \end{align*}

  \item The eigenvectors of $S^2$ and $S_z$ satisfy \[S^2 \ket{s m} = \hbar^2 s (s + 1) \ket{s m}\] and \[S_z \ket{s m} = \hbar m \ket{s m}.\]

  \item Unlike the orbital angular momentum $\vec{L}$ where $\ell$ may only take on integer values, $s$ may take on half-integer values \[s = 0, \,\frac{1}{2}, \,1, \,\frac{3}{2}, \ldots; \ m = -s, \,-s + 1, \,\ldots, \,s - 1, \,s.\]

  \item Every elementary particle has a specific and immutable value of $s$ called the \textbf{spin} of that species: $\pi$ mesons have spin 0, electrons have spin $1 / 2$, etc. On the other hand the orbital angular momentum quantum number $\ell$ may take on any integer value and can change when the system is perturbed.
\end{itemize}

\subsubsection{Spin 1/2}

\begin{itemize}
  \item For spin-1/2 particles, $s = \frac{1}{2}$ and $m$ may take on two values: $-\frac{1}{2}$ or $\frac{1}{2}$. This means there are two eigenstates $\ket{s m}$: $\ket{\frac{1}{2} \frac{1}{2}}$ which is called \textbf{spin up} $\uparrow$ and $\ket{\frac{1}{2} \left( -\frac{1}{2} \right)}$ which is called \textbf{spin down} $\downarrow$.

  \item If we let \[\chi_+ = \begin{pmatrix}
            1 \\
            0
          \end{pmatrix}\] represent spin up and \[\chi_- = \begin{pmatrix}
            0 \\
            1
          \end{pmatrix}\] represent spin down the general state of a spin-1/2 particle can be expressed as \[\chi = \begin{pmatrix}
            a \\
            b
          \end{pmatrix} = a \chi_+ + b \chi_-.\]

  \item With respect to this basis operators become 2x2 matrices: \begin{align*}
          S^2 & = \frac{3}{4} \hbar^2 \begin{pmatrix}
                                        1 & 0 \\
                                        0 & 1
                                      \end{pmatrix},         \\
          S_x & = \frac{\hbar}{2} \begin{pmatrix}
                                    0 & 1 \\
                                    1 & 0 \\
                                  \end{pmatrix},             \\
          S_y & = \frac{\hbar}{2} \begin{pmatrix}
                                    0 & -i \\
                                    i & 0
                                  \end{pmatrix}, \text{ and} \\
          S_z & = \frac{\hbar}{2} \begin{pmatrix}
                                    1 & 0  \\
                                    0 & -1
                                  \end{pmatrix}.
        \end{align*}

  \item Since $S_x$, $S_y$, and $S_z$ all carry a factor of $\hbar / 2$ it is tidier to write $\vec{S} = \frac{\hbar}{2} \vec{\sigma}$ where \begin{align*}
          \sigma_x & = \begin{pmatrix}
                         0 & 1 \\
                         1 & 0
                       \end{pmatrix},             \\
          \sigma_y & = \begin{pmatrix}
                         0 & -i \\
                         i & 0
                       \end{pmatrix}, \text{ and} \\
          \sigma_z & = \begin{pmatrix}
                         1 & 0  \\
                         0 & -1
                       \end{pmatrix}.
        \end{align*} These are called the \textbf{Pauli spin matrices}.

  \item If you measure $S_z$ on a particle in a general state \[\chi = \begin{pmatrix}
            a \\
            b
          \end{pmatrix}\] there is a $|a|^2$ probability of getting $\hbar / 2$ and a $|b|^2$ probability of getting $-\hbar / 2$. Since these are the only possibilities $|a|^2 + |b|^2 = 1$.

  \item According to the generalised statistical interpretation of quantum mechanics, if you measure $S_x$ on a particle in a general state $\chi$ the possible results are the eigenvalues of the $S_x$ matrix. Solving the characteristic equation finds \[\begin{vmatrix}
            -\lambda  & \hbar / 2 \\
            \hbar / 2 & -\lambda
          \end{vmatrix} = 0 \Rightarrow \lambda^2 = \left( \frac{\hbar}{2} \right)^2 \Rightarrow \lambda = \pm \frac{\hbar}{2}\] as expected. The eigenvectors can be determined as usual \[\frac{\hbar}{2} \begin{pmatrix}
            0 & 1 \\
            1 & 0
          \end{pmatrix} \begin{pmatrix}
            \alpha \\
            \beta
          \end{pmatrix} = \pm \frac{\hbar}{2} \begin{pmatrix}
            \alpha \\
            \beta
          \end{pmatrix} \Rightarrow \begin{pmatrix}
            \beta \\
            \alpha
          \end{pmatrix} = \pm \begin{pmatrix}
            \alpha \\
            \beta
          \end{pmatrix}\] so $\beta = \pm \alpha$ and the normalised eigenvectors are \[\chi_+^{(x)} = \begin{pmatrix}
            1 / \sqrt{2} \\
            1 / \sqrt{2}
          \end{pmatrix}\] with eigenvalue $\hbar / 2$ and \[\chi_-^{(x)} = \begin{pmatrix}
            1 / \sqrt{2} \\
            -1 / \sqrt{2}
          \end{pmatrix}\] with eigenvalue $-\hbar / 2$.

  \item As the eigenvectors of a Hermitian matrix are linearly independent, any state \[\chi = \begin{pmatrix}
            a \\
            b
          \end{pmatrix}\] can be expressed as a linear combination of these eigenvectors \[\chi = \left( \frac{a + b}{\sqrt{2}} \right) \chi_+^{(x)} + \left( \frac{a - b}{\sqrt{2}} \right) \chi_-^{(x)}.\] Thus if you measure $S_x$ the probability of getting $\hbar / 2$ is $(1 / 2) |a + b|^2$ and the probability of getting $-\hbar / 2$ is $(1 / 2) |a - b|^2$.
\end{itemize}

\subsubsection{Electron in a Magnetic Field}

\begin{itemize}
  \item A spinning charged particle constitutes a magnetic dipole. Its magnetic dipole moment is proportional to its spin angular momentum \[\vec{\mu} = \gamma \vec{S}\] where $\gamma$ is called the \textbf{gyromagnetic ratio}.

  \item When a magnetic dipole is placed in a magnetic field $\vec{B}$ it experiences a torque $\vec{\mu} \times \vec{B}$ which tends to align it parallel to the field. The energy associated with this torque is \[H = -\vec{\mu} \cdot \vec{B}\] so the Hamiltonian matrix for a spinning charged particle at rest is \[H = -\gamma \vec{B} \cdot \vec{S}\] where \begin{align*}
          \vec{B} \cdot \vec{S} & = \begin{pmatrix}
                                      B_x & B_y & B_z
                                    \end{pmatrix} \cdot \begin{pmatrix}
                                                          S_x \\
                                                          S_y \\
                                                          S_z
                                                        \end{pmatrix}                           \\
                                & = \frac{\hbar}{2} (B_x \sigma_x + B_y \sigma_y + B_z \sigma_z)
        \end{align*} is a $2 \times 2$ matrix.
\end{itemize}

\subsubsection{Addition of Angular Momenta}

\begin{itemize}
  \item If you have two particles with spins $s_1$ and $s_2$, the first is in the state $\ket{s_1 m_1}$, and the second is in the state $\ket{s_2 m_2}$, the composite state can be denoted $\ket{s_1 s_2 m_1 m_2}$.

  \item The $z$ component of the angular momentum, $m$, of the composite state is \begin{align*}
          S_z \ket{s_1 s_2 m_1 m_2} & = S_z^{(1)} \ket{s_1 s_2 m_1 m_2} + S_z^{(2)} \ket{s_1 s_2 m_1 m_2} \\
                                    & = \hbar (m_1 + m_2) \ket{s_1 s_2 m_1 m_2}                           \\
                                    & = \hbar m \ket{s_1 s_2 m_1 m_2}
        \end{align*} where $m = m_1 + m_2$.

  \item If you have two spin-1/2 particles, each can be spin up or down giving four possible states: $\ket{\uparrow \uparrow}$, $\ket{\uparrow \downarrow}$, $\ket{\downarrow \uparrow}$, and $\ket{\downarrow \downarrow}$. The $m$ value for each of these states is the sum of the $m$ values of each particle, giving $1$, $0$, $0$, and $-1$, respectively. In a quantum system we expect the $m$ value to range from $-s$ to $+s$ in integer increments. This suggests that for this system $s = 1$ but we have two states where $m = 0$. If we apply the lowering operator to $\ket{\uparrow \uparrow}$ we get $\hbar (\ket{\uparrow \downarrow} + \ket{\downarrow \uparrow})$. Due to the nature of the lowering operator we know that this state has $s = 1$ and $m = 0$ so the states $\ket{\uparrow \uparrow}$, $\frac{1}{\sqrt{2}} (\ket{\uparrow \downarrow} + \ket{\downarrow \uparrow})$, and $\ket{\downarrow \downarrow}$ form a \textbf{triplet} combination for which $s = 1$. We have one remaining state for which $m = 0$. As $m$ ranges from $-s$ to $+s$ this suggests that we need to find a linear combination of the basis states for which $s = 0$ — this turns out to be $\frac{1}{\sqrt{2}} (\ket{\uparrow \downarrow} - \ket{\downarrow \uparrow})$ and is called a \textbf{singlet state}. If we apply the $S^2$ operator to the triplet states we get $1 (1 + 1) \hbar^2 = 2 \hbar^2$ and if we apply it to the singlet state we get $0 (0 + 1) \hbar^2 = 0$ as expected.

  \item In general, if you combine spin $s_1$ with spin $s_2$ the resulting system can have spins $s_1 + s_2$, $s_1 + s_2 - 1$, $s_1 + s_2 - 2$, $\ldots$, $|s_1 - s_2|$. The greatest total spin occurs when the particles spins are parallel then lowers in integer decrements as one of the particles moves through its possible spin states until the lowest total spin occurs when they're antiparallel.

  \item The state of a composite system $\ket{s m}$ will be a linear combination of the basis states for that system. For the two spin-1/2 particle system above this is most obvious for the singlet state and the $m = 0$ triplet state. In general, \[\ket{s m} = \sum_{m_1 + m_2 = m} C_{m_1 m_2 m}^{s_1 s_2 s} \ket{s_1 s_2 m_1 m_2}\] where $C_{m_1 m_2 m}^{s_1 s_2 s}$ are called the \textbf{Clebsch-Gordan coefficients} and the only states that contribute are those for which $m_1 + m_2 = m$.
\end{itemize}

\subsection{Electromagnetic Interactions}

\subsubsection{Minimal Coupling}

\begin{itemize}
  \item The Hamiltonian for a particle of charge $q$ and momentum $\vec{p}$ in the presence of electromagnetic fields is \[H = \frac{1}{2 m} (\vec{p} - q \vec{A})^2 + q \varphi\] where $\vec{A}$ is the vector potential and $\varphi$ is the scalar potential: \[\vec{E} = -\nabla \varphi - \frac{\partial \vec{A}}{\partial t}, \ \vec{B} = \nabla \times \vec{A}.\] Making the substitution $\vec{p} \rightarrow -i \hbar \nabla$ we obtain the Hamiltonian operator \[\hat{H} = \frac{1}{2 m} (-i \hbar \nabla - q \vec{A})^2 + q \varphi\] and the Schrödinger equation becomes \[i \hbar \frac{\partial \Phi}{\partial t} = \left[ \frac{1}{2 m} (-i \hbar \nabla - q \vec{A})^2 + q \varphi \right] \Psi.\] This is the quantum implementation of the Lorentz force law; it is sometimes called the \textbf{minimal coupling rule}.
\end{itemize}

\section{Identical Particles}

\subsection{Two-Particle Systems}

\begin{itemize}
  \item For a single particle $\Psi(\vec{r}, t)$ is a function of the spatial coordinates $\vec{r}$ and the time $t$. For two particles $\Psi(\vec{r}_1, \vec{r}_2, t)$ is a function of the coordinates of the particles $\vec{r}_1$ and $\vec{r}_2$, and the time $t$.

  \item The evolution of a two-particle wavefunction is determined by the \\ Schrödinger equation \[i \hbar \frac{\partial \Psi}{\partial t} = \hat{H} \Psi\] where \[\hat{H} = -\frac{\hbar^2}{2 m_1} \nabla_1^2 - \frac{\hbar^2}{2 m_2} \nabla_2^2 + V(\vec{r}_1, \vec{r}_2, t).\]

  \item The statistical interpretation is similar where \[|\Psi(\vec{r}_1, \vec{r}_2, t)|^2 \,d^3 \vec{r}_1 \,d^3 \vec{r}_2\] is the probability of finding particle one in the volume $d^3 \vec{r}_1$ and particle two in the volume $d^3 \vec{r}_2$.

  \item As always, the wavefunction must be normalised \[\int |\Psi(\vec{r}_1, \vec{r}_2, t)|^2 \,d^3 \vec{r}_1 \,d^3 \vec{r}_2 = 1.\]

  \item For time-independent potentials we can obtain a complete set of solutions by separation of variables \[\Psi(\vec{r}_1, \vec{r}_2, t) = \psi(\vec{r}_1, \vec{r}_2) e^{-i E t / \hbar}\] where the spatial wavefunciton $\psi$ satisfies the time-independent \\ Schrödinger equation \[-\frac{\hbar^2}{2 m_1} \nabla_1^2 \psi - \frac{\hbar^2}{2 m_2} \nabla_2^2 \psi + V \psi = E \psi.\]

  \item In general, solving the time-independent Schrödinger equation for two particles is difficult but two special cases can be reduced to one-particle problems:

        \begin{enumerate}
          \item \textbf{Noninteracting particles}. In this case the potential energy can be expressed as \[V(\vec{r}_1, \vec{r}_2) = V_1(\vec{r}_1) + V_2(\vec{r}_2)\] and the equation can be solved via separation of variables \[\psi(\vec{r}_1, \vec{r}_2) = \psi_a(\vec{r}_1) \psi_b(\vec{r}_2).\] Plugging this in to the time-independent Schrödinger equation and collecting terms in $\vec{r}_1$ or $\vec{r}_2$ alone we find that $\psi_a$ and $\psi_b$ each satisfy the one-particle Schrödinger equation and the two-particle wave function is the product of the one-particle functions \begin{align*}
                  \Psi(\vec{r}_1, \vec{r}_2, t) & = \psi_a(\vec{r}_1) \psi_b(\vec{r}_2) e^{-i E_a t / \hbar} e^{-i E_b t / \hbar}                               \\
                                                & = \left( \psi_a(\vec{r}_1) e^{-i E_a t / \hbar} \right) \left( \psi_b(\vec{r}_2) e^{-i E_b t / \hbar} \right) \\
                                                & = \Psi_a(\vec{r}_1, t) \Psi_b(\vec{r}_2, t).
                \end{align*} In this case it makes sense to say that particle 1 is in state $a$ and particle 2 is in state $b$. However, any linear combination of these kind of solutions will also satisfy the time-dependent Schrödinger equation, e.g. \[\Psi(\vec{r}_1, \vec{r}_2 ,t) = \frac{3}{5} \Psi_a(\vec{r}_1, t) \Psi_b(\vec{r}_2, t) + \frac{4}{5} \Psi_c(\vec{r}_1, t) \Psi_d(\vec{r}_2, t).\] In this case, the state of particle 1 depends on the state of particle 2 and vice versa in the sense that if you measure that particle 1 is in state $a$ you know particle 2 is in state $b$ (ditto for states $c$ and $d$). We say that the two particles are \textbf{entangled}. An entangled state is one that cannot be written as the product to single-particle states.

          \item \textbf{Central potentials}. If the two particles only interact with one another via a potential that depends on their separation \[V(\vec{r}_1, \vec{r}_2) \rightarrow V(|\vec{r}_1 - \vec{r}_2|)\] the two-body problem reduces to an equivalent one-body problem as it does in classical mechanics. In general, though, the two particles will be subject to other forces that complicates the analysis.
        \end{enumerate}
\end{itemize}

\subsubsection{Bosons and Fermions}

\begin{itemize}
  \item As in the previous section, if you have two noninteracting particles the system's wavefunction is the product of the one-particle wavefunctions \[\psi(\vec{r}_1, \vec{r}_2) = \psi_a(\vec{r}_1) \psi_b(\vec{r}_2).\] However, this assumes that you can distinguish between the two particles upon measurement. If you can't — for example if the system consists of two electrons — the wavefunction can be updated to be noncommittal as to which particle is in which state \[\psi_\pm(\vec{r}_1, \vec{r}_2) = A [\psi_a(\vec{r}_1) \psi_b(\vec{r}_2) \pm \psi_b(\vec{r}_1) \psi_a(\vec{r}_2)],\] i.e. if you measure a particle to be in state $a$ it could be either particle.

  \item This theory admits two kinds of particles: \textbf{bosons} (the plus sign), and \textbf{fermions} (the minus sign). Bosons are \textbf{symmetric} upon interchange ($\psi_+(\vec{r}_1, \vec{r}_2) = \psi_+(\vec{r}_2, \vec{r}_1)$) whereas fermions are \textbf{antisymmetric} upon interchange ($\psi_-(\vec{r}_1, \vec{r}_2) = -\psi_-(\vec{r}_2, \vec{r}_1)$).

  \item All particles with integer spins are bosons and all particles with half integer spins are fermions.

  \item It follows that two identical fermions can't occupy the same state, for if $\psi_a = \psi_b$ then \[\psi_-(\vec{r}_1, \vec{r}_2) = A[\psi_a (\vec{r}_1) \psi_a (\vec{r}_2) - \psi_a(\vec{r}_1) \psi_a(\vec{r}_1)] = 0\] the wavefunction disappears. This is the \textbf{Pauli exclusion principle} and it applies to all fermions.
\end{itemize}

\subsubsection{Exchange Forces}

\begin{itemize}
  \item For distinguishable particles, the expectation value of the squared distance between them is \[\braket{(x_1 - x_2)^2} = \braket{x^2}_a + \braket{x^2}_b - 2 \braket{x}_a \braket{x}_b.\]

  \item For identical particles, the expectation value of the squared distance between them is \[\braket{(x_1 - x_2)^2}_\pm = \braket{x^2}_a + \braket{x^2}_b - 2 \braket{x}_a \braket{x}_b \mp 2 |\braket{x}_{a b}|^2\] where \[\braket{x}_{a b} = \int x \psi_a(x)^* \psi_b(x) \,d x.\]

  \item The difference between the two above expectation values is $\mp 2 |\braket{x}_{a b}|^2$. This shows that identical bosons (the upper sign) tend to be slightly closer than distinguishable particles and identical fermions (the lower sign) tend to be slightly further away. The system behaves as if there were a ``force of attraction'' between identical bosons and a ``force of repulsion'' between identical fermions. This is called an \textbf{exchange force}.

  \item Note that $\braket{x}_{a b}$ vanishes if there is no overlap in the wavefunctions — if $\psi_a$ is $0$ whenever $\psi_b$ is nonzero and vice versa, the intergal will be zero. The consequence of this is that the particles need to be relatively close together in order for this effect to occur.
\end{itemize}

\subsubsection{Spin}

\begin{itemize}
  \item The complete state of, say, an electron includes both its position wave function and a spinor describing the orientation of its spin \[\psi(\vec{r}) \chi.\]

  \item For a two-particle state \[\psi(\vec{r}_1, \vec{r}_2) \chi(1, 2)\] the whole state (not just the spatial part) must be antisymmetric with respect to exchange \[\psi(\vec{r}_1, \vec{r}_2) \chi(1, 2) = -\psi(\vec{r}_2, \vec{r}_1) \chi(2, 1).\]

  \item Because the singlet composite spin state is antisymmetric, it would need to be joined with a symmetric spatial function. Because the triplet composite spin state is symmetric, it would need to be joined with an antisymmetric spatial function. The former means that two electrons can actually occupy the same position state providing they have different spins.
\end{itemize}

\subsubsection{Generalized Symmetrization Principle}

\begin{itemize}
  \item The \textbf{exchange operator} $\hat{P}$ exchanges two particles \[\hat{P} \ket{(1, 2)} = \ket{(2, 1)}.\]

  \item The \textbf{symmetrization axiom} states that for identical particles is its required that \[\ket{(1, 2)} = \pm \ket{(2, 1)}\] with the plus sign for bosons and the minus sign for fermions.

  \item If you have $n$ particles then the state must be symmetric or antisymmetric for exchange of any two.
\end{itemize}

\subsection{Atoms}

\begin{itemize}
  \item A neutral atom of atomic number $Z$ consists of a heavy nucleus with electric charge $Z e$ surrounded by $Z$ electrons of mass $m$ and charge $-e$. The Hamiltonian for this system is \[\hat{H} = \sum_j^Z \left\{ -\frac{\hbar^2}{2 m} \nabla_j^2 - \left( \frac{1}{4 \pi \epsilon_0} \right) \frac{Z e^2}{r_j} \right\} + \frac{1}{2} \left( \frac{1}{4 \pi \epsilon_0} \right) \sum_{j \ne k}^Z \frac{e^2}{|\vec{r}_j - \vec{r}_k|}\] where the term in curly brackets represents the kinetic and potential energy of the $j$th electron in the electric field of the nucleus and the second sum is the potential energy associated with the mutual repulsion of the electrons. This equation cannot be solved exactly except for $Z = 1$ (hydrogen).
\end{itemize}

\subsubsection{Helium}

\begin{itemize}
  \item After hydrogen, the simplest atom is helium with $Z = 2$. Its Hamiltonian is \[\hat{H} = \left\{ -\frac{\hbar^2}{2 m} \nabla_1^2 - \frac{1}{4 \pi \epsilon_0} \frac{2 e^2}{r_1} \right\} + \left\{ -\frac{\hbar^2}{2 m} \nabla_2^2 - \frac{1}{4 \pi \epsilon_0} \frac{2 e^2}{r_2} \right\} + \frac{1}{4 \pi \epsilon_0} \frac{e^2}{|\vec{r}_1 - \vec{r}_2|}.\] If we ignore the last term the Schrödinger equation becomes separable and the solutions can be written as the product of hydrogen wavefunctions \[\psi(\vec{r}_1, \vec{r}_2) = \psi_{n \ell m}(\vec{r}_1) \psi_{n' \ell' m}(\vec{r}_2)\] except they have half the Bohr radius and four times the Bohr energies.

  \item The ground state would be \[\psi_0(\vec{r}_1, \vec{r}_2) = \psi_{1 0 0}(\vec{r}_1) \psi_{1 0 0}(\vec{r}_2) = \frac{8}{\pi a^3} e^{-2 (r_1 + r_2) / a}.\] Because we're dealing with two spin-1/2 particles, the Pauli exclusion principle demands that the total wavefunction is antisymmetric. $\psi_0$ is symmetric so the spin state must be antisymmetric and thus the ground state of helium is in the singlet configuration.

  \item The excited states of helium consist of one electron in the hydrogenic ground state and the other in an excited state \[\psi = \psi_{n \ell m} \psi_{1 0 0}.\] If both electrons were in an excited state, only would immediately drop to ground and release enough energy to ionise the atom. Because the electrons are now in different states the Pauli exclusion principle is satisfied and they can be in both singlet and triplet spin states. The former are called \textbf{parahelium} and the latter are called \textbf{orthohelium}.
\end{itemize}

\end{document}