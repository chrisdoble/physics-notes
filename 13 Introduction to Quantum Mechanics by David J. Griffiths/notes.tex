\documentclass{article}
\usepackage{amsfonts} % For mathbb
\usepackage{amsmath} % For align*
\usepackage{bookmark} % For links
\usepackage{braket} % For bra-ket notation
\usepackage{float} % For the [H] option on figures
\usepackage{graphicx} % For images

\graphicspath{{./images/}}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue
}

\newcommand{\ev}[1]{\langle #1 \rangle}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}

\title{Introduction to Quantum Mechanics by David J. Griffiths Notes}
\author{Chris Doble}
\date{March 2024}

\begin{document}

\maketitle

\tableofcontents

\part{Theory}

\section{The Wave Function}

\subsection{The Schrödinger Equation}

\begin{itemize}
  \item The \textbf{Schrödinger equation} \[i \hbar \frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2 m} \frac{\partial^2 \Psi}{\partial x^2} + V \Psi\] is to quantum mechanics what Newton's second law is to classical mechanics. Given suitable initial conditions — typically $\Psi(x, 0)$ — the Schrödinger equation determines $\Psi(x, t)$ for all future time.
\end{itemize}

\subsection{The Statistical Interpretation}

\begin{itemize}
  \item The \textbf{Born rule} states that $|\Psi(x, t)|^2$ gives the probability of finding the particle at point $x$ at time $t$ or \[\int_a^b |\Psi(x, t)|^2 \,d x\] gives the probability of finding the particle between $a$ and $b$ at time $t$.

  \item This statistical interpretation introduces indeterminacy to quantum mechanics — we can't predict with certainty the particle's position.

  \item Suppose we measure a particle's position to be $C$. Where was it before we took the measurement? In the past there were three main schools of thought:

        \begin{enumerate}
          \item The \textbf{realist} position believes that the particle was at $C$ but $\Psi$ doesn't give us enough information to determine that — there's another \textbf{hidden variable} that would allow us to.

          \item The \textbf{orthodox} position (also known as the \textbf{Copenhagen interpretation}) believes that the particle didn't have a definite position but the act of measuring it forced it to do so.

          \item The \textbf{agnostic} position believes that it doesn't matter and is potentially unknowable.
        \end{enumerate}

  \item \textbf{Bell's theorem} confirms the orthodox interpretation.

  \item If we take two consecutive measurements of a particle, they will both yield the same result. The first measurement causes the wavefunction to \textbf{collapse} such that it is peaked only at the particle's measured location. If the system is allowed to evolve between the measurements the wavefunction will ``spread out'' but if done in quick succession the result won't change.
\end{itemize}

\subsection{Probability}

\subsubsection{Discrete Variables}

\begin{itemize}
  \item The average value of a discrete variable $j$ is \[\ev{j} = \frac{\sum j N(j)}{N} = \sum_{j = 0}^\infty j P(j)\] where $N$ is the size of the population, $N(j)$ is the number of $j$s in the population, and $P(j)$ is the probability of randomly selecting a $j$ from the population.

  \item In quantum mechanics the average is usually the quantity of interest and it is called the \textbf{expectation value} (even though it may not be the most probable value).

  \item The average value of a function $f$ of a discrete variable $j$ is \[\ev{f(j)} = \sum_{j = 0}^\infty f(j) P(j).\]

  \item Two distributions could have the same median, mean, mode, and size but be spread out differently. One way to quantify this could be to calculate how far each element is from the average \[\Delta j = j - \ev{j}\] and calculate the average of $\Delta j$. However, it ends up being zero \begin{align*}
          \langle \Delta j \rangle & = \sum (j - \ev{j}) P(j)         \\
                                   & = \sum j P(j) - \ev{j} \sum P(j) \\
                                   & = \ev{j} - \ev{j}                \\
                                   & = 0.
        \end{align*} To avoid this we calculate the average of the square of $\Delta j$ \[\sigma^2 = \langle (\Delta j)^2 \rangle\] which is known as the \textbf{variance} of the distribution.

  \item The square root of the variance is called the \textbf{standard deviation}.

  \item A useful theorem on variances is \begin{align*}
          \sigma^2 & = \langle (\Delta j)^2 \rangle                              \\
                   & = \sum (\Delta j)^2 P(j)                                    \\
                   & = \sum (j - \ev{j})^2 P(j)                                  \\
                   & = \sum (j^2 - 2 j \ev{j} + \ev{j}^2) P(j)                   \\
                   & = \sum j^2 P(j) - 2 \ev{j} \sum j P(j) + \ev{j}^2 \sum P(j) \\
                   & = \ev{j^2} - 2 \ev{j}^2 + \ev{j}^2                          \\
                   & = \ev{j^2} - \ev{j}^2
        \end{align*} and thus the standard deviation can be calculated as \[\sigma = \sqrt{\ev{j^2} - \ev{j}^2}\] which is usually easier than the full formula.
\end{itemize}

\subsubsection{Continuous Variables}

\begin{itemize}
  \item The equations above translate as expected to continuous variables: \begin{align*}
          P_{a b}   & = \int_a^b \rho(x) \,d x                     \\
          1         & = \int_{-\infty}^{+\infty} \rho(x) \,d x     \\
          \ev{x}    & = \int_{-\infty}^{+\infty} x \rho(x) \,d x   \\
          \ev{f(x)} & = \int{-\infty}^{+\infty} f(x) \rho(x) \,d x \\
          \sigma^2  & = \ev{x^2} - \ev{x}^2
        \end{align*}
\end{itemize}

\subsection{Normalization}

\begin{itemize}
  \item In order for the statistical interpretation of the wavefunction to make sense, it must be the case that \[\int_{-\infty}^\infty |\Psi(x, t)|^2 \,d x = 1,\] i.e. the particle must be somewhere.

  \item The process of multiplying a candidate wavefunction by a complex constant $A$ to make this hold is called \textbf{normalization}.

  \item For some solutions to the Schrödinger equation, the integral is infinite in which case there is no $A$ that normalizes the wavefunction. The same goes for $\Psi = 0$. Such \textbf{non-normalizable} solutions can't represent particles so they must be rejected.

  \item But if we normalize the wavefunction at $t = 0$, how do we know it stays normalized? It turns out that the integral \[\int_{-\infty}^\infty |\Psi(x, t)|^2 \,d x\] is constant (independent of time) so if it's normalized at $t = 0$ it stays normalized.
\end{itemize}

\subsection{Momentum}

\begin{itemize}
  \item What does \[\ev{x} = \int_{-\infty}^\infty x |\Psi(x, t)|^2 \,d x\] mean? It doesn't mean that if you measure the position of a particle over and over again and take the average of the results you'll get $\int x |\Psi|^2 \,d x$. The first measurement causes the wavefunction to collapse so you'll get the same measurement each time. Instead, it means if you have a large number of particles all in the same state $\Psi$, measure the position of each of them, and take the average of the results you'll get $\int x |\Psi|^2 \,d x$.

  \item The expectation value of the velocity is equal to the time derivative of the expectation value of the position \[\ev{v} = \frac{d \ev{x}}{d t} = -\frac{i \hbar}{m} \int \Psi^* \frac{\partial \Psi}{\partial x} \,d x\] however it is customary to work with momentum $p = m v$ rather than velocity \[\ev{p} = m \frac{d \ev{x}}{d t} = -i \hbar \int \Psi^* \frac{\partial \Psi}{\partial x} \,d x.\]

  \item The above can be rewritten in the form \begin{align*}
          \ev{x} & = \int \Psi^* [x] \Psi \,d x                                                  \\
          \ev{p} & = \int \Psi^* \left[ -i \hbar \frac{\partial}{\partial x} \right] \Psi \,d x.
        \end{align*} The values in the square brackets are called \textbf{operators} and we say that the operator $x$ ``represents'' position and the operator $-i \hbar (\partial / \partial x)$ ``represents'' momentum. To calculate expectation values we place the appropriate operator between $\Psi^*$ and $\Psi$ and integrate.

  \item All other values of interest can be expressed in terms of position and momentum, e.g. kinetic energy is \[T = \frac{1}{2} m v^2 = \frac{p^2}{2 m}.\] Their operators can be determined by substituting $p = -i \hbar (\partial / \partial x)$, e.g. kinetic energy is \[-\frac{\hbar^2}{2 m} \frac{\partial^2}{\partial x^2}\] such that \[\ev{T} = -\frac{\hbar^2}{2 m} \int \Psi^* \frac{\partial^2 \Psi}{\partial x^2} \,d x.\] In general \[\ev{Q(x, p)} = \int \Psi^* \left[ Q(x, -i \hbar \partial / \partial x) \right] \Psi \,d x.\]
\end{itemize}

\subsection{The Uncertainty Principle}

\begin{itemize}
  \item If a wave is spread out (e.g. if you shake a rope up and down repeatedly), its position isn't well defined but its wavelength is. On the other hand, if a wave is localised (e.g. if you shake a rope up and down once to make a spike), its position is well defined but its wavelength isn't.

  \item The momentum of a particle is related to its wavelength by the \textbf{de Broglie formula} \[p = \frac{h}{\lambda} = \frac{2 \pi \hbar}{\lambda}.\]

  \item If a wavefunction is periodic its position isn't well defined but its wavelength is and thus so is its momentum. If a wavefunction is localised its position is well defined but its wavelength isn't and thus neither is its momentum. This is formalised by the \textbf{Hiesenberg uncertainty principle} \[\sigma_x \sigma_p \ge \frac{\hbar}{2}\] where $\sigma_x$ and $\sigma_p$ are the standard deviations in $x$ and $p$, respectively.
\end{itemize}

\section{Time-Independent Schrödinger Equation}

\subsection{Stationary States}

\begin{itemize}
  \item The Schrödinger equation \[i \hbar \frac{\partial \Psi}{\partial t} = -\frac{\hbar^2}{2 m} \frac{\partial^2 \Psi}{\partial x^2} + V \Psi\] can be solved via separation of variables if $V$ is independent of $t$. In that case we assume $\Psi(x, t) = \psi(x) \varphi(t)$ and separation gives the two ODEs \[i \hbar \frac{1}{\varphi} \frac{d \varphi}{d t} = E\] which has no name and \[-\frac{\hbar^2}{2 m} \frac{\partial^2 \psi}{d x^2} + V \psi = E \psi\] which is called the \textbf{time-independent Schrödinger equation}. The solution to the former is \[\varphi(t) = C e^{-i E t / \hbar}\] but the constant $C$ can be absorbed into $\psi$. The latter can't be solved until $V$ is specified.

  \item While there are many solutions to the Schrödinger equation that don't have the form $\psi(x) \varphi(t)$, those that do have three interesting properties:

        \begin{enumerate}
          \item They are stationary states. The wave function has the form \[\Psi(x, t) = \psi(x) e^{-i E t / \hbar}\] so the probability density is \[|\Psi|^2 = \Psi^* \Psi = \psi^* e^{i E t / \hbar} \psi e^{-i E t / \hbar} = |\psi|^2,\] i.e. it's constant in time. Similarly, time dependence drops out of the expectation value formula so they're also constant in time.

          \item They are states of definite total energy. In classical mechanics the total energy is called the Hamiltonian \[H(x, p) = \frac{p^2}{2 m} + V(x).\] The equivalent quantum operator is \[\hat{H} = -\frac{\hbar^2}{2 m} \frac{\partial^2}{\partial x^2} + V(x)\] thus the time-independent Schrödinger equation can be written \[\hat{H} \psi = E \psi\] and the expectation value of the total energy can be written \[\ev{H} = \int \psi^* \hat{H} \psi \,d x = E \int |\psi|^2 \,d x = E \int |\Psi|^2 \,d x = E.\] Moreover, \[\hat{H}^2 \psi = \hat{H} (\hat{H} \psi) = \hat{H} (E \psi) = E (\hat{H} \psi) = E^2 \psi\] so \[\ev{H^2} = \int \psi^* \hat{H}^2 \psi \,d x = E^2 \int |\psi|^2 \,d x = E^2 \int |\Psi|^2 \,dx = E^2.\] Thus the variance of $H$ is \[\sigma_H^2 = \ev{H^2} - \ev{H}^2 = 0\] meaning every measurement of the total energy of the system will return the same value $E$.

          \item The general solution to the time-dependent Schrödinger equation is a linear combination of separable solutions. The time-independent Schrödinger equation yields an infinite number of solutions $\psi_1(x)$, $\psi_2(x)$, $\ldots$, $\psi_n(x)$ each with its associated separation constant $E_1$, $E_2$, $\ldots$, $E_n$ — one for each \textbf{allowed energy}. The general solution is then given by \[\Psi(x, t) = \sum_{n = 1}^\infty c_n \psi_n(x) e^{-i E_n t / \hbar}\] with appropriate choice of $c_n$ to fit the initial conditions.
        \end{enumerate}

  \item The general approach to solving the time-dependent Schrödinger equation with given initial conditions and potential energy function is:

        \begin{enumerate}
          \item Solve the time-independent Schrödinger equation to get an infinite set of solutions $\psi_n(x)$.

          \item Fit the linear combination of those solutions \[\Psi(x, 0) = \sum_{n = 1}^\infty c_n \psi_n(x)\] to the initial conditions.

          \item Multiply each term in the sum by its associated time dependence \[\Psi(x, t) = \sum_{n = 1}^\infty c_n \psi_n(x) e^{-i E_n t / \hbar} = \sum_{n = 1}^\infty c_n \Psi_n(x, t).\]
        \end{enumerate}

  \item The physical meaning of the constants $c_n$ is thus: $|c_n|^2$ is the probability that a measurement of the system's energy would return the value $E_n$. This means that \[\sum_{n = 1}^\infty |c_n|^2 = 1\] and \[\ev{H} = \sum_{n = 1}^\infty |c_n|^2 E_n.\] The probability of measuring any particular energy is constant in time and thus the expectation value of $H$ — this is part of energy conservation.

  \item For a solution to the Schrödinger equation to be normalisable, $E$ must be real and must exceed the minimum value of $V$.
\end{itemize}

\subsection{The Infinite Square Well}

\begin{itemize}
  \item The potential of an infinite square well is \[V(x) = \begin{cases}
            0      & 0 \le x \le a    \\
            \infty & \text{otherwise}
          \end{cases}\] and thus $\Psi(x, t) = 0$ for $x$ outside $[0, a]$.

  \item Inside such a well the time-independent Schrödinger equation reads \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} = E \Psi\] where $E$ must be real and greater than $0$. This can be rearranged to \[\frac{d^2 \psi}{d x^2} = -k^2 \psi\] where \[k = \frac{\sqrt{2 m E}}{\hbar}.\] This has the form of a simple harmonic oscillator and has the solution \[\psi = A \sin (k x) + B \cos (k x).\] Applying the boundary condition $\psi(0) = 0$ gives $B = 0$ and thus \[\psi = A \sin (k x).\] Applying the boundary condition $\psi(a) = 0$ gives $A \sin (k a) = 0$. We must reject $A = 0$ as that would result in the non-normalisable solution $\psi = 0$, so $\sin (k a) = 0$ and thus $k a = 0, \,\pm \pi, \,\pm 2 \pi, \,\ldots$ Again, $k = 0$ results in $\psi = 0$ so it must be rejected and we're left with \[\psi = A \sin (k_n x)\] where \[k_n = \frac{n \pi}{a}, \,n \in \mathbb{Z}^+.\]

  \item From the above, the possible values of $E$ are \[E_n = \frac{\hbar^2 k_n^2}{2 m} = \frac{n^2 \pi^2 \hbar^2}{2 m a^2}, \,n \in \mathbb{Z}^+.\] In other words, unlike in classical mechanics, a quantum particle in an infinite square well can't have any energy — it must be one of these values.

  \item To determine the value of the constant $A$ above we normalise $\psi$ \[\int_0^a |A|^2 \sin^2 (k x) \,d x = |A|^2 \frac{a}{2} = 1.\] This only determines the magnitude of $A$ but the phase doesn't affect anything so we might as well pick \[A = \sqrt{\frac{2}{a}}\] and the solutions to the time-independent Schrödinger equation inside the infinite square well are \[\psi_n(x) = \sqrt{\frac{2}{a}} \sin \left( \frac{n \pi}{a} x \right).\]

  \item The solutions $\psi_n$ have some interesting properties:

        \begin{enumerate}
          \item They alternate even and odd with respect to the centre of the well.

          \item Each subsequent solution has one more node (crossing of the axis).

          \item They are mutually orthogonal, i.e. \[\int \psi_m^* \psi_n \,d x = \begin{cases}
                    0 & m \ne n \\
                    1 & m = n
                  \end{cases}.\]

          \item The are complete, in that any other function $f(x)$ can be expressed as a linear combination of them \[f(x) = \sum_{n = 1}^\infty c_n \psi_n(x) = \sqrt{\frac{2}{a}} \sum_{n = 1}^\infty c_n \sin \left( \frac{n \pi}{a} x \right)\] where \[c_n = \int \psi_n(x)^* f(x) \,d x.\]
        \end{enumerate}

  \item The stationary states of the infinite square well are thus \[\Psi_n(x, t) = \sqrt{\frac{2}{a}} \sin \left( \frac{n \pi}{a} x \right) e^{-i (n^2 \pi^2 \hbar / 2 m a^2) t}\] and the general solution to the time-dependent Schrödinger equation is \[\Psi(x, t) = \sum_{n = 1}^\infty c_n \Psi_n(x, t)\] where $c_n$ are given by \[c_n = \sqrt{\frac{2}{a}} \int_0^a \sin \left( \frac{n \pi}{a} x \right) \Psi(x, 0) \,d x.\]
\end{itemize}

\subsection{The Harmonic Oscillator}

\begin{itemize}
  \item The equation of motion for a harmonic oscillator is \[\frac{d^2 x}{d t^2} = -k x\] where $k$ is the force constant and $x$ is the displacement from equilibrium. The solution to this differential equation is \[x = A \sin (\omega t) + B \cos (\omega t)\] where $\omega = \sqrt{k / m}$ is the angular frequency of oscillation.

  \item The potential energy of a harmonic oscillator is \[V(x) = \frac{1}{2} k x^2\] which is a parabola.

  \item Any potential function can be approximated by a parabola in the neighborhood of a local minimum $x_0$ providing the amplitude is small by expanding it as a Taylor series, dropping the first term as it's constant and doesn't affect the force, dropping the second term because $V'(x_0) = 0$, keeping the third term, and dropping higher order terms.

  \item The quantum problem is to solve the Schrödinger equation for potential \[V(x) = \frac{1}{2} m \omega^2 x^2\] where $k$ has been replaced with $m \omega^2$. The first step is to solve the time-independent Schrödinger equation: \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} + \frac{1}{2} m \omega^2 x^2 \psi = E \psi.\] This is typically done in one of two ways: a clever algebraic technique using ladder-operators, and a more straightforward ``brute-force'' technique using power series.
\end{itemize}

\subsubsection{Algebraic Method}

\begin{itemize}
  \item We can factor the time-independent Schrödinger equation above as \[\frac{1}{2 m} \left[ -\hbar^2 \frac{d^2}{d x^2} + (m \omega x)^2 \right] \psi = \frac{1}{2 m} [\hat{p}^2 + (m \omega x)^2] = E \psi.\] As the Hamiltonian is equal to the total energy of the system, this means \[\hat{H} = \frac{1}{2 m} [\hat{p}^2 + (m \omega x)^2].\]

  \item This form invites factorisation of the form $u^2 + v^2 = (i u + v) (-i u + v)$ however $\hat{p}$ and $x$ are operators and they do not, in general, commute (i.e. $\hat{p} x \ne x \hat{p}$). We can try anyway by introducing the quantities \[\hat{a}_\pm = \frac{1}{\sqrt{2 \hbar m \omega}} (\mp i \hat{p} + m \omega x).\] Their product is \begin{align*}
          \hat{a}_- \hat{a}+ & = \frac{1}{2 \hbar m \omega} (i \hat{p} + m \omega x) (-i \hat{p} + m \omega x)                 \\
                             & = \frac{1}{2 \hbar m \omega} [\hat{p}^2 + (m \omega x)^2 - i m \omega (x \hat{p} - \hat{p} x)].
        \end{align*} The extra term involving $x \hat{p} - \hat{p} x$ is called the \textbf{commutator} of $x$ and $\hat{p}$ — it's a measure of how badly they fail to commute.

  \item In general, the commutator of two operators $\hat{A}$ and $\hat{B}$ is \[[\hat{A}, \hat{B}] = \hat{A} \hat{B} - \hat{B} \hat{A}.\] It's often easier to calculate the commutator of two operators if you introduce a test function $f$, expand, and remove $f$ at the end.

  \item Using this notation, the product above can be rewritten \[\hat{a}_- \hat{a}_+ = \frac{1}{2 \hbar m \omega} [\hat{p}^2 + (m \omega x)^2] - \frac{i}{2 \hbar} [x, \hat{p}].\]

  \item It turns out that \[[x, \hat{p}] = i \hbar.\] This equation is called the \textbf{canonical commutation relation} and using it we find \[\hat{a}_- \hat{a}_+ = \frac{1}{\hbar \omega} \hat{H} + \frac{1}{2}\] or \[\hat{H} = \hbar \omega \left( \hat{a}_- \hat{a}_+ - \frac{1}{2} \right).\]

  \item Note that the order of the operators is important. Going through the same process for $\hat{a}_+ \hat{a}_-$ gives \[\hat{a}_+ \hat{a}_- = \frac{1}{\hbar \omega} \hat{H} - \frac{1}{2}\] and \[\hat{H} = \hbar \omega \left( \hat{a}_+ \hat{a}_- + \frac{1}{2} \right).\]

  \item If $\psi$ satisfies the Schrödinger equation with energy $E$, i.e. $\hat{H} \psi = E \psi$, then $\hat{a}_+ \psi$ satisfies it with energy $E + h \omega$ \[\hat{H} (\hat{a}_+ \psi) = (E + h \omega) \psi\] and $\hat{a}_- \psi$ satisfies it with energy $E - h \omega$ \[\hat{H} (\hat{a}_- \psi) = (E - h \omega) \psi.\]

  \item $\hat{a}_\pm$ are called \textbf{ladder operators} because, if we know one solution to the Schrödinger equation, they can be used to climb up and down the energy ladder.

  \item However, we can't apply the lowering operator $\hat{a}_-$ an infinite number of times — eventually we'll reach a state of zero energy. The ``lowest rung'' $\psi_0$ occurs when $\hat{a}_- \psi_0 = 0$. Solving this equation we find \[\psi_0(x) = \left( \frac{m \omega}{\pi \hbar} \right)^{1 / 4} e^{-\frac{m \omega}{2 \hbar} x^2}\] and \[E_0 = \frac{1}{2} \hbar \omega.\]

  \item From here we can apply the raising operator $\hat{a}_+$ to find higher energy states \[\psi_n(x) = \frac{1}{\sqrt{n!}} (\hat{a}_+)^n \psi_0 \text{ with } E_n = \left( n + \frac{1}{2} \right) \hbar \omega.\]

  \item In the case of the infinite square well, the stationary states of the harmonic oscillator are orthogonal \[\int_{-\infty}^\infty \psi_m^* \psi_n \,d x = \delta_{m n}.\]
\end{itemize}

\subsubsection{Analytic Method}

\begin{itemize}
  \item By introducing the term \[\xi = \sqrt{\frac{m \omega}{\hbar}} x\] the Schrödinger equation for the harmonic oscillator \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} + \frac{1}{2} m \omega^2 x^2 \psi = E \psi\] can be rewritten \[\frac{d^2 \psi}{d \xi^2} = (\xi^2 - K) \psi\] where \[K = \frac{2 E}{\hbar \omega}\] is the energy of the system in units of $\hbar \omega / 2$.

  \item For large $\xi$ this has the approximate solution \[\psi(\xi) = h(\xi) e^{-\xi^2 / 2}.\] Substituting this into the Schrödinger equation gives \[\frac{d^2 h}{d \xi^2} - 2 \xi \frac{d h}{d \xi} + (K - 1) h = 0.\] If we assume the solution $h(\xi)$ is a power series in $\xi$ we get \[\sum_{j = 0}^\infty [(j + 1) (j + 2) a_{j + 2} - 2 j a_j + (K - 1) a_j] \xi^j = 0.\] By the uniqueness of power series expansions, the coefficient of each power of $\xi$ must be $0$ in order for the series overall to equal $0$ \[(j + 1) (j + 2) a_{j + 2} - 2 j a_j + (K - 1) a_j = 0.\] This gives a recurrence relation \[a_{j + 1} = \frac{2 j + 1 - K}{(j + 1) (j + 2)} a_j\] that can be used to generate the even- and odd-numbered coefficients by starting with $a_0$ and $a_1$, respectively.

  \item However, not all choices of $a_j$ result in normalisable wavefunctions. In order for them to be valid, the power series must terminate — there must be some ``highest'' $j$ (call it $n$) such that $a_{n + 2} = 0$. This will truncate \textbf{either} the even series or the odd series. The other one must be zero from the start — $a_1 = 0$ if $n$ is even, $a_0 = 0$ if $n$ is odd.

  \item In general $h_n(\xi)$ will be a polynomial of degree $n$ in $\xi$, involving even powers only if $n$ is even, and odd powers only if it's odd. Apart from the overall factor $a_0$ or $a_1$ these are the \textbf{Hermite polynomials, $\boldsymbol{H_n(\xi)}$}.

  \item By convention the arbitrary multiplicative factor is chosen such that the coefficient of the highest power of $\xi$ is $2^n$. With this convention, the normalised stationary states for the harmonic oscillator are \[\psi_n = \left( \frac{m \omega}{\pi \hbar} \right)^{1 / 4} \frac{1}{\sqrt{2^n n!}} H_n(\xi) e^{-\xi^2 / 2}.\]
\end{itemize}

\subsection{The Free Particle}

\begin{itemize}
  \item For a free particle, $V = 0$ everywhere. This gives a time-independent Schrödinger equation of \[-\frac{\hbar^2}{2 m} \frac{d^2 \psi}{d x^2} = E \psi\] or \[\frac{d^2 \psi}{d x^2} = -k^2 \psi \text{ where } k = \frac{\sqrt{2 m E}}{\hbar}\] the solution to which is \[\psi(x) = A e^{i k x} + B e^{-i k x}.\]

  \item Introducing the standard time dependence gives \[\Psi(x, t) = A e^{i k \left( x - \frac{\hbar k}{2 m} t \right)} + B e^{-i k \left( x + \frac{\hbar k}{2 m} t \right)}.\] The first term represents a wave travelling to the left and the second term represents a wave travellign to the right. We can combine them \[\Psi_k(x, t) = A e^{i \left( k x - \frac{\hbar k^2}{2 m} t \right)}\] if we let $k$ go negative \[k = \pm \frac{\sqrt{2 m E}}{\hbar} \text{ with } \begin{cases}
            k > 0 & \text{travelling to the right,} \\
            k < 0 & \text{travelling to the left.}
          \end{cases}\]

  \item Unfortunately this wavefunction can't be normalised \[\int_{-\infty}^\infty \Psi_k^* \Psi_k = |A|^2 \int_{-\infty}^\infty \,d x = |A|^2 (\infty).\] This means that it doesn't represent a physically realisable state. In other words, there's no such thing as a free particle with a definite energy.

  \item The solution is still a linear combination of separable solutions, but it's an integral over the continuous variable $k$ instead of a sum over the discrete variable $n$ \[\Psi(x, t) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \phi(k) e^{i \left( k x - \frac{\hbar k^2}{2 m} t \right)} \,d k\] where $(1 / \sqrt{2 \pi}) \phi(k) \,d k$ plays the role of the coefficient $c_n$ in previous wavefunctions. This wavefunction can be normalised but it carries a range of $k$s and thus a range of energies and speeds. This is called a \textbf{wave packet}.

  \item In a typicaly quantum problem you'll be given $\Psi(x, 0)$ and be asked to find $\Psi(x, t)$. How do you determine $\phi(k)$ to match the initial wave function \[\Psi(x, 0) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \phi(k) e^{i k x} \,d k.\] The answer is given by \textbf{Plancherel's theorem} \[f(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty F(k) e^{i k x} \,d k \Longleftrightarrow F(k) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty f(x) e^{-i k x} \,d x\] where $F(k)$ is the \textbf{Fourier transform} of $f(x)$. Thus \[\phi(k) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \Psi(x, 0) e^{-i k x} \,d x.\]

  \item A wave packet is comprised of multiple constituent waves that have different velocities and wavelengths. The velocity of a constituent wave is called the \textbf{phase velocity} while the velocity of the packet as a whole is called the \textbf{group velocity}.

  \item The relationship between $\omega$ and $k$ (in the equation above it's $\omega = \hbar k^2 / 2 m$) is called the \textbf{dispersion relation} and it determines the group and phase velocities \begin{align*}
          v_\text{group} & = \frac{d \omega}{d k} \\
          v_\text{phase} & = \frac{\omega}{k}.
        \end{align*}
\end{itemize}

\subsection{The Delta-Function Potential}

\subsubsection{Bound States and Scattering States}

\begin{itemize}
  \item In classical mechanics, if the potential energy function $V(x)$ rises higher than the system's total energy $E$ then the particle is ``stuck'' in a potential well. It moves back and forth between the \textbf{turning points} but it can't escape. This is called a \textbf{bound state}.

  \item On the other hand, if $E$ exceeds $V(x)$ on one or both sides then the particle comes in from ``infinity'', slows down or speeds up under the influence of the potential, and returns to infinity. This is called a \textbf{scattering state}.

  \item Some potentials admit only bound states (e.g. the harmonic oscillator), some admit only scattering states, and some admit both kinds depending on the energy of the particle.
\end{itemize}

\subsubsection{The Delta-Function Well}

\begin{itemize}
  \item The \textbf{Dirac-Delta function} is defined as \[\delta(x) = \begin{cases}
            0      & x \ne 0 \\
            \infty & x = 0
          \end{cases}\] and has the property \[\int_{-\infty}^\infty \delta(x) \,d x = 1.\] Note that $\delta(x - a)$ is a spike at the point $x = a$ and thus \[f(x) \delta(x - a) = f(a) \delta(x - a).\] This behaviour can be used to ``pick out'' values from under integrals \[\int_{-\infty}^\infty f(x) \delta(x - a) \,d x = f(a) \int_{-\infty}^\infty \delta(x - a) \,d x = f(a).\]

  \item The boundary conditions for $\psi$ are:

        \begin{enumerate}
          \item $\psi$ is always continuous, and

          \item $d \psi / d x$ is continuous except at points where the potential is infinite.
        \end{enumerate}

  \item The potential function $V(x) = -\alpha \delta(x)$ exhibits:

        \begin{itemize}
          \item a bound state with \[\psi(x) = \frac{\sqrt{m \alpha}}{\hbar} e^{-m \alpha x / \hbar^2}\] for \[E = -\frac{m \alpha^2}{2 \hbar^2}\]

          \item a scattering state with \[\psi(x) = \begin{cases}
                    A e^{i k x} + B e^{-i k x} & x < 0 \\
                    F e^{i k x} + G e^{-i k x} & x > 0
                  \end{cases}\] for $E > 0$
        \end{itemize}

  \item In the scattering states \[R = \frac{\beta^2}{1 + \beta^2} \text{ where } \beta = \frac{m \alpha}{\hbar^2 k}\] is called the \textbf{reflection coefficient}. This is the relative probability that a particle coming in from the left will be reflected — if you have a beam of particles it is the fraction of particles that will be reflected.

  \item In the scattering states \[T = \frac{1}{1 + \beta^2}\] is called the \textbf{transmission coefficient}. This is the relative probability taht a particle coming in from the left will be transmitted over the potential well and out to the right — if you have beam of particles it is the fraction of particles that will be transmitted.

  \item Because $k = \sqrt{2 m E} / \hbar$ this means that $R$ and $T$ are functions of $E$ \[R = \frac{1}{1 + (2 \hbar^2 E / m \alpha^2)} \text{ and } T = \frac{1}{1 + (m \alpha^2 / 2 \hbar^2 E)}.\] As $E$ increases more particles are transmitted over the potential well.
\end{itemize}

\subsection{The Finite Square Well}

\begin{itemize}
  \item The potential function for a finite square well is \[V(x) = \begin{cases}
            -V_0 & -a \le x \le a \\
            0    & |x| > a
          \end{cases}\] where $V_0$ is a positive constant.

  \item The bound states can be found by:

        \begin{enumerate}
          \item setting $E < 0$,

          \item solving the Schrödinger equation for the regions $x < -a$, $-a < x < a$, and $x > a$, and

          \item applying the boundary conditions that $\psi$ and $d \psi / d x$ be continuous at $x = \pm a$.
        \end{enumerate}

        This finds that the allowed energies are described by \[\tan z = \sqrt{(z_0 / z)^2 - 1}\] where \begin{align*}
          z   & = l a                                         \\
          z_0 & = \frac{a}{\hbar} \sqrt{2 m V_0} \text{, and} \\
          l   & = \frac{\sqrt{2 m (E + V_0)}}{\hbar}.
        \end{align*}

  \item If $z_0$ is very large (the well is wide and deep), the intersections of $\tan z$ and $\sqrt{(z_0 / z)^2 - 1}$ occur just below $z_n = n \pi / 2$ with $n$ odd so \[E_n + V_0 \approx \frac{n^2 \pi^2 \hbar^2}{2 m (2 a)^2}.\] The left hand side is the ``distance'' between the energy level and the bottom of the well, while the right hand side is the allowed energies for an infinite square well of width $2 a$. This shows that the allowed energies of a finite square well approach those of an infinite square well as $V_0 \rightarrow \infty$ however for any finite $V_0$ there is a finite number of bound states.

  \item As $z_0$ decreases (the well is narrow and shallow) there are fewer intersections of $\tan z$ and $\sqrt{(z_0 / z)^2 - 1}$, i.e. there are fewer bound states, until $z_0 < \pi / 2$ results in a single bound state. There is always one bound state.

  \item The scattering states can be found by:

        \begin{enumerate}
          \item setting $E > 0$,

          \item solving the Schrödinger equation for the regions $x < -a$, $-a < x < a$, and $x > a$, and

          \item applying the boundary conditions that $\psi$ and $d \psi / d x$ be continuous at $x = \pm a$.
        \end{enumerate}

        Assuming that the incident wave comes in from the left this finds \[\psi(x) = \begin{cases}
            A e^{i k x} + B e^{-i k x}  & x < - a    \\
            C \sin (l x) + D \cos (l x) & -a < x < a \\
            F e^{i k x}                 & x > a
          \end{cases}\] where $A$ is the incident amplitude, $B$ is the reflected amplitude, and $F$ is the transmitted amplitude. The boundary conditions can be used to eliminate $C$ and $D$, resulting in \begin{align*}
          B & = i \frac{\sin (2 l a)}{2 k l} (l^2 - k^2) F \text{, and}                         \\
          F & = \frac{e^{-2 i k a} A}{\cos (2 l a) - i \frac{(k^2 + l^2)}{2 k l} \sin (2 l a)}.
        \end{align*} This gives a transmission coefficient of \[T^{-1} = 1 + \frac{V_0^2}{4 E (E + V_0)} \sin^2 \left( \frac{2 a}{\hbar} \sqrt{2 m (E + V_0)} \right).\]

  \item Notice that $T = 1$ whenever the sine is zero. This happens when \[E_n + V_0 = \frac{n^2 \pi^2 \hbar^2}{2 m (2 a)^2},\] i.e. the energy is equal to one of the allowed energies of an infinite square well.
\end{itemize}

\section{Formalism}

\subsection{Hilbert Space}

\begin{itemize}
  \item Quantum theory is based on two constructs: \textbf{wave functions} and \textbf{operators}.

  \item The \textbf{state} of a system is represented by its wave function, \textbf{observables} are represented by operators.

  \item Mathematically, wave functions satisfy the conditions to be called \textbf{vectors} and operators act on them as \textbf{linear transformations}.

  \item The collection of all functions of $x$ is a vector space but it's too large for use in quantum mechanics. For a wave function to represent a physically realisable state it must be normalised \[\int |\Psi|^2 \,d x = 1.\] The set of all \textbf{square-integrable} functions on a specified interval \[f(x) \text{ such that } \int_a^b |f(x)|^2 \,d x < \infty,\] is called \textbf{Hilbert space}. Thus, wave functions live in Hilbert space.

  \item The inner product of two functions is defined as \[\braket{f | g} = \int_a^b f(x)^* g(x) \,d x.\] If $f$ and $g$ are both square-integrable (i.e. they are both in Hilbert space) their inner product is guaranteed to exist.

  \item Note that \[\braket{g | f} = \braket{f | g}^*.\]

  \item The inner product of a function with itself \[\braket{f | f} = \int_a^b |f(x)|^2 \,d x\] is real and non-negative. It is only zero when $f(x) = 0$.

  \item A function is said to be \textbf{normalised} if its inner product with itself is $1$.

  \item Two functions are said to be \textbf{orthogonal} if their inner product is $0$.

  \item A set of functions is said to be \textbf{orthonormal} if they're normalised and mutually orthogonal.

  \item A set of function is said to be \textbf{complete} if any other function in Hilbert space can be expressed as a linear combination of them \[f(x) = \sum_{n = 1}^\infty c_n f_n(x)\] where \[c_n = \braket{f_n | f}.\]
\end{itemize}

\subsection{Observables}

\subsubsection{Hermitian Operators}

\begin{itemize}
  \item The \textbf{Hermitian conjugate} (or \textbf{adjoint}) of a matrix, indicated by a dagger $\vec{T}^\dagger$, is its transpose conjugate. A square matrix is \textbf{hermitian} (or \textbf{self-adjoint}) if it is equal to its hermitian conjugate. Using this notation, the inner product of two vectors can be written as \[\braket{\alpha | \beta} = \vec{a}^\dagger \vec{b}.\]

  \item The expectation value of an observable $Q(x, p)$ can be expressed with inner-product notation \[\ev{Q} = \int \Psi^* \hat{Q} \Psi \,d x = \braket{\Psi | \hat{Q} \Psi}.\]

  \item The outcome of a measurement has to be real, so \[\ev{Q} = \ev{Q}^*\] but the complex conjugate of an inner product reverses the order so \[\ev{\Psi | \hat{Q} \Psi} = \ev{\hat{Q} \Psi | \Psi}.\] Operators that have this property (that they may be applied to either the first or second member of an inner product with the same result) are called \textbf{hermitian}. They're called this because \begin{align*}
          \braket{\Psi | \hat{Q} \Psi} & = \braket{\hat{Q} \Psi | \Psi}      \\
          \Psi^\dagger \hat{Q} \Psi    & = (\hat{Q} \Psi)^\dagger \Psi       \\
                                       & = \Psi^\dagger \hat{Q}^\dagger \Psi
        \end{align*} i.e. $\hat{Q} = \hat{Q}^\dagger$ or $\hat{Q}$ is a hermitian matrix.

  \item Observables are represented by hermitian operators.

  \item In general, matrix multiplication is not commutative, i.e. $\vec{A} \vec{B} \ne \vec{B} \vec{A}$. The diference between the two orderings is called the \textbf{commutator} \[[\vec{A}, \vec{B}] = \vec{A} \vec{B} - \vec{B} \vec{A}\] and is a measure of ``how much'' they fail to commute.
\end{itemize}

\subsubsection{Determinate States}

\begin{itemize}
  \item Normally when you measure an observable $Q$ on an ensemble of identically prepared systems you don't get the same result each time. A state in which every measurement of $Q$ is guaranteed to return the same value $q$ is called a \textbf{determinate state} for $Q$.

  \item In such states, the standard deviation of $Q$ is $0$, i.e. \[\sigma^2 = \Braket{(Q - \braket{Q})^2} = \Braket{\Psi | (\hat{Q} - q)^2 \Psi} = \Braket{(\hat{Q} - q) \Psi | (\hat{Q} - q) \Psi} = 0.\] The only vector whose inner product with itself vanishes is $0$, so \[\hat{Q} \Psi = q \Psi.\] This is the \textbf{eigenvalue equation} for the operator $\hat{Q}$; $\Psi$ is an eigenfunction of $\hat{Q}$ and $q$ is the corresponding \textbf{eigenvalue}. Thus, \textbf{determinate states of $\boldsymbol{Q}$ are eigenfunctions of $\boldsymbol{\hat{Q}}$}.

  \item The collection of all the eigenvalues of an operator is called its \textbf{spectrum}.

  \item If two or more linearly independent eigenfunctions share the same eigenvalue the spectrum is said to be degenerate.
\end{itemize}

\subsection{Eigenfunctions of a Hermitian Operator}

\begin{itemize}
  \item If the spectrum of eigenvalues of a hermitian operator is \textbf{discrete}, i.e. the eigenvalues are separated from one another, then the eigenfunctions lie in Hilbert space and are physically realisable states.

  \item If the spectrum is \textbf{continuous}, i.e. the eigenvalues fill a range, then the eigenfunctions are not normalisable and don't represent possible wave functions (although linear combinations of them may be normalisable).

  \item Some operators have a discrete spectrum only (e.g. the Hamiltonian for the harmonic oscillator), some have a continuous spectrum only (e.g. the Hamiltonian for a free particle), and some have both (e.g. the Hamiltonian for a finite square well).
\end{itemize}

\subsubsection{Discrete Spectra}

\begin{itemize}
  \item The normalisable eigenfunctions of hermitian operators have two important properties:

        \begin{itemize}
          \item their eigenvalues are real, and

          \item eigenfunctions belonging to distinct eigenvalues are orthogonal.
        \end{itemize}

  \item In degenerate states where two or more eigenfunctions share a single eigenvalue, any linear combination of them is also an eigenfunction. The Gram-Schmidt orthogonalisation procedure can be used to construct orthogonal eigenfunctions in each degenerate subspace, so even in the presence of degeneracy the eigenfunctions can be chosen to be orthonormal.

  \item In a finite-dimensional vector space the eigenvectors of a hermitian matrix span the space, but the proof of this doesn't translate to infinite-dimensional spaces. However this property is essential to quantum mechanics so we take it as an axiom:

        \begin{itemize}
          \item The eigenfunctions of an observable operators are complete: Any function (in Hilbert space) can be expressed as a linear combination of them.
        \end{itemize}
\end{itemize}

\subsubsection{Continuous Spectra}

\begin{itemize}
  \item If the eigenvalues of a hermitian operator are continuous its eigenfunctions aren't normalisable, however they may be \textbf{Dirac orthonormalisable}. For example the general solution to the eigenvalue equation for the momentum operator is \[f_p(x) = A e^{i p x / \hbar}\] which isn't normalisable, but if we restrict ourselves to real eigenvalues then \[\int_{-\infty}^\infty f_{p'}^*(x) f_p(x) \,d x = |A|^2 \int_{-\infty}^\infty e^{i (p - p') x / \hbar} \,d x = |A|^2 2 \pi \hbar \delta(p - p').\] Choosing $A = 1 / \sqrt{2 \pi \hbar}$ then \[\braket{f_{p'} | f_p} = \delta(p - p')\] which is reminiscent of true orthonormality.

  \item The eigenfunctions associated with real eigenvalues are also complete, so any square integrable function $f(x)$ can be written in the form \[f(x) = \int_{-\infty}^\infty c(p) f_p(x) \,d p = \frac{1}{\sqrt{2 \pi \hbar}} \int_{-\infty}^\infty c(p) e^{i p x / \hbar} \,d p\] where the coefficients $c(p)$ can be determined by Fourier's trick: \[\braket{f_{p'} | f_p} = \int_{-\infty}^\infty c(p) \braket{f_{p'} | f_p} \,d p = \int_{-\infty}^\infty c(p) \delta(p - p') \,d p' = c(p').\]

  \item In summary: if an the spectrum of a hermitian operator is continuous, the eigenfunctions are not normalisable, they are not in Hilbert space, and they do not represent physically possibly states. However they are Dirac orthonormalisable and complete.
\end{itemize}

\subsection{Generalised Statistical Interpretation}

\begin{itemize}
  \item The \textbf{generalised statistical interpretation} states: If you measure an observable $Q(x, p)$ on a particle in the state $\Psi(x, t)$, you are certain to get one of the eigenvalues of the hermitian operator $\hat{Q}(x, -i \hbar d / d x)$. If the spectrum of $\hat{Q}$ is discrete, the probability of getting the particular eigenvalue $q_n$ associated with the (orthonormalised) eigenfunction $f_n(x)$ is \[|c_n|^2 \text{ where } c_n = \braket{f_n | \Psi}.\] If the spectrum is continuous, with real eigenvalues $q(z)$ and associated (Dirac-orthonormalised) eigenfunctions $f_z(x)$, the probability of getting a result in the range $d z$ is \[|c(z)|^2 \,d z \text{ where } c(z) = \braket{f_z | \Psi}.\] Upon measurement the wave function ``collapses'' to the corresponding eigenstate.

  \item Because the eigenfunctions of an observable operator are complete the wavefunction can be written as a linear combination of them \[\Psi(x, t) = \sum_n c_n(t) f_n(x)\] and because they're orthonormal the coefficients are given by Fourier's trick \[c_n(t) = \braket{f_n | \Psi} = \int f_n(x)^* \Psi(x, t) \,d x.\]

  \item The Dirac-orthonormalised eigenfunctions of the momentum operator are \[f_p(x) = \frac{1}{\sqrt{2 \pi \hbar}} e^{i p x / \hbar}\] so \[c(p) = \braket{f_p | \Psi} = \frac{1}{\sqrt{2 \pi \hbar}} \int_{-\infty}^\infty e^{i p x / \hbar} \Psi(x, t) \,d x.\] This is such an important quantity that it is called the \textbf{momentum space wave function $\boldsymbol{\Phi(x, t)}$}. It is essentially the Fourier transform of the position space wave function and, according to the generalised statistical interpretation, the probability that a measurement of momentum would yield a result in the range $d p$ is given by \[|\Phi(p, t)|^2 \,d p.\]
\end{itemize}

\subsection{The Uncertainty Principle}

\subsubsection{Proof of the Generalised Uncertainty Principle}

\begin{itemize}
  \item For two observables $A$ and $B$ and associated operators $\hat{A}$ and $\hat{B}$, the \textbf{generalised uncertainty principle} states \[\sigma_A^2 \sigma_B^2 \ge \left( \frac{1}{2 i} \braket{[\hat{A}, \hat{B}]} \right)^2\] where $[\hat{A}, \hat{B}]$ is the commutator of the two operators.

  \item The implication of this is that there's an ``uncertainty principle'' for each pair of observables whose operators don't commute. These are called \textbf{incompatible observables} and they don't have a complete set of common eigenfunctions. On the other hand, \textbf{compatible observables} do have a complete set of common eigenfunctions and taking one measurement gives you a single eigenfunction from which you can determine all the compatible observables without needing to take another measurement.
\end{itemize}

\subsubsection{The Minimum-Uncertainty Wave Packet}

\begin{itemize}
  \item The minimum uncertainty wave packet where $\sigma_x \sigma_p = \hbar / 2$ has the form \[\Psi(x, t) = A e^{-a (x - \braket{x})^2 / 2 \hbar} e^{i \braket{p} x / \hbar}\] i.e. it is Gaussian.
\end{itemize}

\subsubsection{The Energy-Time Uncertainty Principle}

\begin{itemize}
  \item The \textbf{energy-time uncertainty principle} is \[\Delta t \Delta E \ge \frac{\hbar}{2}\] where $\Delta t$ is ``the time it takes the system to change substantially'' and $\Delta E$ is ``the uncertainty in $E$.''

  \item The \textbf{Generalised Ehrenfest theorem} is \[\frac{d}{d t} \braket{Q} = \frac{i}{\hbar} \Braket{\left[ \hat{H}, \hat{Q} \right]} + \Braket{\frac{\partial \hat{Q}}{\partial t}},\] i.e. the time rate change in the expected value of an observable is determined by the commutator of its operator with the Hamiltonian and the derivative of its operator with respect to time. In the typical case where its the operator doesn't explicitly depdend on time, if $\hat{Q}$ commutes with $\hat{H}$ then $\braket{Q}$ is constant and $Q$ is a conserved quantity.

  \item Substituting the above into the generalised undertainty principle we get \[\sigma_H \sigma_Q \ge \frac{\hbar}{2} \left| \frac{d \braket{Q}}{d t} \right|.\] If we define $\Delta E = \sigma_H$ and \[\Delta t = \frac{\sigma_Q}{|d \braket{Q} / d t|}\] then \[\Delta E \Delta t \ge \frac{\hbar}{2}.\] Note that $\Delta t$ is the amount time it takes for the expectation value of $Q$ to change by one standard deviation.

  \item If $\Delta E$ is small then $\Delta t$ must be large — if we know the energy the rate of change of all observables must be gradual. On the other hand, if $\Delta t$ is small we don't know as much about the energy.
\end{itemize}

\end{document}