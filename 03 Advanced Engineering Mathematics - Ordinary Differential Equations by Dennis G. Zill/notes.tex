\documentclass{article}
\usepackage{amsmath} % For align*
\usepackage{enumitem} % For customisable list labels
\usepackage{graphicx} % For images
\usepackage{siunitx} % For units
\graphicspath{{./images/}}

\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\erfc}{erfc}

\title{Advanced Engineering Mathematics Ordinary Differential Equations by Dennis G. Zill Notes}
\author{Chris Doble}
\date{February 2023}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction to Differential Equations}

\subsection{Definitions and Terminology}

\begin{itemize}
  \item An equation containing the derivatives of one or more dependent variables, with respect to one or more independent variables, is said to be a \textbf{differential equation} (DE)

  \item An \textbf{ordinary DE} (ODE) is a DE that contains only ordinary (i.e. non-partial) derivatives of one or more functions with respect to a single independent variable

  \item A \textbf{partial DE} is a DE that contains only partial derivatives of one or more functions of two or more independent variables

  \item The \textbf{order} of a DE is the order of the highest derivative in the equation

  \item First order ODEs are sometimes written in the \textbf{differential form} \[M(x, y) \,dx + N(x, y) \,dy = 0\]

  \item $n$-th order ODEs in one dependent variable can be expressed by the \textbf{general form} \[F(x, y, y', \ldots, y^{(n)}) = 0\]

  \item It's possible to solve ODEs in the general form uniquely for the highest derivative $y^{(n)}$ in terms of the other $n + 1$ variables, allowing them to be expressed in the \textbf{normal form} \[\frac{d^n y}{d x^n} = f(x, y, y', \ldots, y^{(n - 1)})\]

  \item An $n$-th order ODE is said be \textbf{linear} in the variable $y$ if it can be expressed in the form \[a_n(x) y^{(n)} + a_{n-1}(x) y^{(n - 1)} + \cdots + a_1(x) y' + a_0(x) y - g(x) = 0\] i.e. the dependent variable $y$ and all of its derivatives aren't raised to a power or used in nonlinear functions like $e^y$ or $\sin y$, and the coefficients $a_0$, $a_1$, $\ldots$, $a_n$ depend at most on the independent variable $x$

  \item A \textbf{nonlinear} ODE is one that is not linear

  \item A \textbf{solution} to an ODE is a function $\phi$, defined on an interval $I$ and possessing at least $n$ derivatives that are continuous on $I$, such that \[F(x, \phi(x), \phi'(x), \ldots, \phi^{n}(x)) = 0 \text{ for all } x \text{ in } I.\]

  \item The \textbf{interval of definition}, \textbf{interval of validity}, or the \textbf{domain} of a solution is the interval over which the solution is valid

  \item A solution of a DE that is $0$ on an interval $I$ is said to be a \textbf{trivial solution}

  \item Because solutions to DEs must be differentiable over their interval of validity, discontinuities, etc. must be excluded from the interval

  \item An \textbf{explicit solution} to an ODE is one where the dependent variable is expressed solely in terms of the independent variable and constants

  \item An \textbf{implicit solution} to an ODE is a relation $G(x, y) = 0$ over an interval $I$ provided there exists at least one function $\phi$ that satisfies the relation as well as the ODE on $I$

  \item When solving a first-order ODE we usually obtain a solution containing a single arbitrary constant or parameter $c$. A solution containing an arbitrary constant represents a set of solution called a \textbf{one-parameter family of solutions}

  \item When solving an $n$-th order DE we usually obtain an \textbf{$n$-parameter family of solutions}

  \item A solution of a DE that is free from arbitrary parameters is called a \textbf{particular solution}

  \item A \textbf{singular solution} is a solution to a DE that isn't a member of a family of solutions

  \item A \textbf{system of ODEs} is two or more equations involving the derivatives of two or more unknown functions of a single independent variable. A solution of such a system is a differentiable function for each equation defined on a common interval $I$ that satisfy each equation of the system on that interval
\end{itemize}

\subsection{Initial Value Problems}

\begin{itemize}
  \item An \textbf{initial value problem} is the problem of solving a DE with some given \textbf{initial conditions}, e.g. solve \[\frac{d^n y}{dx^n} = f(x, y, y', \ldots, y^{(n - 1)})\] subject to \[y(x_0) = y_0, \,y'(x_0) = y_1, \,\ldots, \,y^{(n - 1)}(x_0) = y_{n - 1}\]

  \item The domain of $y = f(x)$ differs depending on how it's considered:

        \begin{itemize}
          \item As a function its domain is all real numbers for which it's defined

          \item As a solution of a DE its domain is a single interval over which it's defined an differentiable

          \item As a solution of an initial value problem its domain is a single interval over which it's defined, differentiable, and contains the initial conditions
        \end{itemize}

  \item An initial value problem may not have any solutions. If it does it may have multiple.

  \item First-order initial value problems of the form \[\frac{dy}{dx} = f(x, y)\] \[y(x_0) = y_0\] are guaranteed to have a unique solution over an interval $I$ containing $x_0$ if $f(x, y)$ and $\partial f / \partial y$ are continuous
\end{itemize}

\subsection{Differential Equations as Mathematical Models}

\begin{itemize}
  \item A \textbf{mathematical model} is a mathematical description of a system or phenomenon

  \item The \textbf{level of resolution} of a model determines how many variables are included in the model

  \item A simple model of the growth of a population $P$ is \[\frac{dP}{dt} = k P\] where $k > 0$

  \item A simple model of radioactive decay of an amount of substance $A$ is \[\frac{dA}{dt} = k A\] where $k < 0$

  \item Newton's empirical law of cooling/warming states that the rate of change of the temperature of a body is proportional to the difference between the temperature of the body and the temperature of the surrounding medium \[\frac{dT}{dt} = k (T - T_m)\]
\end{itemize}

\section{First-Order Differential Equations}

\subsection{Solution Curves Without a Solution}

\begin{itemize}
  \item An ODE in which the independent variable doesn't appear is said to be \textbf{autonomous}, e.g. \[\frac{d y}{d x} = f(y)\]

  \item A real number $c$ is a \textbf{critical/equilibrium/stationary point} of an autonomous DE if it is a zero of $f$

  \item If $c$ is a critial point of an autonomous DE, then $y(x) = c$ is a solution

  \item A solution of the form $y(x) = c$ is called an \textbf{equilibrium solution}

  \item We can draw several conclusions about the solutions of an autonomous DE with $n$ critical points and $n + 1$ subregions bounded by the critical points:

        \begin{itemize}
          \item If $(x_0, y_0)$ is in a subregion, it remains in that subregion for all $x$

          \item By continuity, $f(y) < 0$ or $f(y) > 0$ for all $y$ in a subregion and thus $y(x)$ can't have maximum/minimum points or oscillate

          \item If $y(x)$ is bounded above by a critical point $c_1$, it must approach $y(x) = c_1$ as $x \rightarrow -\infty$ or $x \rightarrow \infty$

          \item If $y(x)$ is bounded above and below by critical points $c_1$ and $c_2$, it must approach $y(x) = c_1$ as $x \rightarrow -\infty$ and $y(x) = c_2$ as $x \rightarrow \infty$ or vice versa

          \item If $y(x)$ is bounded below by a critical point $c_1$, it must approach $y(x) = c_1$ as $x \rightarrow -\infty$ or $x \rightarrow \infty$
        \end{itemize}
\end{itemize}

\includegraphics*{attractors-and-repellers.png}

\begin{itemize}
  \item If $y(x)$ is a solution of an autonomous differential equation $d y / d x = f(y)$, then $y_1(x) = y (x - k)$, where $k$ is a constant, is also a solution
\end{itemize}

\subsection{Separable Equations}

\begin{itemize}
  \item A first-order ODE of the form \[\frac{d y}{d x} = g(x) h(y)\] is said to be \textbf{separable} or to have \textbf{separate variables}

  \item A separable first-order ODE can be solved by dividing both sides by $h(y)$ then integrating both sides with respect to $x$

        \begin{align*}
          \frac{d y}{d x}                          & = g(x) h(y)      \\
          \frac{1}{h(y)} \frac{d y}{d x}           & = g(x)           \\
          \int \frac{1}{h(y)} \frac{d y}{d x} \,dx & = \int g(x) \,dx \\
          \int \frac{1}{h(y)} \,dy                 & = \int g(x) \,dx \\
          H(y)                                     & = G(x) + c
        \end{align*}

  \item Care should be taken when dividing by $h(y)$ as it removes constant solutions $y = r$ where $h(r) = 0$
\end{itemize}

\subsection{Linear Equations}

\begin{itemize}
  \item A first-order DE of the form \[a_1(x) \frac{d y}{d x} + a_0(x) y = g(x)\] or in standard form \[\frac{d y}{d x} + P(x) y = f(x)\] is said to be a \textbf{linear equation} in the dependent variable $y$

  \item When $g(x) = 0$ or $f(x) = 0$ the linear equation is said to be \textbf{homogeneous} and is solvable via separation of variables, otherwise it is \textbf{nonhomogeneous}

  \item The nonhomogeneous linear equation's solution is the sum of two solutions $y = y_c + y_p$ where $y_c$ is a solution of the associated homogeneous equation \[\frac{d y}{d x} + P(x) y = 0\] and $y_p$ is a particular solution of the nonhomogeneous equation

  \item Nonhomogeneous linear equations can be solved via \textbf{variation of parameters}:

        \begin{enumerate}
          \item Put it into standard form

          \item Determine the \textbf{integrating factor} $e^{\int P(x) \,d x}$

          \item Multiply by the integrating factor

          \item Recognise that the left hand side of the equation is the derivative of the product of the integrating factor and $y$

          \item Integrate both sides of the equation

          \item Solve for $y$
        \end{enumerate}

  \item The \textbf{general solution} of a DE is a family of solutions that contains all possible solutions (except singular solutions)

  \item A term $y = f(x)$ in a solution is called a \textbf{transient term} if $f(x) \rightarrow 0$ as $x \rightarrow \infty$

  \item When either $P(x)$ or $f(x)$ is a piecewise-defined function the equation is then referred to as a \textbf{piecewise-linear differential equation} that can be solved by solving each interval in isolation then choosing appropriate constants to ensure the overall solution is continuous

  \item The \textbf{error function} and \textbf{complementary error function} are defined

        \begin{align*}
          \erf x + \erfc x                                                                                                              & = 1 \\
          \left( \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \,d t \right) + \left( \frac{2}{\sqrt{\pi}} \int_x^\infty e^{-t^2} \,dt \right) & = 1
        \end{align*}
\end{itemize}

\subsection{Exact Equations}

\begin{itemize}
  \item The \textbf{differential} of a function $z = f(x, y)$ is \[dz = \frac{\partial f}{\partial x} \,d x + \frac{\partial f}{\partial y} \, dy\]

  \item A differential expression $M(x, y) \,d x + N(x, y) \,d y$ is an \textbf{exact differential} in the region $R$ of the $xy$-plane if it corresponds to the differential of some function $f(x, y)$

  \item A first-order DE of the form \[M(x, y) \,d x + N(x, y) \,d y = 0\] is said to be an \textbf{exact equation} if the expression on the left side is an exact differential

  \item A necessary and sufficient condition that $M(x, y) \,d x + N(x, y) \,d y$ be an exact differential is \[\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}\]

  \item Exact differentials can be solved by

        \begin{enumerate}
          \item Integrating $M(x, y)$ with respect to $x$ to find an expression for $f(x, y)$

                \begin{align*}
                  \frac{\partial f}{\partial x} & = M(x, y)                   \\
                  f(x, y)                       & = \int M(x, y) \,d x + g(y)
                \end{align*}

          \item Differentiating $f(x, y)$ with respect to $y$ and equating it to $N(x, y)$ to find $g'(y)$

                \begin{align*}
                  \frac{\partial f}{\partial y} = N(x, y) & = \frac{\partial}{\partial y} \int M(x, y) \,d x + g'(y)   \\
                  g'(y)                                   & = N(x, y) - \frac{\partial}{\partial y} \int M(x, y) \,d x
                \end{align*}

          \item Integrating $g'(y)$ with respect to $y$ to find $g(y)$ and substituting it into $f(x, y)$

          \item Equating $f(x, y)$ with an unknown constant $c$
        \end{enumerate}

  \item $x$ and $y$ can be swapped in the steps above (i.e. you can start by integrating $N(x, y)$ with respect to $y$, etc.)

  \item A nonexact DE $M(x, y) \,d x + N(x, y) \,d y = 0$ can sometimes be transformed into an exact DE by finding an appropriate integrating factor

        \begin{itemize}
          \item If $(M_y - N_x) / N$ is a function of $x$ alone, then an integrating factor is \[\mu(x) = e^{\int \frac{M_y - N_x}{N} \,d x}\]

          \item If $(N_x - M_y) / M$ is a function of $y$ alone, then an integrating factor is \[\mu(y) = e^{\int \frac{N_x - M_y}{M} \,d y}\]
        \end{itemize}
\end{itemize}

\subsection{Solutions by Substitution}

\begin{itemize}
  \item A function $f(x, y)$ is said to be a \textbf{homogeneous function} of degree $\alpha$ if \[f(t x, t y) = t^\alpha f(x, y)\]

  \item A first-order DE of the form \[M(x, y) \,d x + N(x, y) \,d y = 0\] is said to be \textbf{homogeneous} if both $M$ and $N$ are homogeneous functions of the same degree

  \item To solve a homogeneous first-order DE:

        \begin{enumerate}
          \item Rewrite it as \[M(x, y) = x^\alpha M(1, u) \text{ and } N(x, y) = x^\alpha N(1, u) \text{ where } u = y / x\] or \[M(x, y) = y^\alpha M(v, 1) \text{ and } N(x, y) = y^\alpha N(v, 1) \text{ where } v = x / y\]

          \item Substitute $y = u x$ and $d y = u \,d x + x \,d u$ or $x = v y$ and $d x = v \,d y + y \,d v$ as appropriate

          \item Solve the resulting first-order separable DE

          \item Substitude $u = y / x$ or $v = x / y$ as appropriate
        \end{enumerate}

  \item The DE \[\frac{d y}{d x} + P(x) y = f(x) y^n\] where $n$ is any real number is called \textbf{Bernoulli's equation}

  \item For $n = 0$ and $n = 1$ Bernoulli's equation is linear

  \item To solve Bernoulli's equation for $n \ne 0$ and $n \ne 1$:

        \begin{enumerate}
          \item Substitude $y = u^{1 / (1 - n)}$ and $\frac{d y}{d x} = \frac{d}{dx} (u^{1 / (1 - n)})$

          \item Solve the resulting linear equation

          \item Substitude $u = y^{n - 1}$
        \end{enumerate}

  \item A DE of the form \[\frac{d y}{d x} = f(A x + B y + C)\] can always be reduced to an equation with separable variables by means of the substitution \[u = A x + B y + C, B \ne 0\]
\end{itemize}

\subsection{A Numerical Method}

\begin{itemize}
  \item Approximate values for points on a solution curve near an initial point can be calculated via a \textbf{linearization} of the solution curve — a straight line that has the same slope as the initial point and passes through it

  \item \textbf{Euler's method} approximates a solution curve by iteratively stepping along its linearizations \[y_{n + 1} = y_n + h f(x_n, y_n)\] where $h$ is the \textbf{step size}
\end{itemize}

\setcounter{subsection}{8}
\subsection{Modeling with Systems of First-Order DEs}

\begin{itemize}
  \item In a system of DEs \[\frac{dx}{dt} = g_1(t, x, y)\] and \[\frac{dy}{dt} = g_2(t, x, y)\] if $g_1$ and $g_2$ are linear in $x$ and $y$, i.e. \[g_1(t, x, y) = c_1 x + c_2 y + f_1(t)\] and \[g_2(t, x, y) = c_3 x + c_4 y + f_2(t)\] it is said to be a \textbf{linear system}
\end{itemize}

\section{Higher-Order Differential Equations}

\subsection{Theory of Linear Equations}

\begin{itemize}
  \item An \textbf{$n$th-order initial-value problem (IVP)} is to solve \[a_n(x) \frac{d^ny}{dx^n} + a_{n - 1}(x) \frac{d^{n - 1}y}{dx_{n - 1}} + \cdots + a_1(x) \frac{dy}{dx} + a_0(x) y = g(x)\] subject to \[y(x_0) = y_0, \,y'(x_0) = y_1, \,\ldots, \,y^{(n - 1)}(x_0) = y_{n - 1}\]

  \item If $a_n(x)$, $a_{n - 1}(x)$, $\ldots$, $a_1(x)$, $a_0(x)$, and $g(x)$ are continuous on an interval $I$ and $a_n(x) \ne 0$ for every $x$ in the interval, then then a unique solution exists for the above IVP for every $x = x_0$ within the interval

  \item An \textbf{initial value problem} is when all of the constraints are located at the same point while a \textbf{boundary value problem} is when they're at different points

  \item Boundary value problems may have many, one, or no solutions

  \item When $g(x) = 0$ the DE is said to be \textbf{homogeneous}, otherwise it's \textbf{nonhomogeneous}

  \item The symbol $D$ is called a \textbf{differential operator} because it transforms a differentiable function into another function \[Dy = \frac{dy}{dx}\]

  \item Higher-order derivatives can be expressed as \[D^n = \frac{d^ny}{dx^n}\]

  \item An \textbf{$n$th-order differential operator} is defined to be \[L = a_n(x) D^n + a_{n - 1}(x) D^{n - 1} + \cdots + a_1(x) D + a_0(x)\]

  \item As a consequence of the properties of differentiation \[D(cf(x)) = cDf(x)\] and \[D\{f(x) + g(x)\} = Df(x) + Dg(x)\]

  \item The superposition principle for homogeneous linear $n$th-order differential equation states that if $y_1$, $y_2$, $\ldots$, $y_k$ are solutions of the equation on an interval $I$ then the linear combination \[y = c_1 y_1(x) + c_2 y_2(x) + \cdots + c_k y_k(x)\] where $c_i$ are arbitrary constants is also a solution on the interval

  \item A set of functions $f_1(x)$, $f_2(x)$, $\ldots$, $f_n(x)$ is said to be \textbf{linearly dependent} on an interval $I$ if there exists constants $c_1$, $c_2$, $\ldots$, $c_n$, not all zero, such that \[c_1 f_1(x) + c_2 f_2(x) + \cdots + c_n f_n(x) = 0\] for every $x$ in the interval. Otherwise it is said to tbe \textbf{linearly independent}

  \item The \textbf{Wronskian} of a set of $n$ functions that are $n - 1$ times differentiable is defined as \[W(f_1, f_2, \ldots, f_n) = \begin{vmatrix}
            f_1           & f_2           & \cdots & f_n           \\
            f_1'          & f_2'          & \cdots & f_n'          \\
            \vdots        & \vdots        &        & \vdots        \\
            f_1^{(n - 1)} & f_2^{(n - 1)} & \cdots & f_n^{(n - 1)}
          \end{vmatrix}\]

  \item If $y_1$, $y_2$, $\ldots$, $y_n$ are $n$ solutions to a homogeneous linear $n$th-order differential equation on an interval $I$ then the set of solutions is \textbf{linearly independent} on $I$ iff $W(y_1, y_2, \ldots, y_n) \ne 0$ for every $x$ in the interval

  \item Any set of $n$ linearly independent solutions of a homogeneous linear $n$th-order differential equation on an interval $I$ is said to be a \textbf{fundamental set of solutions} on the interval

  \item If $y_1$, $y_2$, $\ldots$, $y_n$ are a fundamental set of solutions of a homogeneous linear $n$th-order DE on an interval $I$ then the \textbf{general solution} of the equation on the interval is \[y = c_1 y_1(x) + c_2 y_2(x) + \cdots + c_n y_n(x)\] where $c_i$ are arbitrary constants

  \item Another way of saying the above is that any solution on the interval can be expressed as a linear combination of the fundamental set of solutions

  \item A linear combination of a fundamental set of solutions of a homogenous linear $n$th-order DE \[y_c(x) = c_1 y_1(x) + c_2 y_2(x) + \cdots + c_n y_n(x)\] is called the \textbf{complementary function} of associated nonhomogenous DEs

  \item If $y_p$ is any particular solution to a nonhomogeneous linear $n$th-order DE on an interval $I$ and $y_1$, $y_2$, $\ldots$, $y_n$ are a fundamental set of solutions of the associated homogeneous DE on $I$, then the \textbf{general solution} of the equation on the interval is \[y = c_1 y_1(x) + c_2 y_2(x) + \cdots + c_n y_n(x) + y_p(x)\] where $c_i$ are arbitrary constants

  \item Another way of saying the above is that any solution on the interval can be expressed as $y = y_c + y_p$

  \item The superposition for nonhomogeneous linear $n$th-order differential equations states that if $y_{p_1}$, $y_{p_2}$, $\ldots$, $y_{p_k}$ are $k$ particular solutions of a nonhomogeneous lienar $n$th-order differential equation on an interval $I$ corresponding, in turn, to $k$ distinct functions $g_1$, $g_2$, $\ldots$, $g_k$, then \[y_p(x) = y_{p_1}(x) + y_{p_2}(x) + \cdots + y_{p_k}(x)\] is a particular solution of \[a_n(x) y^{(n)} + a_{n - 1}(x) y^{(n - 1)} + \cdots + a_0(x) y = g_1(x) + g_2(x) + \cdots + g_k(x)\]
\end{itemize}

\subsection{Reduction of Order}

\begin{itemize}
  \item The \textbf{reduction of order} method requires knowledge of one non-trivial solution and comprises the following steps:

        \begin{enumerate}
          \item Recognise that the ratio of two linearly independent functions isn't constant, i.e. \[u(x) = \frac{y_1(x)}{y_2(x)} \text{ or } y_2(x) = u(x) y_1(x)\]

          \item Substitute $y_2(x) = u(x) y_1(x)$ into the DE — this will result in a DE involving only $u''$ and $u'$ which can be treated as a linear first-order DE in $u' = w$

          \item Solve for $w$

          \item Substitute $w = u'$

          \item Integrate to find $u$

          \item Multiply by $y_1$ to find $y_2$
        \end{enumerate}

  \item A formula for the above on a DE in standard form \[y'' + P(x) y' + Q(x) y = 0\] is \[y_2 = y_1(x) \int \frac{e^{-\int P(x) \,dx}}{y_1^2(x)} \,dx\]
\end{itemize}

\subsection{Homogeneous Linear Equations with Constant Coefficients}

\begin{itemize}
  \item All solutions to homogenous linear DEs \[a_n y^{(n)} + a_{n - 1} y^{(n - 1)} + \cdots + a_1 y' + a_0 y = 0\] where $a_i$ are real constants and $a_n \ne 0$ are either exponential functions or constructed from exponential functions

  \item Substituting a solution $y = e^{mx}$ we find \[e^{mx} (a_n m^n + a_{n - 1} m^{n - 1} + \cdots + a_1 m + a_0) = 0\] where the term in brackets is called the \textbf{auxiliary equation} of the DE

  \item Thus, the solution $y = e^{mx}$ is valid if $m$ is a root of the auxiliary equation

  \item Real roots correspond to solutions of the form \[y = c e^{mx}\]

  \item Complex roots $\alpha \pm i \beta$ correspond to solutions of the form \[y_1 = c_1 e^{\alpha x} \cos \beta x \text{ and } y_2 = c_2 e^{\alpha x} \sin \beta x\]

  \item A root $m$ of multiplicity $k$ corresponds to the solutions \[e^{m x}, \: x e^{mx}, \: x^2 e^{mx}, \: \ldots, \: x^{k - 1} e^{mx}\]
\end{itemize}

\subsection{Undetermined Coefficients}

\begin{itemize}
  \item The \textbf{method of undetermined coefficients} may be used to find a particular solution to nonhomogenous linear differential equations where the input function is comprised of constants, polynomials, exponentials $e^{\alpha x}$, sines, and cosines

  \item To apply the method you:

        \begin{enumerate}
          \item Solve the associated homogeneous equation

          \item Assume the particular solution has the same form as the input function

          \item If a term in the proposed solution is present in the complementary function, multiply it by $x^n$ where $n$ is the smallest positive integer that removes the duplication

          \item Substitute the proposed solution into the DE

          \item Solve for the unknown constants
        \end{enumerate}
\end{itemize}

\includegraphics*[scale=0.44]{trial-particular-solutions.png}

\subsection{Variation of Parameters}

\begin{itemize}
  \item The \textbf{method of variation of parameters} can be used to find a particular solution of a nonhomogeneous linear $n$th-order DE

  \item To apply the method you:

        \begin{enumerate}
          \item Solve the homogeneous equation to find the complementary function

          \item Assume the solution has the form \[y_p = u_1(x) y_1(x) + \cdots + u_n(x) y_n(x)\] where $n$ is the order of the equation and $y_i$ are the fundamental set of solutions from the complementary equation

          \item Convert to standard form by dividing by the leading coefficient \[y^{(n)} + a_{n - 1}(x) y^{(n - 1)} + \cdots + a_1(x) y' + a_0(x) y = f(x)\]

          \item Solve the system of linear equations

                \begin{align*}
                  y_1 u_1' + \cdots + y_n u_n'                     & = 0    \\
                  y_1' u_1' + \cdots + y_n' u_n'                   & = 0    \\
                  \vdots \qquad \qquad                                      \\
                  y_1^{(n - 1)} u_1' + \cdots + y_n^{(n - 1)} y_n' & = 0    \\
                  y_1^{(n)} u_1' + \cdots + y_n^{(n)} u_n'         & = f(x)
                \end{align*}

                via Cramer's method:

                \begin{enumerate}
                  \item Compute the Wronskian of $y_i$ \[W = \begin{vmatrix}
                            y_1       & \cdots & y_n       \\
                            y_1'      & \cdots & y_n'      \\
                            \vdots    & \ddots & \vdots    \\
                            y_1^{(n)} & \cdots & y_n^{(n)}
                          \end{vmatrix}\]

                  \item Compute $u_i'$ for $i = 1, \: \ldots, \: n$ where \[u_i' = \frac{W_i}{W}\] and $W_i$ is the determinant of the matrix formed by replacing the $i$th column of the Wronskian matrix with the column vector \[\begin{bmatrix}
                            0      \\
                            \vdots \\
                            0      \\
                            f(x)
                          \end{bmatrix}\]
                \end{enumerate}

          \item Integrate each $u_i'$ to find $u_i$
        \end{enumerate}
\end{itemize}

\subsection{Cauchy-Euler Equations}

\begin{itemize}
  \item A \textbf{Cauchy-Euler equation} is a linear differential equation of the form \[a_n x^n \frac{d^n y}{dx^n} + a_{n - 1} x^{n - 1} \frac{d^{n - 1} y}{dx^{n - 1}} + \cdots + a_1 x \frac{dy}{dx} + a_0 y = g(x)\]

  \item To solve a homogeneous Cauchy-Euler equation you:

        \begin{enumerate}
          \item Assume the equation has a solution of the form $y = x^m$, giving

                \begin{align*}
                  a_n x^n \frac{d^n y}{dx^n} & = a_n x^n m (m - 1) (m - 2) \cdots (m - n + 1) x^{m - n} \\
                                             & = a_n m (m - 1) (m - 2) \cdots (m - n + 1) x^m
                \end{align*}

                and the equation then becomes \[f(m) x^m = 0\] where $f(m)$ is a polynomial in $m$ known as the auxiliary or characteristic equation, the roots of which form the general solution

          \item Solve the auxiliary equation where

                \begin{itemize}
                  \item A real root $m$ corresponds to a solution \[y = c x^m\]

                  \item Complex roots $\alpha \pm i \beta$ correspond to solutions \[x^\alpha (c_1 \cos (\beta \ln x) + c_2 \sin (\beta \ln x))\]

                  \item A root $m$ of multiplicity $k$ corresponds to solutions \[x^m, \: x^m \ln x, \: x^m (\ln x)^2\, \:\ldots, \: x^m (\ln x)^{k - 1}\]
                \end{itemize}
        \end{enumerate}

  \item To solve a nonhomogeneous Cauchy-Euler euqation you:

        \begin{enumerate}
          \item Solve the associated homogeneous equation

          \item Find a particular solution via variation of parameters
        \end{enumerate}
\end{itemize}

\subsection{Nonlinear Equations}

\begin{itemize}
  \item The superposition principle does not hold for nonlinear equations

  \item Nonlinear second order DEs of the form $F(x, y', y'') = 0$ where $y$ is missing can sometimes be solved by:

        \begin{enumerate}
          \item Substitute $u = y'$ (and thus $u' = y''$)

          \item Solve the resulting DE for $u$

          \item Integrate to find $y$
        \end{enumerate}

  \item Nonlinear second order DEs of the form $F(y, y', y'') = 0$ where $x$ is missing can sometimes by solved by:

        \begin{enumerate}
          \item Substitute $u = y'$ and \[y'' = \frac{du}{dy} \frac{dy}{dx} = u \frac{du}{dy}\]

          \item Solve the resulting DE for $u$

          \item Integrate to find $y$
        \end{enumerate}

  \item Nonlinear initial-value problems can sometimes be solved by substituting the initial conditions into a Taylor series centred at $x_0$. The initial conditions can also be substituted into subsequent derivatives to add further terms to the series
\end{itemize}

\setcounter{subsection}{9}
\subsection{Green's Functions}

\begin{itemize}
  \item Green's functions are useful because they allow you to express the solution of a DE in terms of the input function $g(x)$, making it easy to see how different input functions change the solution
\end{itemize}

\subsubsection{Initial-Value Problems}

\begin{itemize}
  \item The solution of a second-order IVP \[y'' + P(x) y' + Q(x) y = f(x), \: y(x_0) = y_0, \: y'(x_0) = y_1\] can be expressed as \[y = y_h + y_p\] where $y_h$ is the solution to the associated homogeneous equation with nonhomogeneous initial conditions \[y'' + P(x) y' + Q(x) y' = 0, \: y(x_0) = y_0, \: y'(x_0) = y_1\] and $y_p$ is the solution to the nonhomogeneous equation with homogeneous initial conditions \[y'' + P(x) y' + y = f(x), \: y(x_0) = 0, \: y'(x_0) = 0\]

  \item If $P(x)$ and $Q(x)$ are constant $y_h$ can be found via the auxiliary / characteristic equation

  \item If $y_1$ and $y_2$ form a fundamental set of solutions to the associated homogeneous equation, then $y_p$ is given by \[y_p(x) = \int_{x_0}^x G(x, t) f(t) \,dt\] where $G(x, y)$ is the Green's function for the differential equation \[G(x, t) = \frac{y_1(t) y_2(x) - y_1(x) y_2(t)}{W(t)}\] and $W(t)$ is the Wronskian \[W(t) = \begin{vmatrix}
            y_1(t)  & y_2(t)  \\
            y_1'(t) & y_2'(t)
          \end{vmatrix}\]
\end{itemize}

\subsubsection{Boundary Value Problems}

\begin{itemize}
  \item If $y_1$ and $y_2$ are linearly independent solutions of \[y'' + P(x) y' + Q(x) = 0\] on $[a, b]$ and satisfy the boundary conditions \[A_1 y_1(a) + B_1 y_1(a) = 0\] and \[A_2 y_2(b) + B_2 y_2(b) = 0\] then the BVP \[y'' + P(x) y' + Q(x) y = f(x)\] subject to the same boundary conditions has a particular solution \[y_p(x) = \int_a^b G(x, t) f(t) \,dt\] where $G(x, t)$ is the Green's function for the differential equation \[G(x, y) = \begin{cases}
            \frac{y_1(t) y_2(x)}{W(t)} & a \le t \le x \\
            \frac{y_1(x) y_2(t)}{W(t)} & x \le t \le b
          \end{cases}\] and $W(t)$ is the Wronskian \[W(t) = \begin{vmatrix}
            y_1(t)  & y_2(t)  \\
            y_1'(t) & y_2'(t)
          \end{vmatrix}\]
\end{itemize}

\setcounter{subsection}{11}
\subsection{Solving Systems of Linear Equations}

\begin{itemize}
  \item Systems of linear differential equations can be solved in a similar manner to systems of equations, namely by adding and subtracting multiples of different equations to eliminate particular variables

  \item We can also apply the differential operator $D$ as part of the elimination process

  \item Once you have an equation for each dependent variable it's important to substitute them back into the original differential equation to determine the constraints on the parameters — not all of them can be chosen arbitrarily
\end{itemize}

\section{The Laplace Transform}

\subsection{Definition of the Laplace Transform}

\begin{itemize}
  \item If a function $f(t)$ is defined for $t \ge 0$ and the limit \[\int_0^\infty K(s, t) f(t) \,dt = \lim_{b \rightarrow \infty} \int_0^b K(s, t) f(t) \,dt\] exist, the integral is said to \textbf{exist} or be \textbf{convergent}, otherwise it does not exist or is \textbf{divergent}

  \item If a function $f(t)$ is defined for $t \ge 0$ then the limit \[\mathcal{L} (f(t)) = \int_0^\infty e^{-s t} f(t) \,dt\] is called the \textbf{Laplace transform} of $f$ providing the integral converges

  \item $\mathcal{L}$ is a linear transform, i.e. \[\mathcal{L}\{\alpha f(t) + \beta g(t)\} = \alpha \mathcal{L}\{f(t)\} + \beta \mathcal{L}\{g(t)\}\]

  \item A function is said to be \textbf{piecewise continuous} on $[0, \infty)$ if, in any interval defined by $0 \le a \le t \le b$, there are at most a finite number of points $t_k$, $k = 1, 2, \ldots, n$ $(t_{k - 1} < t_k)$, at which $f$ has finite discontinuities and is continuous on each open interval defined by $t_{k - 1} < t < t_k$

  \item A function is said to be of \textbf{exponential order} if there exists constants $c$, $M > 0$, and $T > 0$ such that $|f(t)| \le M e^{c t}$ for all $t > T$

  \item If $f(t)$ is piecewise continuous on the interval $[0, \infty)$ and of exponential order, then $\mathcal{L} \{f(t)\}$ exists for $s > c$
\end{itemize}

\subsection{The Inverse Transform and Transforms of Derivatives}

\begin{itemize}
  \item $\mathcal{L}^{-1}$ is a linear transform, i.e. \[\mathcal{L}^{-1}\{\alpha F(s) + \beta G(s)\} = \alpha \mathcal{L}^{-1}\{F(s)\} + \beta \mathcal{L}^{-1}\{G(s)\}\]

  \item If $f$, $f'$, $\ldots$, $f^{(n - 1)}$ are continuous on $[0, \infty)$, are of exponential order, and $f^{(n)}$ is piecewise continuous on $[0, \infty)$, then \[\mathcal{L}\{f^{(n)}(t)\} = s^n F(s) - s^{n - 1} f(0) - s^{n - 2} f'(0) - \cdots - f^{(n - 1)}(0)\] where $F(s) = \mathcal{L}\{f(t)\}$

  \item The Laplace transform can be used to solve linear IVPs:

        \begin{enumerate}
          \item Take the Laplace transform of the DE, resulting in an algebraic equation in $F(s) = \mathcal{L}\{f(s)\}$ where $f(s)$ is the goal

          \item Solve the equation for $F(s)$

          \item Apply the inverse Laplace transform to find $f(s)$
        \end{enumerate}
\end{itemize}

\subsection{Translation Theorems}

\begin{itemize}
  \item The \textbf{first translation theorem} states that if \[\mathcal{L}\{f(t)\} = F(s)\] then \[\mathcal{L}\{e^{a t} f(t)\} = F(s - a)\] and \[\mathcal{L}^{-1}\{F(s - a)\} = \mathcal{L}^{-1}\{F(s)|_{s \rightarrow s - a}\} = e^{a t} f(t)\]

  \item The \textbf{unit step function} or \textbf{Heaviside function} is definde to be \[\mathcal{U}(t - a) = \begin{cases}
            0 & 0 \le t < a \\
            1 & t \ge a
          \end{cases}\]

\item The \textbf{second translation theorem} states that if $a > 0$ and \[\mathcal{L}\{f(s)\} = F(s)\] then \[\mathcal{L}\{f(t - a) \mathcal{U}(t - a)\} = e^{-a s} F(s)\] and \[\mathcal{L}^{-1}\{e^{-a s} F(s)\} = f(t - a) \mathcal{U}(t - a)\]

\item If $f$ and $\mathcal{U}$ aren't shifted by the same amount when applying the second translation theorem, an alternate form can be applied \[\mathcal{L}\{f(t) \mathcal{U}(t - a)\} = e^{-a s} \mathcal{L}\{f(t + a)\}\]
\end{itemize}

\subsection{Additional Operational Properties}

\begin{itemize}
  \item If $F(s) = \mathcal{L}\{f(t)\}$ and $n = 1, 2, 3, \ldots$, then \[\mathcal{L}\{t^n f(t)\} = (-1)^n \frac{d^n}{d s^n} F(s)\]

\item If functions $f$ and $g$ are piecewise continuous on the interval $[0, \infty)$ then the \textbf{convolution} of $f$ and $g$, denoted $f * g$, is a function defined by the integral \[f * g = \int_0^t f(\tau) g(t - \tau) \,d\tau\]

  \item The \textbf{convolution theorem} states that if $f$ and $g$ are piecewise continuous on $[0, \infty)$ and of exponential order, then \[\mathcal{L}\{f * g\} = \mathcal{L}\{f(t)\} \mathcal{L}\{g(t)\} = F(s) G(s)\] and \[\mathcal{L}^{-1}\{F(s) G(s)\} = f * g\]

  \item Under the convolution theorem if $g(t) = 1$ then $\mathcal{L}\{g(t)\} = G(s) = 1 / s$, \[\mathcal{L} \left\{ \int_0^t f(\tau) \,d\tau \right\} = \frac{F(s)}{s},\] and \[\mathcal{L}^{-1}\left\{\frac{F(s)}{s}\right\} = \int_0^t f(\tau) \,d\tau\]

  \item \textbf{Volterra integral equations} have the form \[f(t) = g(t) + \int_0^t f(\tau) g(t - \tau) \,d\tau\] and can be solved by using the convolution theorem while taking the Laplace transform

  \item An \textbf{integro-differential equation} is an equation that involves both integrals and derivatives of a function

\item If $f(t)$ is piecewise continuous on $[0, \infty)$, of exponential order, and periodic with period $T$, then \[\mathcal{L}\{f(t)\} = \frac{1}{1 - e^{-s T}} \int_0^T e^{-s t} f(t) \,dt\]
\end{itemize}

\subsection{The Dirac Delta Function}

\begin{itemize}
  \item A \textbf{unit impulse} function is defined as \[\delta_a(t - t_0) = \begin{cases}
            0             & 0 \le t < t_0 - a       \\
            \frac{1}{2 a} & t_0 - a \le t < t_0 + a \\
            0             & t_0 + a \le t
          \end{cases}\] and it possesses the property \[\int_0^\infty \delta_a(t - t_0) \,dt = 1\]

  \item The \textbf{Dirac delta function} is defined as \[\delta(t - t_0) = \lim_{a \rightarrow 0} \delta_a(t - t_0)\] and has the properties \[\delta(t - t_0) = \begin{cases}
            \infty & t = t_0   \\
            0      & t \ne t_0
          \end{cases}\] and \[\int_0^\infty \delta(t - t_0) \,dt = 1\]

  \item For $t_0 > 0$ \[\mathcal{L}\{\delta(t - t_0)\} = e^{-s t_0}\]
\end{itemize}

\section{Series Solutions of Linear Differential Equations}

\subsection{Solutions about Ordinary Points}

\begin{itemize}
  \item A \textbf{power series} is an infinite series of the form \[\sum_{n = 0}^\infty a_n (x - c)^n = a_0 + a_1 (x - c) + a_2 (x - c)^2 + \ldots\]

  \item A point $x_0$ is said to be an \textbf{ordinary point} of the differential equation \[y'' + P(x) y' + Q(x) y = 0\] if both $P(x)$ and $Q(x)$ are analytic at $x_0$. A point that is not an ordinary point is said to be a \textbf{singular point} of the equation.

  \item If $x = x_0$ is an ordinary point of the differential equation above, we can always find two linearly independent solutions in the form of a power series centred at $x_0$. Such a solution is said to be a \textbf{solution about the ordinary point $x_0$}

  \item A series solution converges at least on some interval $|x - x_0| < R$ where $R$ is the distance from $x_0$ to the closest singular point

  \item A series solution can be found for a homogeneous linear second-order differential equation by

        \begin{enumerate}
          \item Assume the solution is of the form \[y = \sum_{n = 0}^\infty c_n x^n\] and thus \[y' = \sum_{n = 1}^\infty c_n n x^{n - 1}\] and \[y'' = \sum_{n = 2}^\infty c_n n (n - 1) x^{n - 2}\]

          \item Substitute the assumed solution into the DE

          \item Group the summations

          \item Find a recurrence relation for the coefficients which will result in all coefficients being expressed in terms of $c_0$ or $c_1$

          \item Group terms by $c_0$ and $c_1$, giving \[y(x) = c_0 y_1(x) + c_1 y_2(x)\] where $y_1(x)$ and $y_2(x)$ are the two linearly independent solutions
        \end{enumerate}
\end{itemize}

\subsection{Solutions about Singular Points}

\begin{itemize}
  \item A singular point $x_0$ is said to be a \textbf{regular singular point} of the differential equation \[y'' + P(x) y' + Q(x) y = 0\] if the functions $p(x) = (x - x_0) P(x)$ and $q(x) = (x - x_0)^2 Q(x)$ are both analytic at $x_0$. A singular point that is not regular is said to be an \textbf{irregular singular point} of the equation.

  \item This, if $x - x_0$ appears at most to the first power in the denominator of $P(x)$ and at most to the second power of the denominator of $Q(x)$, then $x = x_0$ is a regular singular point

  \item \textbf{Frobenius' theorem} states that if $x = x_0$ is a regular singular point of the differential equation \[a_2(x) y'' + a_1(x) y' + a_0(x) y = 0\] then there exists at least one nonzero  solution of the form \[y = \sum_{n = 0}^\infty c_n (x - x_0)^{n + r}\] where $r$ is a constant to be determined

  \item When applying Frobenius' theorem, $r$ can be determined by equating the total coefficient of the lowest power of $x$ to $0$ and solving for $r$. This coefficient is called the \textbf{indicial equation} and its solutions the \textbf{indicial roots} or \textbf{exponents}

  \item Frobenius' theorem can be applied like so:

        \begin{enumerate}
\item Assume the solution is of the form \[y = \sum_{n = 0}^\infty c_n (x - x_0)^{n + r}\] where $x = x_0$ is a regular singular point and thus \[y' = \sum_{n = 0}^\infty (n + r) c_n (x - x_0)^{n + r - 1}\] and \[y'' = \sum_{n = 0}^\infty (n + r) (n + r - 1) c_n (x - x_0)^{n + r - 2}\]

          \item Substitute the assumed solution into the DE

          \item Group the summations

          \item Solve the indicial equation to determine the value(s) of $r$

          \item Solve the recurrence relation(s) given by the value(s) of $r$ to determine constants

          \item Use the constants to determine the solution(s)
        \end{enumerate}

  \item Assuming the indicial roots are real and $r_1 > r_2$, there are three cases to consider:

        \begin{enumerate}
          \item If $r_1$ and $r_2$ are distinct and don't differ by an integer, then there exist two linearly independent solutions of the form \[y_1(x) = \sum_{n = 0}^\infty c_n x^{n + r_1}\] and \[y_2(x) = \sum_{n = 0}^\infty b_n x^{n + r_2}\]

          \item If $r_1 - r_2 = N$ where $N$ is a positive integer, then there exist two linearly independent solutions of the form \[y_1(x) = \sum_{n = 0}^\infty c_n x^{n + r_1}, \: c_0 \ne 0\] and \[y_2(x) = C y_1(x) \ln x + \sum_{n = 0}^\infty b_n x^{n + r_2}, \: b_0 \ne 0\] where $C$ is a constant that may be zero

          \item If $r_1 = r_2$, then there exist two linearly independent solutions of the form \[y_1(x) = \sum_{n = 0}^\infty c_n x^{n + r_1}, \: c_0 \ne 0\] and \[y_2(x) = y_1(x) \ln x + \sum_{n = 0}^\infty b_n x^{n + r_2}\]
        \end{enumerate}

  \item In cases 2 and 3 above it may not be possible to find a second solution. Instead a second solution can be found using the first solution and reduction of order \[y_2(x) = y_1(x) \int \frac{e^{-\int P(x) \,dx}}{y_1^2(x)} \,dx\]
\end{itemize}

\subsection{Special Functions}

\begin{itemize}
  \item The equation \[x^2 y'' + x y' + (x^2 - \nu^2) y = 0\] is called \textbf{Bessel's equation of order $\boldsymbol{\nu}$} where $nu \ge 0$

  \item The equation \[(1 - x^2) y'' - 2 x y' + n (n + 1) y = 0\] is called \textbf{Legendre's equation of order $\boldsymbol{n}$} where $n$ is a nonnegative integer
\end{itemize}

\subsubsection{Bessel Functions}

\begin{itemize}
  \item The indicial roots are $r_1 = \nu$ and $r_2 = -\nu$

  \item $\Gamma(x)$ is the gamma function and it has the property that \[\Gamma(1 + \alpha) = \alpha \Gamma(\alpha)\]

  \item The first solution is \[J_\nu(x) = \sum_{n = 0}^\infty \frac{(-1)^n}{n! \Gamma(1 + \nu + n)} \left( \frac{x}{2} \right)^{2 n + \nu}\] and it converges on $[0, \infty)$ if $\nu \ge 0$

  \item The second solution is \[J_{-\nu}(x) = \sum_{n = 0}^\infty \frac{(-1)^n}{n! \Gamma(1 - \nu + n)} \left( \frac{x}{2} \right)^{2 n - \nu}\] and, depending on the value of $\nu$, may contain negative powers of $x$ and thus it converges on the interval $(0, \infty)$

  \item These solutions are known as \textbf{Bessel functions of the first kind} of order $\nu$ and $-\nu$

  \item The general solution to a Bessel equation of order $\nu$ is \[y = c_1 J_\nu(x) + c_2 J_{-\nu}(x), \,v \ne \text{integer}\]

  \item The function \[Y_\nu(x) = \frac{\cos \nu \pi J_\nu(x) - J_{-\nu}(x)}{\sin \nu \pi}\] is called the \textbf{Bessel function of the second kind} of order $\nu$

  \item A general solution to a Bessel function of order $\nu$ is \[y = c_1 J_\nu(x) + c_2 Y_\nu(x)\]

  \item Sometimes it's possible to transform a DE into a Bessel function via a change of variable, e.g. by substituting $t = \alpha x$ in the \textbf{parametric Bessel function of order $\boldsymbol{\nu}$} \[x^2 y'' + x y' + (\alpha^2 x^2 - \nu^2) y = 0\] it can be transformed into \[t^2 \frac{d^2 y}{d t^2} + t \frac{d y}{d t} + (t^2 - \nu^2) y = 0\] which has the general solution \[y = c_1 J_\nu(t) + c_2 Y_\nu(t)\] or \[y = c_1 J_\nu(\alpha x) + c_2 Y_\nu(\alpha x)\]
\end{itemize}

\section{Numerical Solutions of Ordinary Differential Equations}

\subsection{Euler Methods and Error Analysis}

\begin{itemize}
  \item \textbf{Round-off error} occurs when calculators or computers round off numbers to fit within the limits of what they can represent

  \item If we assume that $y_n$ is accurate, then the difference between the computed and actual values of $y_{n + 1}$ is called the \textbf{local truncation error}, \textbf{formula error}, or \textbf{discretization error}

  \item The upper bound on the absolute error of the local truncation error for Euler's formula is \[M \frac{h^2}{2!}\] where \[M = \max_{x_n < x < x_{n + 1}} |y''(x)|\]

  \item The local truncation error for Euler's method is $O(h^2)$

  \item If $e(h)$ denotes the error in a numerical calculation depending on $h$, then $e(h)$ is said to be $O(h^n)$ if there is a constant $C$ and a positive integer $n$ such that $|e(h)| \le C h^n$

  \item If $y_n$ isn't necessarily accurate, i.e. it contains its own local truncation error, the difference between the computer and actual values of $y_{n + 1}$ is called the \textbf{global truncation error} (this may be greater than the local truncation error as its affected by the truncation errors of previous values)

  \item The global truncation error for Euler's method is $O(h)$

  \item If a method for the numerical solution of a differential equation has local truncation error $O(h^{\alpha + 1})$ then the global truncation error is $O(h^\alpha)$

  \item The \textbf{improved Euler's method} uses an average of the gradients at the original point and the point predicted by Euler's method \[y_{n + 1} = y_n + h \frac{f(x_n, y_n) + f(x_n, y^*_{n + 1})}{2}\] where \[y^*_{n + 1} = y_n + h f(x_n, y_n)\]

  \item The local truncation error for the improved Euler's method is $O(h^3)$ and the global truncation error is $O(h^2)$
\end{itemize}

\subsection{Runge-Kutta Methods}

\begin{itemize}
  \item Runge-Kutta methods are methods for obtaining approximate solutions to first-order initial value problems

  \item There are Runge-Kutta methods of different orders

  \item Each Runge-Kutta method is a weighted average of slopes over the interval $x_n < x < x_{n + 1}$ \[y_{n + 1} = y_n + h (w_1 k_1 + w_2 k_2+ \cdots + w_m k_m)\] where $m$ is the order of the method

  \item Euler's method is said to be a first-order Runge-Kutta method

  \item The improved Euler's method is said to be a second-order Runge-Kutta method

  \item The local truncation error for RK4 is $y^{(5)}(c)/5!$ or $O(h^5)$ and the global truncation error is $O(h^4)$

  \item Numerical methods that use a variable step size are called \textbf{adaptive methods}

  \item One of the more popular adaptive methods is the \textbf{Runge-Kutta-\\Fehlberg method} or the RKF45 method
\end{itemize}

\subsection{Multistep Methods}

\begin{itemize}
  \item Methods that compute successive values based only on information from the immediately preceeding value are called \textbf{single step} or \textbf{starting methods}

  \item Methods that compute successive values based on information from multiple previous values are called \textbf{multistep} or \textbf{continuing methods}

  \item The \textbf{Adams-Bashforth-Moulton method} is a fourth-order multistep method

  \item A numerical method is said to be \textbf{stable} if small changes in the initial condition result in only small changes in the computed solution

  \item A numerical method is said to be \textbf{unstable} if it's not stable

  \item Sometimes multistep methods are less computationally intensive because you evaluate the function fewer times
\end{itemize}

\subsection{Higher-Order Equations and Systems}

\begin{itemize}
  \item A second-order initial-value problem \[y'' = f(x, y, y'), \:y(x_0) = y_0, \:y'(x_0) = u_0\] can be expressed as an initial value problem for the system of first-order differential equations

        \begin{align*}
          y' & = u          \\
          u' & = f(x, y, u)
        \end{align*}

        where $y' = u$ which allows

        \begin{align*}
          y_{n + 1} & = y_n + h u_n              \\
          u_{n + 1} & = u_n + h f(x_n, y_n, u_n)
        \end{align*}

  \item In general, we can express an $n$th-order differential equation \[y^{(n)} = f(x, y, y', \ldots, y^{(n - 1)})\] as a system of $n$ first-order equations using the substitutions $y = u_1$, $y' = u_2$, $y'' = u_3$, $\ldots$, $y^{(n - 1)} = u_n$
\end{itemize}

\end{document}